{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adirmorgan/Private-Network-Inference/blob/main/SNL_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzEokJoDRLOi"
      },
      "source": [
        "Following - \"Selective Network Linearization for Efficient Private Inference\"\n",
        "\n",
        "https://arxiv.org/abs/2202.02340\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D8rhOK_3d7z",
        "outputId": "3f87486e-469f-460b-d99e-0cf1a3f0145c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iRklMePQuE8",
        "outputId": "41db4543-13d4-409f-a55b-283190e85f7b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "2.6.0+cu124\n",
            "0.21.0+cu124\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the network model"
      ],
      "metadata": {
        "id": "RdxIKTBKAB-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class LearnableAlpha(nn.Module):\n",
        "    def __init__(self, out_channel, layer_dim):\n",
        "        super(LearnableAlpha, self).__init__()\n",
        "        self.alphas = nn.Parameter(torch.ones(1, out_channel, layer_dim, layer_dim), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(x) * self.alphas.expand_as(x) + (1-self.alphas.expand_as(x)) * x\n",
        "        return out"
      ],
      "metadata": {
        "id": "QYbfPhPifACW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrbsi-dLTQjh"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, layer_dim, stride=1):\n",
        "      super(BasicBlock, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                             stride=stride, padding=1, bias=False)\n",
        "      self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "      self.prelu1 = LearnableAlpha(out_channels, layer_dim)\n",
        "      self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                             stride=1, padding=1, bias=False)\n",
        "      self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "      self.prelu2 = LearnableAlpha(out_channels, layer_dim)\n",
        "\n",
        "      self.shortcut = nn.Sequential()\n",
        "      # Sequential is a container to a NN, connecting all inputed layers sequentially.\n",
        "      # Here there are no layers... So it seems just like an \"empty object of NN type\".\n",
        "      # Seems just like a default option, the real \"shortcut\" is the following:\n",
        "\n",
        "      if stride != 1 or in_channels != out_channels:\n",
        "          self.shortcut = nn.Sequential(\n",
        "              nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "              nn.BatchNorm2d(out_channels)\n",
        "          )\n",
        "\n",
        "        # In this case, the shortcut is similar to the \"first part\" of the block,\n",
        "        # but the kernel size is different. Not sure why\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.conv1(x)\n",
        "      out = self.bn1(out)\n",
        "      out = self.prelu1(out)\n",
        "      out = self.conv2(out)\n",
        "      out = self.bn2(out)\n",
        "      out += self.shortcut(x)\n",
        "      out = self.prelu2(out)\n",
        "      return out\n",
        "\n",
        "class ResNet18_SNL(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "      super(ResNet18_SNL, self).__init__()\n",
        "      self.in_channels = 64\n",
        "      self.layer_dim = 32\n",
        "\n",
        "      self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "      self.bn1 = nn.BatchNorm2d(64)\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "      self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)    # 2 BasicBlocks in each ResNet layer\n",
        "      self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
        "      self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
        "      self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
        "\n",
        "      self.avgpool = nn.AdaptiveAvgPool2d((1, 1))    # Parameter is output size\n",
        "      self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "      strides = [stride] + [1] * (num_blocks - 1)    # using stride value for the first layer, then later stride=1\n",
        "      layers = []\n",
        "      for stride in strides:\n",
        "          self.layer_dim = self.layer_dim // 2 if stride == 2 else self.layer_dim\n",
        "          layers.append(block(self.in_channels, out_channels, self.layer_dim, stride))\n",
        "          self.in_channels = out_channels\n",
        "      return nn.Sequential(*layers)\n",
        "      # Each layer is a sequential cascade of the BasicBlocks sub-layer, I think this allows\n",
        "      # implicitly to cascade the forward methods of the BasicBlocks\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.conv1(x)\n",
        "      out = self.bn1(out)\n",
        "      out = self.layer1(out)\n",
        "      out = self.layer2(out)\n",
        "      out = self.layer3(out)\n",
        "      out = self.layer4(out)\n",
        "\n",
        "      out = self.avgpool(out)\n",
        "      out = out.view(out.size(0), -1)\n",
        "      out = self.fc(out)\n",
        "      return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18_SNL()\n",
        "print(model)\n",
        "# Print all learnable parameters in the model\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmm0IsgDmjbK",
        "outputId": "1a4f52b2-6405-4e6b-af5b-5c3a1b3c4a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet18_SNL(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu1): LearnableAlpha()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu2): LearnableAlpha()\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu1): LearnableAlpha()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu2): LearnableAlpha()\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu1): LearnableAlpha()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu2): LearnableAlpha()\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu1): LearnableAlpha()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu2): LearnableAlpha()\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu1): LearnableAlpha()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu2): LearnableAlpha()\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu1): LearnableAlpha()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu2): LearnableAlpha()\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu1): LearnableAlpha()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu2): LearnableAlpha()\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu1): LearnableAlpha()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (prelu2): LearnableAlpha()\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "conv1.weight: torch.Size([64, 3, 3, 3])\n",
            "bn1.weight: torch.Size([64])\n",
            "bn1.bias: torch.Size([64])\n",
            "layer1.0.conv1.weight: torch.Size([64, 64, 3, 3])\n",
            "layer1.0.bn1.weight: torch.Size([64])\n",
            "layer1.0.bn1.bias: torch.Size([64])\n",
            "layer1.0.prelu1.alphas: torch.Size([1, 64, 32, 32])\n",
            "layer1.0.conv2.weight: torch.Size([64, 64, 3, 3])\n",
            "layer1.0.bn2.weight: torch.Size([64])\n",
            "layer1.0.bn2.bias: torch.Size([64])\n",
            "layer1.0.prelu2.alphas: torch.Size([1, 64, 32, 32])\n",
            "layer1.1.conv1.weight: torch.Size([64, 64, 3, 3])\n",
            "layer1.1.bn1.weight: torch.Size([64])\n",
            "layer1.1.bn1.bias: torch.Size([64])\n",
            "layer1.1.prelu1.alphas: torch.Size([1, 64, 32, 32])\n",
            "layer1.1.conv2.weight: torch.Size([64, 64, 3, 3])\n",
            "layer1.1.bn2.weight: torch.Size([64])\n",
            "layer1.1.bn2.bias: torch.Size([64])\n",
            "layer1.1.prelu2.alphas: torch.Size([1, 64, 32, 32])\n",
            "layer2.0.conv1.weight: torch.Size([128, 64, 3, 3])\n",
            "layer2.0.bn1.weight: torch.Size([128])\n",
            "layer2.0.bn1.bias: torch.Size([128])\n",
            "layer2.0.prelu1.alphas: torch.Size([1, 128, 16, 16])\n",
            "layer2.0.conv2.weight: torch.Size([128, 128, 3, 3])\n",
            "layer2.0.bn2.weight: torch.Size([128])\n",
            "layer2.0.bn2.bias: torch.Size([128])\n",
            "layer2.0.prelu2.alphas: torch.Size([1, 128, 16, 16])\n",
            "layer2.0.shortcut.0.weight: torch.Size([128, 64, 1, 1])\n",
            "layer2.0.shortcut.1.weight: torch.Size([128])\n",
            "layer2.0.shortcut.1.bias: torch.Size([128])\n",
            "layer2.1.conv1.weight: torch.Size([128, 128, 3, 3])\n",
            "layer2.1.bn1.weight: torch.Size([128])\n",
            "layer2.1.bn1.bias: torch.Size([128])\n",
            "layer2.1.prelu1.alphas: torch.Size([1, 128, 16, 16])\n",
            "layer2.1.conv2.weight: torch.Size([128, 128, 3, 3])\n",
            "layer2.1.bn2.weight: torch.Size([128])\n",
            "layer2.1.bn2.bias: torch.Size([128])\n",
            "layer2.1.prelu2.alphas: torch.Size([1, 128, 16, 16])\n",
            "layer3.0.conv1.weight: torch.Size([256, 128, 3, 3])\n",
            "layer3.0.bn1.weight: torch.Size([256])\n",
            "layer3.0.bn1.bias: torch.Size([256])\n",
            "layer3.0.prelu1.alphas: torch.Size([1, 256, 8, 8])\n",
            "layer3.0.conv2.weight: torch.Size([256, 256, 3, 3])\n",
            "layer3.0.bn2.weight: torch.Size([256])\n",
            "layer3.0.bn2.bias: torch.Size([256])\n",
            "layer3.0.prelu2.alphas: torch.Size([1, 256, 8, 8])\n",
            "layer3.0.shortcut.0.weight: torch.Size([256, 128, 1, 1])\n",
            "layer3.0.shortcut.1.weight: torch.Size([256])\n",
            "layer3.0.shortcut.1.bias: torch.Size([256])\n",
            "layer3.1.conv1.weight: torch.Size([256, 256, 3, 3])\n",
            "layer3.1.bn1.weight: torch.Size([256])\n",
            "layer3.1.bn1.bias: torch.Size([256])\n",
            "layer3.1.prelu1.alphas: torch.Size([1, 256, 8, 8])\n",
            "layer3.1.conv2.weight: torch.Size([256, 256, 3, 3])\n",
            "layer3.1.bn2.weight: torch.Size([256])\n",
            "layer3.1.bn2.bias: torch.Size([256])\n",
            "layer3.1.prelu2.alphas: torch.Size([1, 256, 8, 8])\n",
            "layer4.0.conv1.weight: torch.Size([512, 256, 3, 3])\n",
            "layer4.0.bn1.weight: torch.Size([512])\n",
            "layer4.0.bn1.bias: torch.Size([512])\n",
            "layer4.0.prelu1.alphas: torch.Size([1, 512, 4, 4])\n",
            "layer4.0.conv2.weight: torch.Size([512, 512, 3, 3])\n",
            "layer4.0.bn2.weight: torch.Size([512])\n",
            "layer4.0.bn2.bias: torch.Size([512])\n",
            "layer4.0.prelu2.alphas: torch.Size([1, 512, 4, 4])\n",
            "layer4.0.shortcut.0.weight: torch.Size([512, 256, 1, 1])\n",
            "layer4.0.shortcut.1.weight: torch.Size([512])\n",
            "layer4.0.shortcut.1.bias: torch.Size([512])\n",
            "layer4.1.conv1.weight: torch.Size([512, 512, 3, 3])\n",
            "layer4.1.bn1.weight: torch.Size([512])\n",
            "layer4.1.bn1.bias: torch.Size([512])\n",
            "layer4.1.prelu1.alphas: torch.Size([1, 512, 4, 4])\n",
            "layer4.1.conv2.weight: torch.Size([512, 512, 3, 3])\n",
            "layer4.1.bn2.weight: torch.Size([512])\n",
            "layer4.1.bn2.bias: torch.Size([512])\n",
            "layer4.1.prelu2.alphas: torch.Size([1, 512, 4, 4])\n",
            "fc.weight: torch.Size([10, 512])\n",
            "fc.bias: torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Training data and split to train & val sets"
      ],
      "metadata": {
        "id": "VxWo8CeMAGNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Data Preparation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "train_size = int(0.8 * len(trainset))  # 80% for training\n",
        "val_size = len(trainset) - train_size  # Remaining 20% for validation\n",
        "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZLPEQiPMyEg",
        "outputId": "17284498-48b5-4f44-e965-589b2f936c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 13.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training process preliminaries\n"
      ],
      "metadata": {
        "id": "hl8yit09BE5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrain_params = {'epochs': 200,\n",
        "                   'batch_size': 128,\n",
        "                   'lr': 0.1,\n",
        "                   'momentum': 0.9,\n",
        "                   'weight_decay': 5e-4,  # l2 weights regularization factor\n",
        "                   'lr_milestones': [100, 150],\n",
        "                   'gamma': 0.1,  # LR is multiplied by gamma on schedule.\n",
        "                   }\n",
        "\n",
        "SNL_training_params = {'epochs': 2000,\n",
        "                      'batch_size': 128,\n",
        "                      'lr': 1e-3,\n",
        "                      'momentum': 0.9,\n",
        "                      'weight_decay': 5e-4,  # l2 weights regularization factor\n",
        "                      'lr_step_size': 30,  # How often to decrease learning by gamma\n",
        "                      'lr_milestones': [80, 120],\n",
        "                      'gamma': 0.1,  # LR is multiplied by gamma on schedule.\n",
        "                      }\n",
        "\n",
        "SNL_params = {'relu_budget': 10000,\n",
        "              'relu_lin_threshold': 1e-2,\n",
        "              'initial_lasso_weight': 1e-5,\n",
        "              'lasso_weight_factor': 1.1\n",
        "              }\n",
        "\n",
        "fine_tuning_params = {'epochs': 100,\n",
        "                      'batch_size': 128,\n",
        "                      'lr': 1e-3,\n",
        "                      'lr_step_size': 30,  # How often to decrease learning by gamma\n",
        "                      'momentum': 0.9,\n",
        "                      'weight_decay': 5e-4,  # l2 weights regularization factor\n",
        "                      'gamma': 0.1,  # LR is multiplied by gamma on schedule.\n",
        "                      }\n",
        "\n"
      ],
      "metadata": {
        "id": "2kSTEXlwBJfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD, Optimizer, Adam\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
        "import time\n"
      ],
      "metadata": {
        "id": "cPP5i444FNTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "7Q5MDMNTI2VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "metadata": {
        "id": "ZqYIwvKFIhjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(loader: DataLoader, model: torch.nn.Module, criterion, optimizer: Optimizer,\n",
        "          epoch: int, device, print_freq=100, display=True):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    end = time.time()\n",
        "#     print(\"Entered training function\")\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute output\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, acc5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        top1.update(acc1.item(), inputs.size(0))\n",
        "        top5.update(acc5.item(), inputs.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0 and display == True:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "                  'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "                epoch, i, len(loader), batch_time=batch_time,\n",
        "                data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "\n",
        "    return (losses.avg, top1.avg, top5.avg)"
      ],
      "metadata": {
        "id": "TWrQPpJxIQCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(loader: DataLoader, model: torch.nn.Module, criterion, device, print_freq, display=False):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    end = time.time()\n",
        "\n",
        "    # switch to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(loader):\n",
        "            # measure data loading time\n",
        "            data_time.update(time.time() - end)\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # compute output\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            top1.update(acc1.item(), inputs.size(0))\n",
        "            top5.update(acc5.item(), inputs.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % print_freq == 0 and display == True:\n",
        "                print('Test : [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "                      'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "                    i, len(loader), batch_time=batch_time,\n",
        "                    data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "\n",
        "        print(\n",
        "            'Test Loss  ({loss.avg:.4f})\\t'\n",
        "            'Test Acc@1 ({top1.avg:.3f})\\t'\n",
        "            'Test Acc@5 ({top5.avg:.3f})'.format(\n",
        "        loss=losses, top1=top1, top5=top5)\n",
        "        )\n",
        "\n",
        "        return (losses.avg, top1.avg, top5.avg)"
      ],
      "metadata": {
        "id": "pl-slE8PI96T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretraining phase"
      ],
      "metadata": {
        "id": "ogjIwYnoARdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(\n",
        "    train_dataset,  # Training dataset\n",
        "    batch_size=pretrain_params['batch_size'],  # Number of samples per batch\n",
        "    shuffle=True,  # Shuffle the training data\n",
        "    num_workers=2  # Number of subprocesses for data loading\n",
        ")\n",
        "\n",
        "valloader = DataLoader(\n",
        "    val_dataset,  # Validation dataset\n",
        "    batch_size=pretrain_params['batch_size'],  # Number of samples per batch\n",
        "    shuffle=False,  # No need to shuffle validation data\n",
        "    num_workers=2  # Number of subprocesses for data loading\n",
        ")\n"
      ],
      "metadata": {
        "id": "uBBg05fXFrPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet18_SNL()\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if 'alpha' in name:\n",
        "        param.requires_grad = False\n",
        "        print(param.data)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJw9smDwQxyp",
        "outputId": "bdf1ae3c-d5d8-4557-aaf0-82cf50738871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]]]])\n",
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18_SNL(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = SGD(model.parameters(), lr=pretrain_params['lr'], momentum=pretrain_params['momentum'], weight_decay=pretrain_params['weight_decay'])\n",
        "scheduler = MultiStepLR(optimizer, milestones=pretrain_params['lr_milestones'], gamma=0.1)\n",
        "\n",
        "best_top1 = 0\n",
        "\n",
        "for epoch in range(pretrain_params['epochs']):\n",
        "\n",
        "  # training\n",
        "  train(trainloader, model, criterion, optimizer, epoch, device)\n",
        "\n",
        "  # validation\n",
        "  cur_step = (epoch+1) * len(trainloader)\n",
        "  _, top1, _ = test(valloader, model, criterion, device, cur_step)\n",
        "  scheduler.step()\n",
        "\n",
        "  # save\n",
        "  if best_top1 < top1:\n",
        "      best_top1 = top1\n",
        "      is_best = True\n",
        "  else:\n",
        "      is_best = False\n",
        "\n",
        "  if is_best:\n",
        "      model_path = '/content/gdrive/My Drive/SNL_results/ResNet_18_SNL_pretrained_10epochs.pth'\n",
        "      torch.save(model.state_dict(), model_path)\n",
        "\n",
        "  print(\"\")\n",
        "\n",
        "print(\"Best model's validation acc: {:.4%}\".format(best_top1 / 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LorhFMDUmz-W",
        "outputId": "440d664c-8873-4b93-e542-e6a01aca5323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/313]\tTime 1.698 (1.698)\tData 0.269 (0.269)\tLoss 2.4033 (2.4033)\tAcc@1 6.250 (6.250)\tAcc@5 50.000 (50.000)\n",
            "Epoch: [0][100/313]\tTime 0.124 (0.128)\tData 0.002 (0.005)\tLoss 1.7899 (2.5436)\tAcc@1 28.906 (19.624)\tAcc@5 89.062 (71.419)\n",
            "Epoch: [0][200/313]\tTime 0.112 (0.120)\tData 0.002 (0.004)\tLoss 1.8270 (2.2022)\tAcc@1 35.938 (24.351)\tAcc@5 86.719 (77.903)\n",
            "Epoch: [0][300/313]\tTime 0.113 (0.118)\tData 0.002 (0.004)\tLoss 1.5967 (2.0304)\tAcc@1 46.094 (28.782)\tAcc@5 90.625 (81.227)\n",
            "Test Loss  (1.7328)\tTest Acc@1 (37.110)\tTest Acc@5 (87.360)\n",
            "\n",
            "Epoch: [1][0/313]\tTime 0.383 (0.383)\tData 0.319 (0.319)\tLoss 1.5365 (1.5365)\tAcc@1 42.188 (42.188)\tAcc@5 91.406 (91.406)\n",
            "Epoch: [1][100/313]\tTime 0.119 (0.118)\tData 0.012 (0.006)\tLoss 1.5324 (1.5983)\tAcc@1 46.875 (40.811)\tAcc@5 90.625 (90.138)\n",
            "Epoch: [1][200/313]\tTime 0.114 (0.117)\tData 0.002 (0.005)\tLoss 1.6092 (1.5439)\tAcc@1 42.969 (43.132)\tAcc@5 89.844 (91.002)\n",
            "Epoch: [1][300/313]\tTime 0.117 (0.117)\tData 0.002 (0.005)\tLoss 1.4653 (1.4943)\tAcc@1 45.312 (45.172)\tAcc@5 93.750 (91.666)\n",
            "Test Loss  (1.2863)\tTest Acc@1 (53.160)\tTest Acc@5 (94.760)\n",
            "\n",
            "Epoch: [2][0/313]\tTime 0.306 (0.306)\tData 0.226 (0.226)\tLoss 1.2340 (1.2340)\tAcc@1 57.031 (57.031)\tAcc@5 95.312 (95.312)\n",
            "Epoch: [2][100/313]\tTime 0.122 (0.119)\tData 0.002 (0.006)\tLoss 1.3499 (1.2986)\tAcc@1 57.031 (52.885)\tAcc@5 91.406 (94.284)\n",
            "Epoch: [2][200/313]\tTime 0.120 (0.118)\tData 0.002 (0.005)\tLoss 1.0095 (1.2640)\tAcc@1 67.188 (54.081)\tAcc@5 97.656 (94.667)\n",
            "Epoch: [2][300/313]\tTime 0.117 (0.118)\tData 0.002 (0.005)\tLoss 1.2368 (1.2345)\tAcc@1 57.812 (55.404)\tAcc@5 93.750 (94.832)\n",
            "Test Loss  (1.1105)\tTest Acc@1 (60.730)\tTest Acc@5 (95.950)\n",
            "\n",
            "Epoch: [3][0/313]\tTime 0.284 (0.284)\tData 0.215 (0.215)\tLoss 1.1056 (1.1056)\tAcc@1 62.500 (62.500)\tAcc@5 92.969 (92.969)\n",
            "Epoch: [3][100/313]\tTime 0.116 (0.121)\tData 0.002 (0.008)\tLoss 1.2191 (1.0804)\tAcc@1 57.812 (61.672)\tAcc@5 95.312 (96.024)\n",
            "Epoch: [3][200/313]\tTime 0.117 (0.120)\tData 0.002 (0.006)\tLoss 1.1181 (1.0567)\tAcc@1 57.812 (62.271)\tAcc@5 96.875 (96.261)\n",
            "Epoch: [3][300/313]\tTime 0.118 (0.120)\tData 0.002 (0.005)\tLoss 1.1360 (1.0393)\tAcc@1 61.719 (62.975)\tAcc@5 95.312 (96.405)\n",
            "Test Loss  (1.0084)\tTest Acc@1 (65.090)\tTest Acc@5 (96.050)\n",
            "\n",
            "Epoch: [4][0/313]\tTime 0.338 (0.338)\tData 0.263 (0.263)\tLoss 0.8825 (0.8825)\tAcc@1 73.438 (73.438)\tAcc@5 96.875 (96.875)\n",
            "Epoch: [4][100/313]\tTime 0.121 (0.122)\tData 0.002 (0.006)\tLoss 0.7916 (0.9412)\tAcc@1 71.875 (66.499)\tAcc@5 96.875 (97.239)\n",
            "Epoch: [4][200/313]\tTime 0.120 (0.121)\tData 0.002 (0.005)\tLoss 0.8834 (0.9238)\tAcc@1 67.969 (67.195)\tAcc@5 97.656 (97.353)\n",
            "Epoch: [4][300/313]\tTime 0.120 (0.120)\tData 0.002 (0.004)\tLoss 1.0605 (0.9106)\tAcc@1 64.844 (67.746)\tAcc@5 93.750 (97.350)\n",
            "Test Loss  (1.0916)\tTest Acc@1 (63.480)\tTest Acc@5 (95.000)\n",
            "\n",
            "Epoch: [5][0/313]\tTime 0.240 (0.240)\tData 0.176 (0.176)\tLoss 0.8215 (0.8215)\tAcc@1 71.875 (71.875)\tAcc@5 97.656 (97.656)\n",
            "Epoch: [5][100/313]\tTime 0.127 (0.121)\tData 0.004 (0.005)\tLoss 0.7330 (0.8328)\tAcc@1 75.781 (70.405)\tAcc@5 97.656 (97.981)\n",
            "Epoch: [5][200/313]\tTime 0.119 (0.121)\tData 0.009 (0.004)\tLoss 0.6601 (0.8124)\tAcc@1 74.219 (71.047)\tAcc@5 100.000 (98.084)\n",
            "Epoch: [5][300/313]\tTime 0.121 (0.121)\tData 0.002 (0.004)\tLoss 0.7045 (0.7954)\tAcc@1 75.781 (71.797)\tAcc@5 98.438 (98.131)\n",
            "Test Loss  (0.9094)\tTest Acc@1 (68.060)\tTest Acc@5 (97.520)\n",
            "\n",
            "Epoch: [6][0/313]\tTime 0.261 (0.261)\tData 0.202 (0.202)\tLoss 0.6888 (0.6888)\tAcc@1 76.562 (76.562)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [6][100/313]\tTime 0.119 (0.123)\tData 0.004 (0.005)\tLoss 0.7289 (0.7094)\tAcc@1 72.656 (74.907)\tAcc@5 96.094 (98.530)\n",
            "Epoch: [6][200/313]\tTime 0.126 (0.122)\tData 0.013 (0.004)\tLoss 0.6595 (0.7042)\tAcc@1 74.219 (75.214)\tAcc@5 99.219 (98.546)\n",
            "Epoch: [6][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.6398 (0.6974)\tAcc@1 77.344 (75.613)\tAcc@5 100.000 (98.562)\n",
            "Test Loss  (0.8750)\tTest Acc@1 (69.830)\tTest Acc@5 (98.170)\n",
            "\n",
            "Epoch: [7][0/313]\tTime 0.251 (0.251)\tData 0.184 (0.184)\tLoss 0.8011 (0.8011)\tAcc@1 75.781 (75.781)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [7][100/313]\tTime 0.121 (0.124)\tData 0.002 (0.006)\tLoss 0.5470 (0.6471)\tAcc@1 81.250 (77.212)\tAcc@5 99.219 (98.639)\n",
            "Epoch: [7][200/313]\tTime 0.113 (0.123)\tData 0.003 (0.005)\tLoss 0.7949 (0.6406)\tAcc@1 70.312 (77.697)\tAcc@5 98.438 (98.745)\n",
            "Epoch: [7][300/313]\tTime 0.118 (0.123)\tData 0.012 (0.005)\tLoss 0.6277 (0.6366)\tAcc@1 78.906 (78.003)\tAcc@5 98.438 (98.754)\n",
            "Test Loss  (0.7070)\tTest Acc@1 (75.790)\tTest Acc@5 (98.660)\n",
            "\n",
            "Epoch: [8][0/313]\tTime 0.244 (0.244)\tData 0.181 (0.181)\tLoss 0.6661 (0.6661)\tAcc@1 75.781 (75.781)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [8][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.5202 (0.5800)\tAcc@1 84.375 (80.036)\tAcc@5 99.219 (99.041)\n",
            "Epoch: [8][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.5233 (0.5875)\tAcc@1 79.688 (79.656)\tAcc@5 100.000 (98.962)\n",
            "Epoch: [8][300/313]\tTime 0.130 (0.122)\tData 0.009 (0.004)\tLoss 0.6605 (0.5807)\tAcc@1 79.688 (79.877)\tAcc@5 100.000 (99.001)\n",
            "Test Loss  (0.6069)\tTest Acc@1 (79.000)\tTest Acc@5 (98.930)\n",
            "\n",
            "Epoch: [9][0/313]\tTime 0.273 (0.273)\tData 0.216 (0.216)\tLoss 0.4896 (0.4896)\tAcc@1 84.375 (84.375)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [9][100/313]\tTime 0.122 (0.124)\tData 0.002 (0.006)\tLoss 0.5936 (0.5375)\tAcc@1 76.562 (81.057)\tAcc@5 100.000 (99.025)\n",
            "Epoch: [9][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.5620 (0.5531)\tAcc@1 82.031 (80.780)\tAcc@5 97.656 (98.986)\n",
            "Epoch: [9][300/313]\tTime 0.118 (0.123)\tData 0.003 (0.005)\tLoss 0.6746 (0.5509)\tAcc@1 75.781 (80.944)\tAcc@5 98.438 (99.006)\n",
            "Test Loss  (0.6887)\tTest Acc@1 (76.370)\tTest Acc@5 (98.420)\n",
            "\n",
            "Epoch: [10][0/313]\tTime 0.215 (0.215)\tData 0.156 (0.156)\tLoss 0.5551 (0.5551)\tAcc@1 81.250 (81.250)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [10][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.5888 (0.5119)\tAcc@1 78.906 (82.488)\tAcc@5 98.438 (99.196)\n",
            "Epoch: [10][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.004)\tLoss 0.6096 (0.5152)\tAcc@1 83.594 (82.377)\tAcc@5 98.438 (99.180)\n",
            "Epoch: [10][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.6968 (0.5184)\tAcc@1 75.000 (82.197)\tAcc@5 100.000 (99.146)\n",
            "Test Loss  (0.6650)\tTest Acc@1 (77.590)\tTest Acc@5 (98.620)\n",
            "\n",
            "Epoch: [11][0/313]\tTime 0.232 (0.232)\tData 0.160 (0.160)\tLoss 0.4380 (0.4380)\tAcc@1 86.719 (86.719)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [11][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.3916 (0.4868)\tAcc@1 87.500 (83.485)\tAcc@5 98.438 (99.226)\n",
            "Epoch: [11][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.004)\tLoss 0.5661 (0.4990)\tAcc@1 79.688 (83.011)\tAcc@5 99.219 (99.227)\n",
            "Epoch: [11][300/313]\tTime 0.124 (0.123)\tData 0.002 (0.004)\tLoss 0.6691 (0.5014)\tAcc@1 79.688 (82.901)\tAcc@5 99.219 (99.175)\n",
            "Test Loss  (0.6208)\tTest Acc@1 (79.050)\tTest Acc@5 (98.440)\n",
            "\n",
            "Epoch: [12][0/313]\tTime 0.256 (0.256)\tData 0.201 (0.201)\tLoss 0.4139 (0.4139)\tAcc@1 89.062 (89.062)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [12][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.5267 (0.4816)\tAcc@1 79.688 (83.841)\tAcc@5 99.219 (99.296)\n",
            "Epoch: [12][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.4170 (0.4780)\tAcc@1 85.156 (83.524)\tAcc@5 100.000 (99.300)\n",
            "Epoch: [12][300/313]\tTime 0.121 (0.123)\tData 0.003 (0.005)\tLoss 0.5959 (0.4887)\tAcc@1 77.344 (83.176)\tAcc@5 99.219 (99.237)\n",
            "Test Loss  (0.5974)\tTest Acc@1 (79.830)\tTest Acc@5 (98.760)\n",
            "\n",
            "Epoch: [13][0/313]\tTime 0.248 (0.248)\tData 0.189 (0.189)\tLoss 0.4342 (0.4342)\tAcc@1 83.594 (83.594)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [13][100/313]\tTime 0.120 (0.123)\tData 0.003 (0.006)\tLoss 0.4908 (0.4587)\tAcc@1 85.156 (84.290)\tAcc@5 99.219 (99.288)\n",
            "Epoch: [13][200/313]\tTime 0.126 (0.123)\tData 0.002 (0.005)\tLoss 0.4087 (0.4692)\tAcc@1 81.250 (83.889)\tAcc@5 99.219 (99.258)\n",
            "Epoch: [13][300/313]\tTime 0.122 (0.123)\tData 0.002 (0.004)\tLoss 0.7294 (0.4685)\tAcc@1 72.656 (83.848)\tAcc@5 100.000 (99.278)\n",
            "Test Loss  (0.6105)\tTest Acc@1 (80.030)\tTest Acc@5 (98.690)\n",
            "\n",
            "Epoch: [14][0/313]\tTime 0.239 (0.239)\tData 0.178 (0.178)\tLoss 0.4928 (0.4928)\tAcc@1 86.719 (86.719)\tAcc@5 97.656 (97.656)\n",
            "Epoch: [14][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.005)\tLoss 0.4709 (0.4631)\tAcc@1 82.812 (83.981)\tAcc@5 100.000 (99.304)\n",
            "Epoch: [14][200/313]\tTime 0.123 (0.123)\tData 0.002 (0.005)\tLoss 0.4971 (0.4633)\tAcc@1 80.469 (83.881)\tAcc@5 98.438 (99.355)\n",
            "Epoch: [14][300/313]\tTime 0.122 (0.123)\tData 0.002 (0.004)\tLoss 0.6040 (0.4645)\tAcc@1 78.906 (83.978)\tAcc@5 100.000 (99.320)\n",
            "Test Loss  (0.5522)\tTest Acc@1 (81.560)\tTest Acc@5 (98.760)\n",
            "\n",
            "Epoch: [15][0/313]\tTime 0.253 (0.253)\tData 0.194 (0.194)\tLoss 0.4656 (0.4656)\tAcc@1 82.031 (82.031)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [15][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.4682 (0.4336)\tAcc@1 81.250 (84.568)\tAcc@5 100.000 (99.489)\n",
            "Epoch: [15][200/313]\tTime 0.124 (0.123)\tData 0.002 (0.005)\tLoss 0.5635 (0.4343)\tAcc@1 81.250 (84.783)\tAcc@5 98.438 (99.456)\n",
            "Epoch: [15][300/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.4961 (0.4408)\tAcc@1 84.375 (84.712)\tAcc@5 99.219 (99.400)\n",
            "Test Loss  (0.5740)\tTest Acc@1 (80.290)\tTest Acc@5 (98.930)\n",
            "\n",
            "Epoch: [16][0/313]\tTime 0.249 (0.249)\tData 0.186 (0.186)\tLoss 0.3966 (0.3966)\tAcc@1 89.062 (89.062)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [16][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.5739 (0.4274)\tAcc@1 79.688 (85.249)\tAcc@5 98.438 (99.482)\n",
            "Epoch: [16][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.4367 (0.4344)\tAcc@1 82.812 (85.036)\tAcc@5 100.000 (99.429)\n",
            "Epoch: [16][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.5201 (0.4350)\tAcc@1 82.031 (85.024)\tAcc@5 98.438 (99.421)\n",
            "Test Loss  (0.6998)\tTest Acc@1 (77.410)\tTest Acc@5 (98.100)\n",
            "\n",
            "Epoch: [17][0/313]\tTime 0.224 (0.224)\tData 0.159 (0.159)\tLoss 0.3103 (0.3103)\tAcc@1 89.062 (89.062)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [17][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.4086 (0.4098)\tAcc@1 89.062 (86.154)\tAcc@5 98.438 (99.489)\n",
            "Epoch: [17][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.4469 (0.4227)\tAcc@1 86.719 (85.677)\tAcc@5 98.438 (99.409)\n",
            "Epoch: [17][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.4142 (0.4238)\tAcc@1 85.938 (85.504)\tAcc@5 100.000 (99.416)\n",
            "Test Loss  (0.6230)\tTest Acc@1 (79.290)\tTest Acc@5 (98.920)\n",
            "\n",
            "Epoch: [18][0/313]\tTime 0.240 (0.240)\tData 0.174 (0.174)\tLoss 0.4459 (0.4459)\tAcc@1 85.156 (85.156)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [18][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.4520 (0.4181)\tAcc@1 83.594 (85.628)\tAcc@5 99.219 (99.451)\n",
            "Epoch: [18][200/313]\tTime 0.121 (0.123)\tData 0.003 (0.005)\tLoss 0.4533 (0.4173)\tAcc@1 82.812 (85.700)\tAcc@5 99.219 (99.448)\n",
            "Epoch: [18][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.3962 (0.4221)\tAcc@1 85.938 (85.714)\tAcc@5 99.219 (99.411)\n",
            "Test Loss  (0.5489)\tTest Acc@1 (81.910)\tTest Acc@5 (99.000)\n",
            "\n",
            "Epoch: [19][0/313]\tTime 0.344 (0.344)\tData 0.254 (0.254)\tLoss 0.3265 (0.3265)\tAcc@1 86.719 (86.719)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [19][100/313]\tTime 0.121 (0.124)\tData 0.003 (0.007)\tLoss 0.5109 (0.4178)\tAcc@1 82.031 (85.319)\tAcc@5 99.219 (99.443)\n",
            "Epoch: [19][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.4489 (0.4134)\tAcc@1 84.375 (85.708)\tAcc@5 100.000 (99.471)\n",
            "Epoch: [19][300/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.4084 (0.4162)\tAcc@1 85.938 (85.717)\tAcc@5 100.000 (99.442)\n",
            "Test Loss  (0.5103)\tTest Acc@1 (82.080)\tTest Acc@5 (99.150)\n",
            "\n",
            "Epoch: [20][0/313]\tTime 0.353 (0.353)\tData 0.285 (0.285)\tLoss 0.3493 (0.3493)\tAcc@1 87.500 (87.500)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [20][100/313]\tTime 0.122 (0.124)\tData 0.002 (0.006)\tLoss 0.3564 (0.3997)\tAcc@1 89.062 (86.262)\tAcc@5 100.000 (99.497)\n",
            "Epoch: [20][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.5722 (0.4005)\tAcc@1 82.031 (86.447)\tAcc@5 99.219 (99.495)\n",
            "Epoch: [20][300/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.4895 (0.4032)\tAcc@1 82.031 (86.285)\tAcc@5 100.000 (99.512)\n",
            "Test Loss  (0.8276)\tTest Acc@1 (75.110)\tTest Acc@5 (98.300)\n",
            "\n",
            "Epoch: [21][0/313]\tTime 0.331 (0.331)\tData 0.273 (0.273)\tLoss 0.3398 (0.3398)\tAcc@1 86.719 (86.719)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [21][100/313]\tTime 0.122 (0.124)\tData 0.009 (0.006)\tLoss 0.3720 (0.4002)\tAcc@1 88.281 (86.278)\tAcc@5 100.000 (99.505)\n",
            "Epoch: [21][200/313]\tTime 0.116 (0.123)\tData 0.002 (0.005)\tLoss 0.4186 (0.3989)\tAcc@1 85.156 (86.318)\tAcc@5 100.000 (99.456)\n",
            "Epoch: [21][300/313]\tTime 0.121 (0.123)\tData 0.002 (0.004)\tLoss 0.2785 (0.3995)\tAcc@1 92.188 (86.257)\tAcc@5 100.000 (99.473)\n",
            "Test Loss  (0.5461)\tTest Acc@1 (81.560)\tTest Acc@5 (98.940)\n",
            "\n",
            "Epoch: [22][0/313]\tTime 0.402 (0.402)\tData 0.303 (0.303)\tLoss 0.3154 (0.3154)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [22][100/313]\tTime 0.121 (0.124)\tData 0.002 (0.006)\tLoss 0.2524 (0.3900)\tAcc@1 90.625 (86.812)\tAcc@5 100.000 (99.451)\n",
            "Epoch: [22][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.4884 (0.3939)\tAcc@1 84.375 (86.703)\tAcc@5 99.219 (99.502)\n",
            "Epoch: [22][300/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.3939 (0.3921)\tAcc@1 88.281 (86.747)\tAcc@5 100.000 (99.491)\n",
            "Test Loss  (0.6014)\tTest Acc@1 (79.820)\tTest Acc@5 (98.550)\n",
            "\n",
            "Epoch: [23][0/313]\tTime 0.258 (0.258)\tData 0.195 (0.195)\tLoss 0.4579 (0.4579)\tAcc@1 83.594 (83.594)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [23][100/313]\tTime 0.127 (0.123)\tData 0.009 (0.005)\tLoss 0.5187 (0.3860)\tAcc@1 83.594 (86.757)\tAcc@5 99.219 (99.536)\n",
            "Epoch: [23][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.3115 (0.3870)\tAcc@1 89.844 (86.905)\tAcc@5 98.438 (99.522)\n",
            "Epoch: [23][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.5192 (0.3900)\tAcc@1 85.938 (86.776)\tAcc@5 98.438 (99.489)\n",
            "Test Loss  (0.5755)\tTest Acc@1 (80.260)\tTest Acc@5 (99.140)\n",
            "\n",
            "Epoch: [24][0/313]\tTime 0.219 (0.219)\tData 0.154 (0.154)\tLoss 0.4395 (0.4395)\tAcc@1 83.594 (83.594)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [24][100/313]\tTime 0.127 (0.123)\tData 0.011 (0.005)\tLoss 0.3549 (0.3654)\tAcc@1 87.500 (87.376)\tAcc@5 100.000 (99.559)\n",
            "Epoch: [24][200/313]\tTime 0.126 (0.122)\tData 0.003 (0.004)\tLoss 0.3640 (0.3769)\tAcc@1 86.719 (87.034)\tAcc@5 100.000 (99.549)\n",
            "Epoch: [24][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.2558 (0.3848)\tAcc@1 92.188 (86.789)\tAcc@5 100.000 (99.491)\n",
            "Test Loss  (0.5490)\tTest Acc@1 (81.690)\tTest Acc@5 (98.990)\n",
            "\n",
            "Epoch: [25][0/313]\tTime 0.224 (0.224)\tData 0.162 (0.162)\tLoss 0.2720 (0.2720)\tAcc@1 92.188 (92.188)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [25][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.3806 (0.3788)\tAcc@1 85.156 (87.075)\tAcc@5 100.000 (99.513)\n",
            "Epoch: [25][200/313]\tTime 0.128 (0.122)\tData 0.009 (0.004)\tLoss 0.2314 (0.3778)\tAcc@1 90.625 (87.053)\tAcc@5 100.000 (99.541)\n",
            "Epoch: [25][300/313]\tTime 0.121 (0.122)\tData 0.009 (0.004)\tLoss 0.4208 (0.3844)\tAcc@1 83.594 (86.955)\tAcc@5 100.000 (99.489)\n",
            "Test Loss  (0.5688)\tTest Acc@1 (80.730)\tTest Acc@5 (99.060)\n",
            "\n",
            "Epoch: [26][0/313]\tTime 0.220 (0.220)\tData 0.155 (0.155)\tLoss 0.3138 (0.3138)\tAcc@1 85.938 (85.938)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [26][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.3806 (0.3626)\tAcc@1 87.500 (87.670)\tAcc@5 98.438 (99.567)\n",
            "Epoch: [26][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.004)\tLoss 0.4064 (0.3690)\tAcc@1 86.719 (87.360)\tAcc@5 100.000 (99.569)\n",
            "Epoch: [26][300/313]\tTime 0.130 (0.122)\tData 0.003 (0.004)\tLoss 0.4266 (0.3714)\tAcc@1 86.719 (87.329)\tAcc@5 99.219 (99.567)\n",
            "Test Loss  (0.4689)\tTest Acc@1 (84.140)\tTest Acc@5 (99.320)\n",
            "\n",
            "Epoch: [27][0/313]\tTime 0.245 (0.245)\tData 0.187 (0.187)\tLoss 0.3548 (0.3548)\tAcc@1 88.281 (88.281)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [27][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.2984 (0.3747)\tAcc@1 87.500 (87.082)\tAcc@5 100.000 (99.621)\n",
            "Epoch: [27][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.2500 (0.3747)\tAcc@1 91.406 (87.065)\tAcc@5 100.000 (99.580)\n",
            "Epoch: [27][300/313]\tTime 0.121 (0.122)\tData 0.014 (0.004)\tLoss 0.2980 (0.3752)\tAcc@1 89.062 (86.942)\tAcc@5 100.000 (99.574)\n",
            "Test Loss  (0.6423)\tTest Acc@1 (78.860)\tTest Acc@5 (98.210)\n",
            "\n",
            "Epoch: [28][0/313]\tTime 0.224 (0.224)\tData 0.155 (0.155)\tLoss 0.3862 (0.3862)\tAcc@1 85.156 (85.156)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [28][100/313]\tTime 0.121 (0.124)\tData 0.002 (0.006)\tLoss 0.2833 (0.3551)\tAcc@1 92.188 (87.655)\tAcc@5 100.000 (99.559)\n",
            "Epoch: [28][200/313]\tTime 0.112 (0.123)\tData 0.009 (0.005)\tLoss 0.3085 (0.3555)\tAcc@1 89.844 (87.644)\tAcc@5 100.000 (99.615)\n",
            "Epoch: [28][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.2773 (0.3641)\tAcc@1 92.969 (87.409)\tAcc@5 100.000 (99.611)\n",
            "Test Loss  (0.7140)\tTest Acc@1 (77.930)\tTest Acc@5 (98.810)\n",
            "\n",
            "Epoch: [29][0/313]\tTime 0.240 (0.240)\tData 0.177 (0.177)\tLoss 0.2446 (0.2446)\tAcc@1 92.188 (92.188)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [29][100/313]\tTime 0.124 (0.123)\tData 0.002 (0.005)\tLoss 0.3170 (0.3712)\tAcc@1 89.844 (86.982)\tAcc@5 100.000 (99.590)\n",
            "Epoch: [29][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.4741 (0.3721)\tAcc@1 85.938 (87.088)\tAcc@5 99.219 (99.557)\n",
            "Epoch: [29][300/313]\tTime 0.135 (0.122)\tData 0.014 (0.004)\tLoss 0.3712 (0.3750)\tAcc@1 87.500 (87.082)\tAcc@5 99.219 (99.548)\n",
            "Test Loss  (0.4650)\tTest Acc@1 (84.430)\tTest Acc@5 (99.340)\n",
            "\n",
            "Epoch: [30][0/313]\tTime 0.266 (0.266)\tData 0.212 (0.212)\tLoss 0.2966 (0.2966)\tAcc@1 88.281 (88.281)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [30][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.2800 (0.3397)\tAcc@1 91.406 (88.011)\tAcc@5 99.219 (99.590)\n",
            "Epoch: [30][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.2913 (0.3458)\tAcc@1 89.062 (88.040)\tAcc@5 98.438 (99.607)\n",
            "Epoch: [30][300/313]\tTime 0.129 (0.122)\tData 0.002 (0.004)\tLoss 0.3649 (0.3516)\tAcc@1 85.156 (87.915)\tAcc@5 99.219 (99.546)\n",
            "Test Loss  (0.6377)\tTest Acc@1 (79.540)\tTest Acc@5 (98.710)\n",
            "\n",
            "Epoch: [31][0/313]\tTime 0.229 (0.229)\tData 0.164 (0.164)\tLoss 0.3783 (0.3783)\tAcc@1 87.500 (87.500)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [31][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.3082 (0.3524)\tAcc@1 88.281 (88.142)\tAcc@5 99.219 (99.575)\n",
            "Epoch: [31][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.4709 (0.3601)\tAcc@1 82.812 (87.815)\tAcc@5 98.438 (99.553)\n",
            "Epoch: [31][300/313]\tTime 0.124 (0.122)\tData 0.002 (0.004)\tLoss 0.4487 (0.3712)\tAcc@1 86.719 (87.339)\tAcc@5 96.875 (99.512)\n",
            "Test Loss  (0.5794)\tTest Acc@1 (81.000)\tTest Acc@5 (99.100)\n",
            "\n",
            "Epoch: [32][0/313]\tTime 0.217 (0.217)\tData 0.155 (0.155)\tLoss 0.4048 (0.4048)\tAcc@1 89.844 (89.844)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [32][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.1868 (0.3355)\tAcc@1 96.094 (88.382)\tAcc@5 100.000 (99.660)\n",
            "Epoch: [32][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.3079 (0.3455)\tAcc@1 86.719 (88.130)\tAcc@5 99.219 (99.545)\n",
            "Epoch: [32][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.2435 (0.3510)\tAcc@1 92.969 (87.993)\tAcc@5 100.000 (99.543)\n",
            "Test Loss  (0.4714)\tTest Acc@1 (84.330)\tTest Acc@5 (99.200)\n",
            "\n",
            "Epoch: [33][0/313]\tTime 0.229 (0.229)\tData 0.172 (0.172)\tLoss 0.3514 (0.3514)\tAcc@1 87.500 (87.500)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [33][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.3434 (0.3276)\tAcc@1 90.625 (88.970)\tAcc@5 99.219 (99.520)\n",
            "Epoch: [33][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.4032 (0.3507)\tAcc@1 85.938 (87.978)\tAcc@5 98.438 (99.487)\n",
            "Epoch: [33][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.4973 (0.3573)\tAcc@1 83.594 (87.734)\tAcc@5 99.219 (99.525)\n",
            "Test Loss  (0.6222)\tTest Acc@1 (79.390)\tTest Acc@5 (98.930)\n",
            "\n",
            "Epoch: [34][0/313]\tTime 0.231 (0.231)\tData 0.161 (0.161)\tLoss 0.3723 (0.3723)\tAcc@1 87.500 (87.500)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [34][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.2675 (0.3437)\tAcc@1 90.625 (88.165)\tAcc@5 100.000 (99.613)\n",
            "Epoch: [34][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.4588 (0.3443)\tAcc@1 85.156 (88.301)\tAcc@5 99.219 (99.580)\n",
            "Epoch: [34][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.3269 (0.3505)\tAcc@1 90.625 (88.048)\tAcc@5 99.219 (99.598)\n",
            "Test Loss  (0.5197)\tTest Acc@1 (83.010)\tTest Acc@5 (98.650)\n",
            "\n",
            "Epoch: [35][0/313]\tTime 0.223 (0.223)\tData 0.161 (0.161)\tLoss 0.2541 (0.2541)\tAcc@1 92.188 (92.188)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [35][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.3374 (0.3257)\tAcc@1 83.594 (88.660)\tAcc@5 100.000 (99.683)\n",
            "Epoch: [35][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.5068 (0.3387)\tAcc@1 82.031 (88.277)\tAcc@5 100.000 (99.576)\n",
            "Epoch: [35][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.3262 (0.3461)\tAcc@1 87.500 (88.144)\tAcc@5 100.000 (99.572)\n",
            "Test Loss  (0.5099)\tTest Acc@1 (83.710)\tTest Acc@5 (99.180)\n",
            "\n",
            "Epoch: [36][0/313]\tTime 0.229 (0.229)\tData 0.159 (0.159)\tLoss 0.3385 (0.3385)\tAcc@1 89.062 (89.062)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [36][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.3760 (0.3539)\tAcc@1 85.156 (87.724)\tAcc@5 100.000 (99.706)\n",
            "Epoch: [36][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.3519 (0.3520)\tAcc@1 85.938 (87.990)\tAcc@5 100.000 (99.619)\n",
            "Epoch: [36][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.3026 (0.3468)\tAcc@1 89.062 (88.170)\tAcc@5 100.000 (99.634)\n",
            "Test Loss  (0.5428)\tTest Acc@1 (82.270)\tTest Acc@5 (98.810)\n",
            "\n",
            "Epoch: [37][0/313]\tTime 0.260 (0.260)\tData 0.204 (0.204)\tLoss 0.2930 (0.2930)\tAcc@1 90.625 (90.625)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [37][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.2827 (0.3328)\tAcc@1 89.062 (88.575)\tAcc@5 100.000 (99.536)\n",
            "Epoch: [37][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.4365 (0.3429)\tAcc@1 87.500 (88.227)\tAcc@5 98.438 (99.588)\n",
            "Epoch: [37][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.005)\tLoss 0.2929 (0.3499)\tAcc@1 87.500 (87.946)\tAcc@5 100.000 (99.582)\n",
            "Test Loss  (0.4285)\tTest Acc@1 (85.500)\tTest Acc@5 (99.380)\n",
            "\n",
            "Epoch: [38][0/313]\tTime 0.259 (0.259)\tData 0.182 (0.182)\tLoss 0.4591 (0.4591)\tAcc@1 85.156 (85.156)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [38][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.5145 (0.3285)\tAcc@1 81.250 (88.877)\tAcc@5 99.219 (99.652)\n",
            "Epoch: [38][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.3753 (0.3404)\tAcc@1 89.844 (88.429)\tAcc@5 99.219 (99.646)\n",
            "Epoch: [38][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.4168 (0.3427)\tAcc@1 85.156 (88.382)\tAcc@5 100.000 (99.631)\n",
            "Test Loss  (0.6450)\tTest Acc@1 (78.390)\tTest Acc@5 (98.470)\n",
            "\n",
            "Epoch: [39][0/313]\tTime 0.323 (0.323)\tData 0.263 (0.263)\tLoss 0.4707 (0.4707)\tAcc@1 81.250 (81.250)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [39][100/313]\tTime 0.120 (0.124)\tData 0.002 (0.006)\tLoss 0.3090 (0.3351)\tAcc@1 89.062 (88.668)\tAcc@5 100.000 (99.660)\n",
            "Epoch: [39][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.004)\tLoss 0.4011 (0.3418)\tAcc@1 83.594 (88.161)\tAcc@5 99.219 (99.642)\n",
            "Epoch: [39][300/313]\tTime 0.123 (0.123)\tData 0.002 (0.004)\tLoss 0.2743 (0.3489)\tAcc@1 89.062 (87.957)\tAcc@5 100.000 (99.611)\n",
            "Test Loss  (0.5488)\tTest Acc@1 (82.350)\tTest Acc@5 (99.060)\n",
            "\n",
            "Epoch: [40][0/313]\tTime 0.272 (0.272)\tData 0.185 (0.185)\tLoss 0.3336 (0.3336)\tAcc@1 88.281 (88.281)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [40][100/313]\tTime 0.126 (0.124)\tData 0.014 (0.006)\tLoss 0.2902 (0.3337)\tAcc@1 91.406 (88.575)\tAcc@5 100.000 (99.691)\n",
            "Epoch: [40][200/313]\tTime 0.113 (0.123)\tData 0.010 (0.006)\tLoss 0.4082 (0.3442)\tAcc@1 86.719 (88.219)\tAcc@5 99.219 (99.631)\n",
            "Epoch: [40][300/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.4604 (0.3505)\tAcc@1 83.594 (88.014)\tAcc@5 100.000 (99.611)\n",
            "Test Loss  (0.6099)\tTest Acc@1 (80.730)\tTest Acc@5 (98.460)\n",
            "\n",
            "Epoch: [41][0/313]\tTime 0.217 (0.217)\tData 0.162 (0.162)\tLoss 0.2975 (0.2975)\tAcc@1 90.625 (90.625)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [41][100/313]\tTime 0.123 (0.122)\tData 0.016 (0.005)\tLoss 0.2959 (0.3320)\tAcc@1 89.844 (88.691)\tAcc@5 100.000 (99.667)\n",
            "Epoch: [41][200/313]\tTime 0.123 (0.122)\tData 0.010 (0.004)\tLoss 0.3389 (0.3309)\tAcc@1 88.281 (88.732)\tAcc@5 100.000 (99.658)\n",
            "Epoch: [41][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.4185 (0.3351)\tAcc@1 84.375 (88.616)\tAcc@5 100.000 (99.616)\n",
            "Test Loss  (0.5502)\tTest Acc@1 (81.830)\tTest Acc@5 (98.830)\n",
            "\n",
            "Epoch: [42][0/313]\tTime 0.228 (0.228)\tData 0.159 (0.159)\tLoss 0.2879 (0.2879)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [42][100/313]\tTime 0.118 (0.123)\tData 0.002 (0.005)\tLoss 0.3523 (0.3277)\tAcc@1 88.281 (88.598)\tAcc@5 100.000 (99.660)\n",
            "Epoch: [42][200/313]\tTime 0.124 (0.122)\tData 0.011 (0.004)\tLoss 0.2681 (0.3357)\tAcc@1 89.062 (88.460)\tAcc@5 100.000 (99.631)\n",
            "Epoch: [42][300/313]\tTime 0.121 (0.122)\tData 0.012 (0.004)\tLoss 0.3231 (0.3363)\tAcc@1 87.500 (88.481)\tAcc@5 100.000 (99.608)\n",
            "Test Loss  (0.5421)\tTest Acc@1 (81.720)\tTest Acc@5 (98.810)\n",
            "\n",
            "Epoch: [43][0/313]\tTime 0.229 (0.229)\tData 0.162 (0.162)\tLoss 0.3091 (0.3091)\tAcc@1 88.281 (88.281)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [43][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.3667 (0.3241)\tAcc@1 88.281 (88.954)\tAcc@5 99.219 (99.683)\n",
            "Epoch: [43][200/313]\tTime 0.128 (0.122)\tData 0.002 (0.005)\tLoss 0.4145 (0.3314)\tAcc@1 88.281 (88.717)\tAcc@5 100.000 (99.650)\n",
            "Epoch: [43][300/313]\tTime 0.122 (0.122)\tData 0.007 (0.004)\tLoss 0.3520 (0.3359)\tAcc@1 88.281 (88.551)\tAcc@5 100.000 (99.577)\n",
            "Test Loss  (0.6809)\tTest Acc@1 (78.980)\tTest Acc@5 (97.890)\n",
            "\n",
            "Epoch: [44][0/313]\tTime 0.216 (0.216)\tData 0.162 (0.162)\tLoss 0.3540 (0.3540)\tAcc@1 89.844 (89.844)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [44][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.3546 (0.3150)\tAcc@1 86.719 (89.155)\tAcc@5 100.000 (99.660)\n",
            "Epoch: [44][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.3910 (0.3296)\tAcc@1 86.719 (88.713)\tAcc@5 99.219 (99.592)\n",
            "Epoch: [44][300/313]\tTime 0.120 (0.122)\tData 0.011 (0.004)\tLoss 0.2529 (0.3344)\tAcc@1 92.188 (88.577)\tAcc@5 99.219 (99.605)\n",
            "Test Loss  (0.4275)\tTest Acc@1 (85.450)\tTest Acc@5 (99.340)\n",
            "\n",
            "Epoch: [45][0/313]\tTime 0.231 (0.231)\tData 0.174 (0.174)\tLoss 0.3698 (0.3698)\tAcc@1 87.500 (87.500)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [45][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.2849 (0.3171)\tAcc@1 89.844 (89.295)\tAcc@5 100.000 (99.691)\n",
            "Epoch: [45][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.3087 (0.3239)\tAcc@1 85.938 (89.020)\tAcc@5 99.219 (99.631)\n",
            "Epoch: [45][300/313]\tTime 0.133 (0.122)\tData 0.014 (0.004)\tLoss 0.5468 (0.3289)\tAcc@1 82.031 (88.870)\tAcc@5 100.000 (99.590)\n",
            "Test Loss  (0.4520)\tTest Acc@1 (84.800)\tTest Acc@5 (99.290)\n",
            "\n",
            "Epoch: [46][0/313]\tTime 0.216 (0.216)\tData 0.155 (0.155)\tLoss 0.3395 (0.3395)\tAcc@1 89.062 (89.062)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [46][100/313]\tTime 0.124 (0.123)\tData 0.002 (0.006)\tLoss 0.3539 (0.3159)\tAcc@1 87.500 (89.387)\tAcc@5 99.219 (99.722)\n",
            "Epoch: [46][200/313]\tTime 0.137 (0.122)\tData 0.008 (0.005)\tLoss 0.2973 (0.3289)\tAcc@1 90.625 (88.845)\tAcc@5 100.000 (99.650)\n",
            "Epoch: [46][300/313]\tTime 0.125 (0.122)\tData 0.002 (0.005)\tLoss 0.3536 (0.3292)\tAcc@1 87.500 (88.831)\tAcc@5 97.656 (99.639)\n",
            "Test Loss  (0.5840)\tTest Acc@1 (80.790)\tTest Acc@5 (99.030)\n",
            "\n",
            "Epoch: [47][0/313]\tTime 0.209 (0.209)\tData 0.155 (0.155)\tLoss 0.2195 (0.2195)\tAcc@1 91.406 (91.406)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [47][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.2823 (0.3163)\tAcc@1 91.406 (89.093)\tAcc@5 100.000 (99.698)\n",
            "Epoch: [47][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.3526 (0.3273)\tAcc@1 88.281 (88.771)\tAcc@5 100.000 (99.658)\n",
            "Epoch: [47][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.2088 (0.3268)\tAcc@1 93.750 (88.870)\tAcc@5 100.000 (99.676)\n",
            "Test Loss  (0.4412)\tTest Acc@1 (85.590)\tTest Acc@5 (99.320)\n",
            "\n",
            "Epoch: [48][0/313]\tTime 0.240 (0.240)\tData 0.180 (0.180)\tLoss 0.2637 (0.2637)\tAcc@1 91.406 (91.406)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [48][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.2815 (0.3174)\tAcc@1 91.406 (88.962)\tAcc@5 100.000 (99.737)\n",
            "Epoch: [48][200/313]\tTime 0.119 (0.122)\tData 0.002 (0.005)\tLoss 0.2988 (0.3325)\tAcc@1 90.625 (88.705)\tAcc@5 100.000 (99.681)\n",
            "Epoch: [48][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.2875 (0.3378)\tAcc@1 89.844 (88.453)\tAcc@5 100.000 (99.650)\n",
            "Test Loss  (0.6286)\tTest Acc@1 (79.230)\tTest Acc@5 (98.620)\n",
            "\n",
            "Epoch: [49][0/313]\tTime 0.232 (0.232)\tData 0.171 (0.171)\tLoss 0.2832 (0.2832)\tAcc@1 94.531 (94.531)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [49][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.2685 (0.3070)\tAcc@1 89.062 (89.588)\tAcc@5 100.000 (99.698)\n",
            "Epoch: [49][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.2072 (0.3245)\tAcc@1 92.969 (88.895)\tAcc@5 100.000 (99.623)\n",
            "Epoch: [49][300/313]\tTime 0.121 (0.122)\tData 0.006 (0.004)\tLoss 0.1962 (0.3283)\tAcc@1 93.750 (88.735)\tAcc@5 100.000 (99.647)\n",
            "Test Loss  (0.4746)\tTest Acc@1 (84.010)\tTest Acc@5 (99.270)\n",
            "\n",
            "Epoch: [50][0/313]\tTime 0.213 (0.213)\tData 0.158 (0.158)\tLoss 0.2413 (0.2413)\tAcc@1 90.625 (90.625)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [50][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.2794 (0.3145)\tAcc@1 91.406 (89.333)\tAcc@5 99.219 (99.706)\n",
            "Epoch: [50][200/313]\tTime 0.123 (0.122)\tData 0.002 (0.005)\tLoss 0.3234 (0.3249)\tAcc@1 89.844 (88.942)\tAcc@5 98.438 (99.666)\n",
            "Epoch: [50][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.3608 (0.3282)\tAcc@1 88.281 (88.813)\tAcc@5 99.219 (99.644)\n",
            "Test Loss  (0.5590)\tTest Acc@1 (81.520)\tTest Acc@5 (98.440)\n",
            "\n",
            "Epoch: [51][0/313]\tTime 0.223 (0.223)\tData 0.158 (0.158)\tLoss 0.2962 (0.2962)\tAcc@1 90.625 (90.625)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [51][100/313]\tTime 0.117 (0.123)\tData 0.002 (0.006)\tLoss 0.4111 (0.3109)\tAcc@1 85.156 (89.256)\tAcc@5 99.219 (99.691)\n",
            "Epoch: [51][200/313]\tTime 0.123 (0.122)\tData 0.002 (0.005)\tLoss 0.2948 (0.3194)\tAcc@1 89.844 (89.047)\tAcc@5 100.000 (99.642)\n",
            "Epoch: [51][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.2347 (0.3210)\tAcc@1 91.406 (88.943)\tAcc@5 100.000 (99.631)\n",
            "Test Loss  (0.5376)\tTest Acc@1 (82.060)\tTest Acc@5 (98.970)\n",
            "\n",
            "Epoch: [52][0/313]\tTime 0.236 (0.236)\tData 0.180 (0.180)\tLoss 0.2870 (0.2870)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [52][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.5837 (0.3338)\tAcc@1 78.906 (88.776)\tAcc@5 100.000 (99.683)\n",
            "Epoch: [52][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.3188 (0.3288)\tAcc@1 90.625 (88.833)\tAcc@5 99.219 (99.693)\n",
            "Epoch: [52][300/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.3015 (0.3308)\tAcc@1 90.625 (88.725)\tAcc@5 99.219 (99.644)\n",
            "Test Loss  (0.4259)\tTest Acc@1 (85.270)\tTest Acc@5 (99.460)\n",
            "\n",
            "Epoch: [53][0/313]\tTime 0.218 (0.218)\tData 0.160 (0.160)\tLoss 0.2395 (0.2395)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [53][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.1839 (0.3200)\tAcc@1 94.531 (89.202)\tAcc@5 100.000 (99.691)\n",
            "Epoch: [53][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.2456 (0.3245)\tAcc@1 92.969 (89.109)\tAcc@5 100.000 (99.646)\n",
            "Epoch: [53][300/313]\tTime 0.119 (0.122)\tData 0.002 (0.004)\tLoss 0.2926 (0.3251)\tAcc@1 89.844 (88.912)\tAcc@5 100.000 (99.678)\n",
            "Test Loss  (0.4499)\tTest Acc@1 (85.020)\tTest Acc@5 (99.320)\n",
            "\n",
            "Epoch: [54][0/313]\tTime 0.225 (0.225)\tData 0.164 (0.164)\tLoss 0.2539 (0.2539)\tAcc@1 92.188 (92.188)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [54][100/313]\tTime 0.118 (0.123)\tData 0.002 (0.006)\tLoss 0.2715 (0.3112)\tAcc@1 92.188 (89.380)\tAcc@5 99.219 (99.621)\n",
            "Epoch: [54][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.3179 (0.3181)\tAcc@1 89.062 (89.179)\tAcc@5 100.000 (99.674)\n",
            "Epoch: [54][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.3259 (0.3181)\tAcc@1 89.844 (89.133)\tAcc@5 99.219 (99.668)\n",
            "Test Loss  (0.5906)\tTest Acc@1 (81.190)\tTest Acc@5 (98.450)\n",
            "\n",
            "Epoch: [55][0/313]\tTime 0.229 (0.229)\tData 0.169 (0.169)\tLoss 0.2879 (0.2879)\tAcc@1 92.188 (92.188)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [55][100/313]\tTime 0.125 (0.123)\tData 0.002 (0.006)\tLoss 0.2870 (0.3285)\tAcc@1 91.406 (88.683)\tAcc@5 100.000 (99.675)\n",
            "Epoch: [55][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.3473 (0.3277)\tAcc@1 90.625 (88.705)\tAcc@5 98.438 (99.615)\n",
            "Epoch: [55][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.3053 (0.3245)\tAcc@1 90.625 (88.839)\tAcc@5 99.219 (99.650)\n",
            "Test Loss  (0.4621)\tTest Acc@1 (84.500)\tTest Acc@5 (99.240)\n",
            "\n",
            "Epoch: [56][0/313]\tTime 0.265 (0.265)\tData 0.171 (0.171)\tLoss 0.3238 (0.3238)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [56][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.2379 (0.3204)\tAcc@1 91.406 (89.101)\tAcc@5 100.000 (99.660)\n",
            "Epoch: [56][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.2443 (0.3151)\tAcc@1 92.188 (89.335)\tAcc@5 100.000 (99.670)\n",
            "Epoch: [56][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.2383 (0.3143)\tAcc@1 91.406 (89.309)\tAcc@5 100.000 (99.676)\n",
            "Test Loss  (0.5177)\tTest Acc@1 (83.300)\tTest Acc@5 (99.190)\n",
            "\n",
            "Epoch: [57][0/313]\tTime 0.263 (0.263)\tData 0.201 (0.201)\tLoss 0.3366 (0.3366)\tAcc@1 89.062 (89.062)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [57][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.3205 (0.3176)\tAcc@1 87.500 (89.194)\tAcc@5 99.219 (99.714)\n",
            "Epoch: [57][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.2547 (0.3250)\tAcc@1 92.188 (89.008)\tAcc@5 100.000 (99.689)\n",
            "Epoch: [57][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.2552 (0.3220)\tAcc@1 90.625 (89.005)\tAcc@5 100.000 (99.657)\n",
            "Test Loss  (0.9851)\tTest Acc@1 (69.190)\tTest Acc@5 (98.380)\n",
            "\n",
            "Epoch: [58][0/313]\tTime 0.310 (0.310)\tData 0.255 (0.255)\tLoss 0.3895 (0.3895)\tAcc@1 85.156 (85.156)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [58][100/313]\tTime 0.120 (0.124)\tData 0.002 (0.006)\tLoss 0.3991 (0.3235)\tAcc@1 85.938 (89.078)\tAcc@5 99.219 (99.590)\n",
            "Epoch: [58][200/313]\tTime 0.125 (0.123)\tData 0.002 (0.005)\tLoss 0.5012 (0.3220)\tAcc@1 83.594 (88.954)\tAcc@5 97.656 (99.650)\n",
            "Epoch: [58][300/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.3164 (0.3219)\tAcc@1 90.625 (88.920)\tAcc@5 100.000 (99.676)\n",
            "Test Loss  (0.5949)\tTest Acc@1 (81.580)\tTest Acc@5 (98.970)\n",
            "\n",
            "Epoch: [59][0/313]\tTime 0.269 (0.269)\tData 0.210 (0.210)\tLoss 0.1912 (0.1912)\tAcc@1 92.188 (92.188)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [59][100/313]\tTime 0.133 (0.123)\tData 0.002 (0.005)\tLoss 0.3156 (0.3075)\tAcc@1 89.844 (89.735)\tAcc@5 100.000 (99.667)\n",
            "Epoch: [59][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.004)\tLoss 0.2798 (0.3195)\tAcc@1 89.062 (89.269)\tAcc@5 99.219 (99.666)\n",
            "Epoch: [59][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.3664 (0.3215)\tAcc@1 85.938 (89.135)\tAcc@5 100.000 (99.673)\n",
            "Test Loss  (0.5069)\tTest Acc@1 (83.010)\tTest Acc@5 (99.020)\n",
            "\n",
            "Epoch: [60][0/313]\tTime 0.366 (0.366)\tData 0.301 (0.301)\tLoss 0.4369 (0.4369)\tAcc@1 89.062 (89.062)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [60][100/313]\tTime 0.121 (0.124)\tData 0.009 (0.006)\tLoss 0.2851 (0.2922)\tAcc@1 89.844 (90.192)\tAcc@5 100.000 (99.698)\n",
            "Epoch: [60][200/313]\tTime 0.123 (0.123)\tData 0.002 (0.005)\tLoss 0.2536 (0.3039)\tAcc@1 94.531 (89.735)\tAcc@5 100.000 (99.728)\n",
            "Epoch: [60][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.2229 (0.3137)\tAcc@1 92.969 (89.449)\tAcc@5 100.000 (99.699)\n",
            "Test Loss  (0.4801)\tTest Acc@1 (84.420)\tTest Acc@5 (99.290)\n",
            "\n",
            "Epoch: [61][0/313]\tTime 0.214 (0.214)\tData 0.152 (0.152)\tLoss 0.3581 (0.3581)\tAcc@1 87.500 (87.500)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [61][100/313]\tTime 0.108 (0.122)\tData 0.002 (0.005)\tLoss 0.4468 (0.3133)\tAcc@1 83.594 (89.310)\tAcc@5 99.219 (99.691)\n",
            "Epoch: [61][200/313]\tTime 0.122 (0.122)\tData 0.010 (0.004)\tLoss 0.3762 (0.3171)\tAcc@1 85.938 (89.269)\tAcc@5 99.219 (99.685)\n",
            "Epoch: [61][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.2988 (0.3171)\tAcc@1 91.406 (89.205)\tAcc@5 99.219 (99.689)\n",
            "Test Loss  (0.5007)\tTest Acc@1 (83.810)\tTest Acc@5 (99.310)\n",
            "\n",
            "Epoch: [62][0/313]\tTime 0.228 (0.228)\tData 0.174 (0.174)\tLoss 0.2039 (0.2039)\tAcc@1 93.750 (93.750)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [62][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.3144 (0.3034)\tAcc@1 88.281 (89.527)\tAcc@5 100.000 (99.760)\n",
            "Epoch: [62][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.1598 (0.3110)\tAcc@1 95.312 (89.319)\tAcc@5 100.000 (99.720)\n",
            "Epoch: [62][300/313]\tTime 0.122 (0.122)\tData 0.004 (0.004)\tLoss 0.3009 (0.3152)\tAcc@1 89.844 (89.216)\tAcc@5 100.000 (99.683)\n",
            "Test Loss  (0.5483)\tTest Acc@1 (81.820)\tTest Acc@5 (98.870)\n",
            "\n",
            "Epoch: [63][0/313]\tTime 0.214 (0.214)\tData 0.155 (0.155)\tLoss 0.5052 (0.5052)\tAcc@1 80.469 (80.469)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [63][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.3456 (0.3103)\tAcc@1 86.719 (89.194)\tAcc@5 100.000 (99.722)\n",
            "Epoch: [63][200/313]\tTime 0.128 (0.122)\tData 0.003 (0.004)\tLoss 0.2202 (0.3104)\tAcc@1 92.969 (89.338)\tAcc@5 99.219 (99.689)\n",
            "Epoch: [63][300/313]\tTime 0.124 (0.122)\tData 0.005 (0.004)\tLoss 0.2073 (0.3164)\tAcc@1 92.969 (89.122)\tAcc@5 100.000 (99.660)\n",
            "Test Loss  (0.5836)\tTest Acc@1 (81.230)\tTest Acc@5 (98.630)\n",
            "\n",
            "Epoch: [64][0/313]\tTime 0.226 (0.226)\tData 0.160 (0.160)\tLoss 0.3601 (0.3601)\tAcc@1 85.938 (85.938)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [64][100/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.3866 (0.3169)\tAcc@1 86.719 (89.055)\tAcc@5 100.000 (99.729)\n",
            "Epoch: [64][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.2645 (0.3166)\tAcc@1 89.844 (89.008)\tAcc@5 100.000 (99.716)\n",
            "Epoch: [64][300/313]\tTime 0.119 (0.122)\tData 0.017 (0.004)\tLoss 0.3685 (0.3197)\tAcc@1 89.062 (88.857)\tAcc@5 99.219 (99.694)\n",
            "Test Loss  (0.5409)\tTest Acc@1 (82.490)\tTest Acc@5 (98.990)\n",
            "\n",
            "Epoch: [65][0/313]\tTime 0.223 (0.223)\tData 0.161 (0.161)\tLoss 0.2391 (0.2391)\tAcc@1 91.406 (91.406)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [65][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.3528 (0.3044)\tAcc@1 87.500 (89.310)\tAcc@5 100.000 (99.675)\n",
            "Epoch: [65][200/313]\tTime 0.124 (0.122)\tData 0.002 (0.004)\tLoss 0.4483 (0.3152)\tAcc@1 83.594 (89.078)\tAcc@5 99.219 (99.670)\n",
            "Epoch: [65][300/313]\tTime 0.125 (0.122)\tData 0.020 (0.004)\tLoss 0.3031 (0.3154)\tAcc@1 89.062 (89.096)\tAcc@5 100.000 (99.652)\n",
            "Test Loss  (0.5486)\tTest Acc@1 (82.150)\tTest Acc@5 (99.130)\n",
            "\n",
            "Epoch: [66][0/313]\tTime 0.224 (0.224)\tData 0.163 (0.163)\tLoss 0.3184 (0.3184)\tAcc@1 89.844 (89.844)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [66][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.4240 (0.3086)\tAcc@1 82.812 (89.194)\tAcc@5 100.000 (99.636)\n",
            "Epoch: [66][200/313]\tTime 0.123 (0.123)\tData 0.002 (0.005)\tLoss 0.4401 (0.3136)\tAcc@1 85.938 (89.121)\tAcc@5 98.438 (99.646)\n",
            "Epoch: [66][300/313]\tTime 0.133 (0.122)\tData 0.010 (0.004)\tLoss 0.4408 (0.3184)\tAcc@1 80.469 (89.005)\tAcc@5 100.000 (99.624)\n",
            "Test Loss  (0.6956)\tTest Acc@1 (78.560)\tTest Acc@5 (98.610)\n",
            "\n",
            "Epoch: [67][0/313]\tTime 0.241 (0.241)\tData 0.183 (0.183)\tLoss 0.3228 (0.3228)\tAcc@1 91.406 (91.406)\tAcc@5 98.438 (98.438)\n",
            "Epoch: [67][100/313]\tTime 0.118 (0.123)\tData 0.003 (0.006)\tLoss 0.3679 (0.3065)\tAcc@1 85.156 (89.496)\tAcc@5 99.219 (99.683)\n",
            "Epoch: [67][200/313]\tTime 0.121 (0.123)\tData 0.004 (0.005)\tLoss 0.3324 (0.3128)\tAcc@1 89.844 (89.346)\tAcc@5 100.000 (99.642)\n",
            "Epoch: [67][300/313]\tTime 0.129 (0.122)\tData 0.007 (0.005)\tLoss 0.2313 (0.3136)\tAcc@1 92.969 (89.345)\tAcc@5 100.000 (99.637)\n",
            "Test Loss  (0.7146)\tTest Acc@1 (77.920)\tTest Acc@5 (98.010)\n",
            "\n",
            "Epoch: [68][0/313]\tTime 0.243 (0.243)\tData 0.176 (0.176)\tLoss 0.2031 (0.2031)\tAcc@1 93.750 (93.750)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [68][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.3396 (0.2972)\tAcc@1 89.062 (89.650)\tAcc@5 99.219 (99.722)\n",
            "Epoch: [68][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.3434 (0.3082)\tAcc@1 85.938 (89.272)\tAcc@5 99.219 (99.697)\n",
            "Epoch: [68][300/313]\tTime 0.122 (0.122)\tData 0.010 (0.005)\tLoss 0.2643 (0.3086)\tAcc@1 89.844 (89.351)\tAcc@5 100.000 (99.683)\n",
            "Test Loss  (0.5505)\tTest Acc@1 (82.340)\tTest Acc@5 (99.160)\n",
            "\n",
            "Epoch: [69][0/313]\tTime 0.218 (0.218)\tData 0.163 (0.163)\tLoss 0.3273 (0.3273)\tAcc@1 89.844 (89.844)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [69][100/313]\tTime 0.122 (0.124)\tData 0.002 (0.005)\tLoss 0.2624 (0.3016)\tAcc@1 90.625 (89.859)\tAcc@5 100.000 (99.652)\n",
            "Epoch: [69][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.2979 (0.3061)\tAcc@1 89.844 (89.591)\tAcc@5 100.000 (99.604)\n",
            "Epoch: [69][300/313]\tTime 0.117 (0.123)\tData 0.002 (0.004)\tLoss 0.3270 (0.3186)\tAcc@1 91.406 (89.213)\tAcc@5 100.000 (99.603)\n",
            "Test Loss  (0.4907)\tTest Acc@1 (83.950)\tTest Acc@5 (99.260)\n",
            "\n",
            "Epoch: [70][0/313]\tTime 0.245 (0.245)\tData 0.186 (0.186)\tLoss 0.2584 (0.2584)\tAcc@1 90.625 (90.625)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [70][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.3375 (0.3015)\tAcc@1 89.062 (89.774)\tAcc@5 100.000 (99.683)\n",
            "Epoch: [70][200/313]\tTime 0.125 (0.122)\tData 0.002 (0.005)\tLoss 0.4521 (0.3127)\tAcc@1 83.594 (89.276)\tAcc@5 99.219 (99.677)\n",
            "Epoch: [70][300/313]\tTime 0.128 (0.122)\tData 0.002 (0.005)\tLoss 0.4002 (0.3169)\tAcc@1 85.938 (89.140)\tAcc@5 99.219 (99.668)\n",
            "Test Loss  (0.4322)\tTest Acc@1 (85.500)\tTest Acc@5 (99.400)\n",
            "\n",
            "Epoch: [71][0/313]\tTime 0.222 (0.222)\tData 0.163 (0.163)\tLoss 0.3378 (0.3378)\tAcc@1 84.375 (84.375)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [71][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.2270 (0.2988)\tAcc@1 92.969 (89.759)\tAcc@5 100.000 (99.621)\n",
            "Epoch: [71][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.3458 (0.3004)\tAcc@1 87.500 (89.793)\tAcc@5 100.000 (99.615)\n",
            "Epoch: [71][300/313]\tTime 0.122 (0.122)\tData 0.011 (0.004)\tLoss 0.2601 (0.3077)\tAcc@1 91.406 (89.470)\tAcc@5 100.000 (99.629)\n",
            "Test Loss  (0.4299)\tTest Acc@1 (85.650)\tTest Acc@5 (99.190)\n",
            "\n",
            "Epoch: [72][0/313]\tTime 0.264 (0.264)\tData 0.199 (0.199)\tLoss 0.1719 (0.1719)\tAcc@1 94.531 (94.531)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [72][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.1949 (0.2920)\tAcc@1 92.969 (90.107)\tAcc@5 100.000 (99.760)\n",
            "Epoch: [72][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.2254 (0.3037)\tAcc@1 90.625 (89.544)\tAcc@5 100.000 (99.732)\n",
            "Epoch: [72][300/313]\tTime 0.131 (0.122)\tData 0.002 (0.004)\tLoss 0.4085 (0.3062)\tAcc@1 88.281 (89.400)\tAcc@5 98.438 (99.709)\n",
            "Test Loss  (0.6294)\tTest Acc@1 (80.160)\tTest Acc@5 (98.970)\n",
            "\n",
            "Epoch: [73][0/313]\tTime 0.223 (0.223)\tData 0.167 (0.167)\tLoss 0.2738 (0.2738)\tAcc@1 91.406 (91.406)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [73][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.2498 (0.3030)\tAcc@1 91.406 (89.712)\tAcc@5 100.000 (99.698)\n",
            "Epoch: [73][200/313]\tTime 0.124 (0.122)\tData 0.002 (0.005)\tLoss 0.3946 (0.3082)\tAcc@1 85.938 (89.494)\tAcc@5 100.000 (99.736)\n",
            "Epoch: [73][300/313]\tTime 0.122 (0.122)\tData 0.010 (0.004)\tLoss 0.3058 (0.3114)\tAcc@1 87.500 (89.345)\tAcc@5 100.000 (99.704)\n",
            "Test Loss  (0.6053)\tTest Acc@1 (81.360)\tTest Acc@5 (98.870)\n",
            "\n",
            "Epoch: [74][0/313]\tTime 0.234 (0.234)\tData 0.165 (0.165)\tLoss 0.2706 (0.2706)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [74][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.2475 (0.3126)\tAcc@1 92.188 (89.519)\tAcc@5 100.000 (99.652)\n",
            "Epoch: [74][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.3060 (0.3127)\tAcc@1 89.062 (89.272)\tAcc@5 100.000 (99.677)\n",
            "Epoch: [74][300/313]\tTime 0.119 (0.122)\tData 0.011 (0.004)\tLoss 0.3475 (0.3153)\tAcc@1 90.625 (89.164)\tAcc@5 100.000 (99.676)\n",
            "Test Loss  (0.4625)\tTest Acc@1 (85.070)\tTest Acc@5 (99.020)\n",
            "\n",
            "Epoch: [75][0/313]\tTime 0.231 (0.231)\tData 0.164 (0.164)\tLoss 0.2638 (0.2638)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [75][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.3310 (0.2968)\tAcc@1 88.281 (89.921)\tAcc@5 100.000 (99.768)\n",
            "Epoch: [75][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.4060 (0.3002)\tAcc@1 85.938 (89.735)\tAcc@5 98.438 (99.712)\n",
            "Epoch: [75][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.2783 (0.3059)\tAcc@1 88.281 (89.480)\tAcc@5 100.000 (99.704)\n",
            "Test Loss  (0.4797)\tTest Acc@1 (83.870)\tTest Acc@5 (99.220)\n",
            "\n",
            "Epoch: [76][0/313]\tTime 0.232 (0.232)\tData 0.172 (0.172)\tLoss 0.2542 (0.2542)\tAcc@1 92.188 (92.188)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [76][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.2606 (0.2998)\tAcc@1 89.844 (89.689)\tAcc@5 99.219 (99.737)\n",
            "Epoch: [76][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.3506 (0.3019)\tAcc@1 90.625 (89.614)\tAcc@5 100.000 (99.708)\n",
            "Epoch: [76][300/313]\tTime 0.125 (0.123)\tData 0.007 (0.005)\tLoss 0.3861 (0.3085)\tAcc@1 85.156 (89.358)\tAcc@5 100.000 (99.722)\n",
            "Test Loss  (0.4134)\tTest Acc@1 (85.950)\tTest Acc@5 (99.420)\n",
            "\n",
            "Epoch: [77][0/313]\tTime 0.246 (0.246)\tData 0.181 (0.181)\tLoss 0.2736 (0.2736)\tAcc@1 88.281 (88.281)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [77][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.3871 (0.2929)\tAcc@1 84.375 (90.022)\tAcc@5 100.000 (99.737)\n",
            "Epoch: [77][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.3954 (0.3114)\tAcc@1 88.281 (89.440)\tAcc@5 100.000 (99.689)\n",
            "Epoch: [77][300/313]\tTime 0.117 (0.122)\tData 0.011 (0.005)\tLoss 0.4442 (0.3145)\tAcc@1 83.594 (89.332)\tAcc@5 100.000 (99.689)\n",
            "Test Loss  (0.5092)\tTest Acc@1 (83.540)\tTest Acc@5 (98.900)\n",
            "\n",
            "Epoch: [78][0/313]\tTime 0.251 (0.251)\tData 0.186 (0.186)\tLoss 0.3294 (0.3294)\tAcc@1 92.188 (92.188)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [78][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.2873 (0.3082)\tAcc@1 93.750 (89.496)\tAcc@5 98.438 (99.652)\n",
            "Epoch: [78][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.2247 (0.3049)\tAcc@1 91.406 (89.603)\tAcc@5 100.000 (99.666)\n",
            "Epoch: [78][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.3993 (0.3063)\tAcc@1 89.062 (89.527)\tAcc@5 100.000 (99.699)\n",
            "Test Loss  (0.6042)\tTest Acc@1 (80.650)\tTest Acc@5 (98.380)\n",
            "\n",
            "Epoch: [79][0/313]\tTime 0.239 (0.239)\tData 0.170 (0.170)\tLoss 0.3910 (0.3910)\tAcc@1 85.938 (85.938)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [79][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.005)\tLoss 0.3802 (0.2957)\tAcc@1 87.500 (89.875)\tAcc@5 99.219 (99.776)\n",
            "Epoch: [79][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.2718 (0.3056)\tAcc@1 87.500 (89.525)\tAcc@5 100.000 (99.685)\n",
            "Epoch: [79][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.2629 (0.3088)\tAcc@1 92.969 (89.441)\tAcc@5 98.438 (99.657)\n",
            "Test Loss  (0.6396)\tTest Acc@1 (79.940)\tTest Acc@5 (98.770)\n",
            "\n",
            "Epoch: [80][0/313]\tTime 0.236 (0.236)\tData 0.167 (0.167)\tLoss 0.2255 (0.2255)\tAcc@1 93.750 (93.750)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [80][100/313]\tTime 0.119 (0.123)\tData 0.002 (0.006)\tLoss 0.2706 (0.2845)\tAcc@1 89.844 (90.532)\tAcc@5 100.000 (99.613)\n",
            "Epoch: [80][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.1833 (0.2993)\tAcc@1 92.188 (89.871)\tAcc@5 100.000 (99.654)\n",
            "Epoch: [80][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.3124 (0.3078)\tAcc@1 88.281 (89.576)\tAcc@5 100.000 (99.678)\n",
            "Test Loss  (0.5995)\tTest Acc@1 (80.370)\tTest Acc@5 (98.530)\n",
            "\n",
            "Epoch: [81][0/313]\tTime 0.223 (0.223)\tData 0.163 (0.163)\tLoss 0.2755 (0.2755)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [81][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.3965 (0.2982)\tAcc@1 85.938 (89.790)\tAcc@5 100.000 (99.698)\n",
            "Epoch: [81][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.2443 (0.3125)\tAcc@1 92.188 (89.300)\tAcc@5 100.000 (99.674)\n",
            "Epoch: [81][300/313]\tTime 0.118 (0.122)\tData 0.002 (0.004)\tLoss 0.3419 (0.3146)\tAcc@1 87.500 (89.192)\tAcc@5 100.000 (99.683)\n",
            "Test Loss  (0.5656)\tTest Acc@1 (82.030)\tTest Acc@5 (98.770)\n",
            "\n",
            "Epoch: [82][0/313]\tTime 0.233 (0.233)\tData 0.161 (0.161)\tLoss 0.2375 (0.2375)\tAcc@1 91.406 (91.406)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [82][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.2770 (0.2905)\tAcc@1 93.750 (89.743)\tAcc@5 100.000 (99.768)\n",
            "Epoch: [82][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.2759 (0.2974)\tAcc@1 90.625 (89.828)\tAcc@5 99.219 (99.740)\n",
            "Epoch: [82][300/313]\tTime 0.132 (0.123)\tData 0.005 (0.005)\tLoss 0.3889 (0.3037)\tAcc@1 86.719 (89.574)\tAcc@5 99.219 (99.709)\n",
            "Test Loss  (0.4430)\tTest Acc@1 (84.950)\tTest Acc@5 (99.140)\n",
            "\n",
            "Epoch: [83][0/313]\tTime 0.220 (0.220)\tData 0.159 (0.159)\tLoss 0.1916 (0.1916)\tAcc@1 92.969 (92.969)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [83][100/313]\tTime 0.119 (0.123)\tData 0.002 (0.006)\tLoss 0.2746 (0.2907)\tAcc@1 89.844 (89.944)\tAcc@5 100.000 (99.745)\n",
            "Epoch: [83][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.4130 (0.2975)\tAcc@1 87.500 (89.704)\tAcc@5 100.000 (99.740)\n",
            "Epoch: [83][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.2143 (0.3070)\tAcc@1 92.188 (89.390)\tAcc@5 100.000 (99.712)\n",
            "Test Loss  (0.4953)\tTest Acc@1 (83.270)\tTest Acc@5 (99.110)\n",
            "\n",
            "Epoch: [84][0/313]\tTime 0.230 (0.230)\tData 0.164 (0.164)\tLoss 0.2839 (0.2839)\tAcc@1 89.062 (89.062)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [84][100/313]\tTime 0.119 (0.123)\tData 0.002 (0.006)\tLoss 0.2099 (0.2767)\tAcc@1 93.750 (90.455)\tAcc@5 100.000 (99.706)\n",
            "Epoch: [84][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.2807 (0.2958)\tAcc@1 91.406 (89.817)\tAcc@5 100.000 (99.747)\n",
            "Epoch: [84][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.3736 (0.3023)\tAcc@1 90.625 (89.701)\tAcc@5 100.000 (99.707)\n",
            "Test Loss  (0.4707)\tTest Acc@1 (84.090)\tTest Acc@5 (99.190)\n",
            "\n",
            "Epoch: [85][0/313]\tTime 0.235 (0.235)\tData 0.174 (0.174)\tLoss 0.4312 (0.4312)\tAcc@1 88.281 (88.281)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [85][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.2021 (0.2898)\tAcc@1 92.188 (90.068)\tAcc@5 100.000 (99.768)\n",
            "Epoch: [85][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.2302 (0.2954)\tAcc@1 92.969 (89.801)\tAcc@5 100.000 (99.736)\n",
            "Epoch: [85][300/313]\tTime 0.119 (0.122)\tData 0.002 (0.004)\tLoss 0.3905 (0.3059)\tAcc@1 86.719 (89.483)\tAcc@5 99.219 (99.714)\n",
            "Test Loss  (0.4325)\tTest Acc@1 (85.450)\tTest Acc@5 (99.330)\n",
            "\n",
            "Epoch: [86][0/313]\tTime 0.250 (0.250)\tData 0.187 (0.187)\tLoss 0.3411 (0.3411)\tAcc@1 85.938 (85.938)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [86][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.2631 (0.2914)\tAcc@1 88.281 (89.952)\tAcc@5 100.000 (99.729)\n",
            "Epoch: [86][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.3737 (0.3013)\tAcc@1 85.938 (89.626)\tAcc@5 100.000 (99.705)\n",
            "Epoch: [86][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.2571 (0.3041)\tAcc@1 89.062 (89.548)\tAcc@5 100.000 (99.670)\n",
            "Test Loss  (0.4668)\tTest Acc@1 (84.410)\tTest Acc@5 (99.130)\n",
            "\n",
            "Epoch: [87][0/313]\tTime 0.231 (0.231)\tData 0.168 (0.168)\tLoss 0.3104 (0.3104)\tAcc@1 86.719 (86.719)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [87][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.3181 (0.2864)\tAcc@1 89.844 (90.246)\tAcc@5 98.438 (99.698)\n",
            "Epoch: [87][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.3119 (0.3059)\tAcc@1 89.844 (89.595)\tAcc@5 100.000 (99.662)\n",
            "Epoch: [87][300/313]\tTime 0.125 (0.122)\tData 0.002 (0.004)\tLoss 0.3763 (0.3045)\tAcc@1 86.719 (89.566)\tAcc@5 100.000 (99.686)\n",
            "Test Loss  (0.5237)\tTest Acc@1 (83.390)\tTest Acc@5 (99.020)\n",
            "\n",
            "Epoch: [88][0/313]\tTime 0.234 (0.234)\tData 0.172 (0.172)\tLoss 0.2849 (0.2849)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [88][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.2873 (0.2972)\tAcc@1 89.062 (90.068)\tAcc@5 100.000 (99.706)\n",
            "Epoch: [88][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.2607 (0.3068)\tAcc@1 91.406 (89.715)\tAcc@5 100.000 (99.701)\n",
            "Epoch: [88][300/313]\tTime 0.108 (0.123)\tData 0.008 (0.006)\tLoss 0.4924 (0.3076)\tAcc@1 82.812 (89.626)\tAcc@5 99.219 (99.712)\n",
            "Test Loss  (0.5309)\tTest Acc@1 (82.700)\tTest Acc@5 (98.490)\n",
            "\n",
            "Epoch: [89][0/313]\tTime 0.236 (0.236)\tData 0.174 (0.174)\tLoss 0.2021 (0.2021)\tAcc@1 94.531 (94.531)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [89][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.2928 (0.2892)\tAcc@1 91.406 (90.184)\tAcc@5 99.219 (99.667)\n",
            "Epoch: [89][200/313]\tTime 0.122 (0.122)\tData 0.003 (0.005)\tLoss 0.3176 (0.3050)\tAcc@1 89.062 (89.506)\tAcc@5 100.000 (99.662)\n",
            "Epoch: [89][300/313]\tTime 0.115 (0.122)\tData 0.009 (0.005)\tLoss 0.3575 (0.3102)\tAcc@1 88.281 (89.332)\tAcc@5 100.000 (99.655)\n",
            "Test Loss  (0.4844)\tTest Acc@1 (83.890)\tTest Acc@5 (99.140)\n",
            "\n",
            "Epoch: [90][0/313]\tTime 0.245 (0.245)\tData 0.175 (0.175)\tLoss 0.2163 (0.2163)\tAcc@1 92.188 (92.188)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [90][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.3571 (0.3188)\tAcc@1 85.938 (89.055)\tAcc@5 100.000 (99.706)\n",
            "Epoch: [90][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.2744 (0.3131)\tAcc@1 89.062 (89.327)\tAcc@5 100.000 (99.708)\n",
            "Epoch: [90][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.3159 (0.3089)\tAcc@1 85.938 (89.496)\tAcc@5 100.000 (99.691)\n",
            "Test Loss  (0.4516)\tTest Acc@1 (84.760)\tTest Acc@5 (99.290)\n",
            "\n",
            "Epoch: [91][0/313]\tTime 0.221 (0.221)\tData 0.167 (0.167)\tLoss 0.1562 (0.1562)\tAcc@1 96.094 (96.094)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [91][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.3572 (0.2845)\tAcc@1 89.844 (90.130)\tAcc@5 100.000 (99.737)\n",
            "Epoch: [91][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.2476 (0.2939)\tAcc@1 92.969 (89.976)\tAcc@5 100.000 (99.642)\n",
            "Epoch: [91][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.3274 (0.2958)\tAcc@1 89.062 (89.961)\tAcc@5 99.219 (99.647)\n",
            "Test Loss  (0.5628)\tTest Acc@1 (82.000)\tTest Acc@5 (98.650)\n",
            "\n",
            "Epoch: [92][0/313]\tTime 0.239 (0.239)\tData 0.176 (0.176)\tLoss 0.2716 (0.2716)\tAcc@1 88.281 (88.281)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [92][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.2320 (0.3104)\tAcc@1 91.406 (89.558)\tAcc@5 100.000 (99.722)\n",
            "Epoch: [92][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.2955 (0.3134)\tAcc@1 92.188 (89.350)\tAcc@5 99.219 (99.712)\n",
            "Epoch: [92][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.3616 (0.3107)\tAcc@1 89.844 (89.379)\tAcc@5 100.000 (99.691)\n",
            "Test Loss  (0.5873)\tTest Acc@1 (81.240)\tTest Acc@5 (98.230)\n",
            "\n",
            "Epoch: [93][0/313]\tTime 0.242 (0.242)\tData 0.178 (0.178)\tLoss 0.2600 (0.2600)\tAcc@1 87.500 (87.500)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [93][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.3618 (0.2940)\tAcc@1 85.938 (89.821)\tAcc@5 100.000 (99.683)\n",
            "Epoch: [93][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.3502 (0.3008)\tAcc@1 87.500 (89.587)\tAcc@5 99.219 (99.689)\n",
            "Epoch: [93][300/313]\tTime 0.122 (0.122)\tData 0.003 (0.005)\tLoss 0.3301 (0.3061)\tAcc@1 89.062 (89.418)\tAcc@5 100.000 (99.676)\n",
            "Test Loss  (0.4883)\tTest Acc@1 (84.250)\tTest Acc@5 (99.130)\n",
            "\n",
            "Epoch: [94][0/313]\tTime 0.232 (0.232)\tData 0.169 (0.169)\tLoss 0.2812 (0.2812)\tAcc@1 89.844 (89.844)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [94][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.007)\tLoss 0.3700 (0.2833)\tAcc@1 86.719 (90.246)\tAcc@5 97.656 (99.776)\n",
            "Epoch: [94][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.3449 (0.3017)\tAcc@1 86.719 (89.747)\tAcc@5 100.000 (99.670)\n",
            "Epoch: [94][300/313]\tTime 0.125 (0.123)\tData 0.002 (0.006)\tLoss 0.2789 (0.3088)\tAcc@1 87.500 (89.530)\tAcc@5 99.219 (99.665)\n",
            "Test Loss  (0.5398)\tTest Acc@1 (82.490)\tTest Acc@5 (98.940)\n",
            "\n",
            "Epoch: [95][0/313]\tTime 0.223 (0.223)\tData 0.163 (0.163)\tLoss 0.3032 (0.3032)\tAcc@1 87.500 (87.500)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [95][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.2758 (0.2866)\tAcc@1 89.844 (90.408)\tAcc@5 99.219 (99.783)\n",
            "Epoch: [95][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.2765 (0.3010)\tAcc@1 89.062 (89.879)\tAcc@5 98.438 (99.720)\n",
            "Epoch: [95][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.2624 (0.3065)\tAcc@1 91.406 (89.587)\tAcc@5 100.000 (99.707)\n",
            "Test Loss  (0.6173)\tTest Acc@1 (80.640)\tTest Acc@5 (99.040)\n",
            "\n",
            "Epoch: [96][0/313]\tTime 0.242 (0.242)\tData 0.177 (0.177)\tLoss 0.4050 (0.4050)\tAcc@1 86.719 (86.719)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [96][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.3401 (0.3005)\tAcc@1 86.719 (89.705)\tAcc@5 100.000 (99.691)\n",
            "Epoch: [96][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.3115 (0.3008)\tAcc@1 87.500 (89.669)\tAcc@5 100.000 (99.674)\n",
            "Epoch: [96][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.3119 (0.3016)\tAcc@1 89.062 (89.678)\tAcc@5 100.000 (99.702)\n",
            "Test Loss  (0.5515)\tTest Acc@1 (81.590)\tTest Acc@5 (99.200)\n",
            "\n",
            "Epoch: [97][0/313]\tTime 0.229 (0.229)\tData 0.166 (0.166)\tLoss 0.1569 (0.1569)\tAcc@1 96.094 (96.094)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [97][100/313]\tTime 0.128 (0.123)\tData 0.002 (0.006)\tLoss 0.2761 (0.2897)\tAcc@1 88.281 (89.898)\tAcc@5 100.000 (99.807)\n",
            "Epoch: [97][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.2432 (0.2990)\tAcc@1 90.625 (89.688)\tAcc@5 100.000 (99.743)\n",
            "Epoch: [97][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.4858 (0.3033)\tAcc@1 85.156 (89.634)\tAcc@5 98.438 (99.712)\n",
            "Test Loss  (0.4249)\tTest Acc@1 (85.540)\tTest Acc@5 (99.320)\n",
            "\n",
            "Epoch: [98][0/313]\tTime 0.243 (0.243)\tData 0.172 (0.172)\tLoss 0.2732 (0.2732)\tAcc@1 90.625 (90.625)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [98][100/313]\tTime 0.119 (0.123)\tData 0.002 (0.005)\tLoss 0.3923 (0.2769)\tAcc@1 87.500 (90.594)\tAcc@5 98.438 (99.737)\n",
            "Epoch: [98][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.2627 (0.3006)\tAcc@1 92.969 (89.852)\tAcc@5 100.000 (99.654)\n",
            "Epoch: [98][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.2497 (0.3020)\tAcc@1 88.281 (89.787)\tAcc@5 100.000 (99.670)\n",
            "Test Loss  (0.5509)\tTest Acc@1 (82.550)\tTest Acc@5 (98.920)\n",
            "\n",
            "Epoch: [99][0/313]\tTime 0.233 (0.233)\tData 0.162 (0.162)\tLoss 0.1873 (0.1873)\tAcc@1 92.188 (92.188)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [99][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.2136 (0.2956)\tAcc@1 92.969 (89.712)\tAcc@5 100.000 (99.675)\n",
            "Epoch: [99][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.2876 (0.2966)\tAcc@1 91.406 (89.653)\tAcc@5 100.000 (99.685)\n",
            "Epoch: [99][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.3249 (0.3029)\tAcc@1 87.500 (89.579)\tAcc@5 100.000 (99.681)\n",
            "Test Loss  (0.4644)\tTest Acc@1 (84.380)\tTest Acc@5 (99.240)\n",
            "\n",
            "Epoch: [100][0/313]\tTime 0.354 (0.354)\tData 0.293 (0.293)\tLoss 0.2429 (0.2429)\tAcc@1 92.188 (92.188)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [100][100/313]\tTime 0.122 (0.124)\tData 0.002 (0.006)\tLoss 0.0912 (0.1961)\tAcc@1 98.438 (93.464)\tAcc@5 100.000 (99.822)\n",
            "Epoch: [100][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.1458 (0.1710)\tAcc@1 92.969 (94.337)\tAcc@5 100.000 (99.887)\n",
            "Epoch: [100][300/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.1996 (0.1587)\tAcc@1 93.750 (94.786)\tAcc@5 100.000 (99.912)\n",
            "Test Loss  (0.2310)\tTest Acc@1 (92.160)\tTest Acc@5 (99.830)\n",
            "\n",
            "Epoch: [101][0/313]\tTime 0.320 (0.320)\tData 0.262 (0.262)\tLoss 0.0950 (0.0950)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [101][100/313]\tTime 0.119 (0.123)\tData 0.002 (0.006)\tLoss 0.1266 (0.1148)\tAcc@1 95.312 (96.140)\tAcc@5 100.000 (99.961)\n",
            "Epoch: [101][200/313]\tTime 0.117 (0.123)\tData 0.002 (0.005)\tLoss 0.0455 (0.1114)\tAcc@1 98.438 (96.261)\tAcc@5 100.000 (99.961)\n",
            "Epoch: [101][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.1393 (0.1090)\tAcc@1 93.750 (96.353)\tAcc@5 100.000 (99.961)\n",
            "Test Loss  (0.2160)\tTest Acc@1 (93.000)\tTest Acc@5 (99.810)\n",
            "\n",
            "Epoch: [102][0/313]\tTime 0.253 (0.253)\tData 0.190 (0.190)\tLoss 0.0996 (0.0996)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [102][100/313]\tTime 0.126 (0.123)\tData 0.012 (0.006)\tLoss 0.0848 (0.0879)\tAcc@1 97.656 (97.092)\tAcc@5 100.000 (99.977)\n",
            "Epoch: [102][200/313]\tTime 0.122 (0.122)\tData 0.006 (0.004)\tLoss 0.0530 (0.0895)\tAcc@1 98.438 (97.081)\tAcc@5 100.000 (99.981)\n",
            "Epoch: [102][300/313]\tTime 0.118 (0.122)\tData 0.005 (0.004)\tLoss 0.0986 (0.0896)\tAcc@1 96.875 (97.085)\tAcc@5 100.000 (99.966)\n",
            "Test Loss  (0.2157)\tTest Acc@1 (93.280)\tTest Acc@5 (99.730)\n",
            "\n",
            "Epoch: [103][0/313]\tTime 0.268 (0.268)\tData 0.209 (0.209)\tLoss 0.0638 (0.0638)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [103][100/313]\tTime 0.120 (0.124)\tData 0.002 (0.006)\tLoss 0.0667 (0.0727)\tAcc@1 98.438 (97.610)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [103][200/313]\tTime 0.126 (0.123)\tData 0.010 (0.005)\tLoss 0.0695 (0.0763)\tAcc@1 96.875 (97.458)\tAcc@5 100.000 (99.984)\n",
            "Epoch: [103][300/313]\tTime 0.138 (0.122)\tData 0.002 (0.004)\tLoss 0.0664 (0.0774)\tAcc@1 97.656 (97.397)\tAcc@5 100.000 (99.979)\n",
            "Test Loss  (0.2111)\tTest Acc@1 (93.430)\tTest Acc@5 (99.870)\n",
            "\n",
            "Epoch: [104][0/313]\tTime 0.260 (0.260)\tData 0.198 (0.198)\tLoss 0.1541 (0.1541)\tAcc@1 96.094 (96.094)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [104][100/313]\tTime 0.120 (0.124)\tData 0.002 (0.007)\tLoss 0.0612 (0.0656)\tAcc@1 98.438 (97.881)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [104][200/313]\tTime 0.116 (0.123)\tData 0.009 (0.005)\tLoss 0.0654 (0.0645)\tAcc@1 97.656 (97.924)\tAcc@5 100.000 (99.988)\n",
            "Epoch: [104][300/313]\tTime 0.118 (0.123)\tData 0.011 (0.005)\tLoss 0.0814 (0.0650)\tAcc@1 96.875 (97.900)\tAcc@5 100.000 (99.987)\n",
            "Test Loss  (0.2138)\tTest Acc@1 (93.420)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [105][0/313]\tTime 0.226 (0.226)\tData 0.170 (0.170)\tLoss 0.0713 (0.0713)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [105][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.0938 (0.0576)\tAcc@1 97.656 (98.244)\tAcc@5 100.000 (99.977)\n",
            "Epoch: [105][200/313]\tTime 0.123 (0.123)\tData 0.002 (0.005)\tLoss 0.0714 (0.0572)\tAcc@1 96.094 (98.169)\tAcc@5 100.000 (99.984)\n",
            "Epoch: [105][300/313]\tTime 0.124 (0.122)\tData 0.002 (0.005)\tLoss 0.0559 (0.0586)\tAcc@1 97.656 (98.110)\tAcc@5 100.000 (99.987)\n",
            "Test Loss  (0.2161)\tTest Acc@1 (93.490)\tTest Acc@5 (99.830)\n",
            "\n",
            "Epoch: [106][0/313]\tTime 0.266 (0.266)\tData 0.195 (0.195)\tLoss 0.0478 (0.0478)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [106][100/313]\tTime 0.117 (0.126)\tData 0.002 (0.008)\tLoss 0.0211 (0.0492)\tAcc@1 99.219 (98.515)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [106][200/313]\tTime 0.133 (0.124)\tData 0.015 (0.006)\tLoss 0.0510 (0.0502)\tAcc@1 96.875 (98.371)\tAcc@5 100.000 (99.988)\n",
            "Epoch: [106][300/313]\tTime 0.123 (0.123)\tData 0.002 (0.005)\tLoss 0.0219 (0.0515)\tAcc@1 99.219 (98.318)\tAcc@5 100.000 (99.992)\n",
            "Test Loss  (0.2209)\tTest Acc@1 (93.440)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [107][0/313]\tTime 0.244 (0.244)\tData 0.180 (0.180)\tLoss 0.0461 (0.0461)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [107][100/313]\tTime 0.124 (0.123)\tData 0.002 (0.006)\tLoss 0.0666 (0.0423)\tAcc@1 97.656 (98.670)\tAcc@5 100.000 (99.977)\n",
            "Epoch: [107][200/313]\tTime 0.121 (0.123)\tData 0.006 (0.005)\tLoss 0.0468 (0.0436)\tAcc@1 99.219 (98.655)\tAcc@5 100.000 (99.984)\n",
            "Epoch: [107][300/313]\tTime 0.121 (0.122)\tData 0.011 (0.004)\tLoss 0.0522 (0.0457)\tAcc@1 99.219 (98.557)\tAcc@5 100.000 (99.987)\n",
            "Test Loss  (0.2249)\tTest Acc@1 (93.330)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [108][0/313]\tTime 0.234 (0.234)\tData 0.171 (0.171)\tLoss 0.0368 (0.0368)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [108][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.0653 (0.0390)\tAcc@1 98.438 (98.731)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [108][200/313]\tTime 0.122 (0.122)\tData 0.003 (0.005)\tLoss 0.0554 (0.0419)\tAcc@1 98.438 (98.659)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [108][300/313]\tTime 0.119 (0.122)\tData 0.006 (0.004)\tLoss 0.0657 (0.0419)\tAcc@1 97.656 (98.676)\tAcc@5 100.000 (99.995)\n",
            "Test Loss  (0.2314)\tTest Acc@1 (93.400)\tTest Acc@5 (99.840)\n",
            "\n",
            "Epoch: [109][0/313]\tTime 0.224 (0.224)\tData 0.169 (0.169)\tLoss 0.0105 (0.0105)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [109][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.0142 (0.0352)\tAcc@1 100.000 (98.933)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [109][200/313]\tTime 0.125 (0.122)\tData 0.002 (0.005)\tLoss 0.0402 (0.0378)\tAcc@1 97.656 (98.756)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [109][300/313]\tTime 0.121 (0.122)\tData 0.013 (0.004)\tLoss 0.0254 (0.0394)\tAcc@1 100.000 (98.694)\tAcc@5 100.000 (99.992)\n",
            "Test Loss  (0.2269)\tTest Acc@1 (93.580)\tTest Acc@5 (99.810)\n",
            "\n",
            "Epoch: [110][0/313]\tTime 0.254 (0.254)\tData 0.188 (0.188)\tLoss 0.0093 (0.0093)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [110][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0492 (0.0334)\tAcc@1 98.438 (99.025)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [110][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0172 (0.0342)\tAcc@1 100.000 (99.024)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [110][300/313]\tTime 0.111 (0.122)\tData 0.002 (0.005)\tLoss 0.0882 (0.0338)\tAcc@1 96.875 (98.988)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2320)\tTest Acc@1 (93.270)\tTest Acc@5 (99.830)\n",
            "\n",
            "Epoch: [111][0/313]\tTime 0.227 (0.227)\tData 0.164 (0.164)\tLoss 0.0146 (0.0146)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [111][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0379 (0.0290)\tAcc@1 99.219 (99.049)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [111][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0205 (0.0315)\tAcc@1 100.000 (98.962)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [111][300/313]\tTime 0.122 (0.122)\tData 0.003 (0.005)\tLoss 0.0093 (0.0312)\tAcc@1 100.000 (99.003)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2308)\tTest Acc@1 (93.470)\tTest Acc@5 (99.790)\n",
            "\n",
            "Epoch: [112][0/313]\tTime 0.349 (0.349)\tData 0.293 (0.293)\tLoss 0.0608 (0.0608)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [112][100/313]\tTime 0.117 (0.125)\tData 0.002 (0.008)\tLoss 0.0153 (0.0279)\tAcc@1 99.219 (99.095)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [112][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0213 (0.0273)\tAcc@1 99.219 (99.164)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [112][300/313]\tTime 0.123 (0.123)\tData 0.002 (0.005)\tLoss 0.0619 (0.0283)\tAcc@1 98.438 (99.138)\tAcc@5 100.000 (99.995)\n",
            "Test Loss  (0.2353)\tTest Acc@1 (93.240)\tTest Acc@5 (99.850)\n",
            "\n",
            "Epoch: [113][0/313]\tTime 0.236 (0.236)\tData 0.172 (0.172)\tLoss 0.0392 (0.0392)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [113][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.0719 (0.0269)\tAcc@1 96.094 (99.242)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [113][200/313]\tTime 0.120 (0.122)\tData 0.003 (0.005)\tLoss 0.0083 (0.0265)\tAcc@1 100.000 (99.223)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [113][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.1032 (0.0277)\tAcc@1 96.875 (99.136)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2467)\tTest Acc@1 (93.150)\tTest Acc@5 (99.820)\n",
            "\n",
            "Epoch: [114][0/313]\tTime 0.233 (0.233)\tData 0.178 (0.178)\tLoss 0.0166 (0.0166)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [114][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0302 (0.0257)\tAcc@1 99.219 (99.257)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [114][200/313]\tTime 0.123 (0.122)\tData 0.002 (0.005)\tLoss 0.0238 (0.0246)\tAcc@1 100.000 (99.308)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [114][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.0131 (0.0241)\tAcc@1 100.000 (99.315)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2336)\tTest Acc@1 (93.670)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [115][0/313]\tTime 0.267 (0.267)\tData 0.208 (0.208)\tLoss 0.0086 (0.0086)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [115][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0273 (0.0204)\tAcc@1 99.219 (99.443)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [115][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.0338 (0.0229)\tAcc@1 99.219 (99.355)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [115][300/313]\tTime 0.120 (0.122)\tData 0.003 (0.005)\tLoss 0.0322 (0.0231)\tAcc@1 99.219 (99.343)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2462)\tTest Acc@1 (93.300)\tTest Acc@5 (99.790)\n",
            "\n",
            "Epoch: [116][0/313]\tTime 0.219 (0.219)\tData 0.155 (0.155)\tLoss 0.0289 (0.0289)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [116][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.0123 (0.0241)\tAcc@1 100.000 (99.196)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [116][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0124 (0.0247)\tAcc@1 100.000 (99.203)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [116][300/313]\tTime 0.121 (0.122)\tData 0.003 (0.005)\tLoss 0.0169 (0.0250)\tAcc@1 100.000 (99.214)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2535)\tTest Acc@1 (93.270)\tTest Acc@5 (99.760)\n",
            "\n",
            "Epoch: [117][0/313]\tTime 0.225 (0.225)\tData 0.164 (0.164)\tLoss 0.0169 (0.0169)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [117][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0167 (0.0188)\tAcc@1 100.000 (99.435)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [117][200/313]\tTime 0.118 (0.123)\tData 0.002 (0.005)\tLoss 0.0102 (0.0196)\tAcc@1 100.000 (99.444)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [117][300/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0309 (0.0215)\tAcc@1 98.438 (99.385)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2528)\tTest Acc@1 (93.290)\tTest Acc@5 (99.720)\n",
            "\n",
            "Epoch: [118][0/313]\tTime 0.233 (0.233)\tData 0.165 (0.165)\tLoss 0.0098 (0.0098)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [118][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0075 (0.0190)\tAcc@1 100.000 (99.443)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [118][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.004)\tLoss 0.0308 (0.0199)\tAcc@1 99.219 (99.413)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [118][300/313]\tTime 0.119 (0.122)\tData 0.002 (0.004)\tLoss 0.0311 (0.0226)\tAcc@1 98.438 (99.328)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2487)\tTest Acc@1 (93.400)\tTest Acc@5 (99.760)\n",
            "\n",
            "Epoch: [119][0/313]\tTime 0.319 (0.319)\tData 0.235 (0.235)\tLoss 0.0210 (0.0210)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [119][100/313]\tTime 0.120 (0.124)\tData 0.002 (0.006)\tLoss 0.0226 (0.0200)\tAcc@1 99.219 (99.428)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [119][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0237 (0.0191)\tAcc@1 100.000 (99.487)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [119][300/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0176 (0.0206)\tAcc@1 99.219 (99.411)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2569)\tTest Acc@1 (93.210)\tTest Acc@5 (99.700)\n",
            "\n",
            "Epoch: [120][0/313]\tTime 0.426 (0.426)\tData 0.320 (0.320)\tLoss 0.0098 (0.0098)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [120][100/313]\tTime 0.113 (0.124)\tData 0.003 (0.006)\tLoss 0.0084 (0.0212)\tAcc@1 100.000 (99.373)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [120][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0271 (0.0208)\tAcc@1 99.219 (99.370)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [120][300/313]\tTime 0.122 (0.123)\tData 0.002 (0.004)\tLoss 0.0065 (0.0209)\tAcc@1 100.000 (99.380)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2630)\tTest Acc@1 (93.200)\tTest Acc@5 (99.730)\n",
            "\n",
            "Epoch: [121][0/313]\tTime 0.378 (0.378)\tData 0.301 (0.301)\tLoss 0.0060 (0.0060)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [121][100/313]\tTime 0.116 (0.124)\tData 0.009 (0.006)\tLoss 0.0216 (0.0172)\tAcc@1 99.219 (99.435)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [121][200/313]\tTime 0.124 (0.123)\tData 0.003 (0.004)\tLoss 0.0215 (0.0209)\tAcc@1 99.219 (99.351)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [121][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.0118 (0.0224)\tAcc@1 100.000 (99.299)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2580)\tTest Acc@1 (93.190)\tTest Acc@5 (99.740)\n",
            "\n",
            "Epoch: [122][0/313]\tTime 0.227 (0.227)\tData 0.166 (0.166)\tLoss 0.0048 (0.0048)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [122][100/313]\tTime 0.127 (0.123)\tData 0.012 (0.004)\tLoss 0.0327 (0.0229)\tAcc@1 98.438 (99.350)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [122][200/313]\tTime 0.119 (0.122)\tData 0.013 (0.004)\tLoss 0.0127 (0.0221)\tAcc@1 99.219 (99.347)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [122][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.0099 (0.0223)\tAcc@1 100.000 (99.317)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2549)\tTest Acc@1 (93.120)\tTest Acc@5 (99.740)\n",
            "\n",
            "Epoch: [123][0/313]\tTime 0.226 (0.226)\tData 0.163 (0.163)\tLoss 0.0176 (0.0176)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [123][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0372 (0.0200)\tAcc@1 99.219 (99.366)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [123][200/313]\tTime 0.128 (0.123)\tData 0.007 (0.005)\tLoss 0.0280 (0.0203)\tAcc@1 99.219 (99.351)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [123][300/313]\tTime 0.118 (0.122)\tData 0.005 (0.004)\tLoss 0.0064 (0.0208)\tAcc@1 100.000 (99.328)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2474)\tTest Acc@1 (93.560)\tTest Acc@5 (99.850)\n",
            "\n",
            "Epoch: [124][0/313]\tTime 0.235 (0.235)\tData 0.166 (0.166)\tLoss 0.0133 (0.0133)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [124][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.0218 (0.0202)\tAcc@1 99.219 (99.335)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [124][200/313]\tTime 0.122 (0.122)\tData 0.011 (0.005)\tLoss 0.0196 (0.0212)\tAcc@1 99.219 (99.339)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [124][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.004)\tLoss 0.0140 (0.0218)\tAcc@1 100.000 (99.351)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2804)\tTest Acc@1 (92.840)\tTest Acc@5 (99.750)\n",
            "\n",
            "Epoch: [125][0/313]\tTime 0.280 (0.280)\tData 0.205 (0.205)\tLoss 0.0276 (0.0276)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [125][100/313]\tTime 0.120 (0.124)\tData 0.002 (0.007)\tLoss 0.0117 (0.0207)\tAcc@1 99.219 (99.343)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [125][200/313]\tTime 0.117 (0.123)\tData 0.002 (0.006)\tLoss 0.0034 (0.0226)\tAcc@1 100.000 (99.281)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [125][300/313]\tTime 0.128 (0.122)\tData 0.018 (0.005)\tLoss 0.0689 (0.0233)\tAcc@1 98.438 (99.273)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2721)\tTest Acc@1 (92.830)\tTest Acc@5 (99.730)\n",
            "\n",
            "Epoch: [126][0/313]\tTime 0.239 (0.239)\tData 0.170 (0.170)\tLoss 0.0217 (0.0217)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [126][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0149 (0.0202)\tAcc@1 99.219 (99.397)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [126][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0065 (0.0207)\tAcc@1 100.000 (99.382)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [126][300/313]\tTime 0.129 (0.123)\tData 0.019 (0.005)\tLoss 0.0389 (0.0219)\tAcc@1 98.438 (99.328)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2644)\tTest Acc@1 (93.030)\tTest Acc@5 (99.760)\n",
            "\n",
            "Epoch: [127][0/313]\tTime 0.268 (0.268)\tData 0.199 (0.199)\tLoss 0.0337 (0.0337)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [127][100/313]\tTime 0.122 (0.124)\tData 0.002 (0.007)\tLoss 0.0104 (0.0256)\tAcc@1 100.000 (99.172)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [127][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0281 (0.0242)\tAcc@1 97.656 (99.246)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [127][300/313]\tTime 0.132 (0.122)\tData 0.010 (0.005)\tLoss 0.0248 (0.0227)\tAcc@1 99.219 (99.304)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2668)\tTest Acc@1 (93.100)\tTest Acc@5 (99.790)\n",
            "\n",
            "Epoch: [128][0/313]\tTime 0.253 (0.253)\tData 0.185 (0.185)\tLoss 0.0333 (0.0333)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [128][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.0066 (0.0221)\tAcc@1 100.000 (99.358)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [128][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0334 (0.0217)\tAcc@1 98.438 (99.331)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [128][300/313]\tTime 0.122 (0.122)\tData 0.008 (0.005)\tLoss 0.0103 (0.0213)\tAcc@1 100.000 (99.364)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2701)\tTest Acc@1 (93.050)\tTest Acc@5 (99.720)\n",
            "\n",
            "Epoch: [129][0/313]\tTime 0.235 (0.235)\tData 0.173 (0.173)\tLoss 0.0062 (0.0062)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [129][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0419 (0.0229)\tAcc@1 98.438 (99.335)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [129][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0059 (0.0237)\tAcc@1 100.000 (99.296)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [129][300/313]\tTime 0.107 (0.122)\tData 0.009 (0.005)\tLoss 0.0341 (0.0240)\tAcc@1 98.438 (99.245)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2626)\tTest Acc@1 (93.160)\tTest Acc@5 (99.750)\n",
            "\n",
            "Epoch: [130][0/313]\tTime 0.241 (0.241)\tData 0.169 (0.169)\tLoss 0.0057 (0.0057)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [130][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0063 (0.0225)\tAcc@1 100.000 (99.312)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [130][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0092 (0.0248)\tAcc@1 100.000 (99.246)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [130][300/313]\tTime 0.099 (0.122)\tData 0.003 (0.005)\tLoss 0.0750 (0.0254)\tAcc@1 99.219 (99.234)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2549)\tTest Acc@1 (93.280)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [131][0/313]\tTime 0.227 (0.227)\tData 0.164 (0.164)\tLoss 0.0136 (0.0136)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [131][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0174 (0.0196)\tAcc@1 100.000 (99.451)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [131][200/313]\tTime 0.119 (0.122)\tData 0.002 (0.005)\tLoss 0.0262 (0.0224)\tAcc@1 99.219 (99.269)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [131][300/313]\tTime 0.116 (0.122)\tData 0.007 (0.005)\tLoss 0.0345 (0.0236)\tAcc@1 99.219 (99.229)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2635)\tTest Acc@1 (92.850)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [132][0/313]\tTime 0.228 (0.228)\tData 0.165 (0.165)\tLoss 0.0123 (0.0123)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [132][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0363 (0.0194)\tAcc@1 98.438 (99.389)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [132][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0263 (0.0215)\tAcc@1 99.219 (99.324)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [132][300/313]\tTime 0.114 (0.122)\tData 0.002 (0.004)\tLoss 0.0059 (0.0224)\tAcc@1 100.000 (99.315)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2915)\tTest Acc@1 (92.670)\tTest Acc@5 (99.650)\n",
            "\n",
            "Epoch: [133][0/313]\tTime 0.222 (0.222)\tData 0.165 (0.165)\tLoss 0.0184 (0.0184)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [133][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.0239 (0.0234)\tAcc@1 99.219 (99.188)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [133][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0032 (0.0248)\tAcc@1 100.000 (99.215)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [133][300/313]\tTime 0.129 (0.122)\tData 0.013 (0.005)\tLoss 0.0327 (0.0258)\tAcc@1 99.219 (99.185)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2726)\tTest Acc@1 (92.790)\tTest Acc@5 (99.650)\n",
            "\n",
            "Epoch: [134][0/313]\tTime 0.232 (0.232)\tData 0.167 (0.167)\tLoss 0.0157 (0.0157)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [134][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0145 (0.0244)\tAcc@1 99.219 (99.288)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [134][200/313]\tTime 0.130 (0.123)\tData 0.005 (0.005)\tLoss 0.0199 (0.0255)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [134][300/313]\tTime 0.128 (0.122)\tData 0.009 (0.005)\tLoss 0.0103 (0.0269)\tAcc@1 100.000 (99.195)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2867)\tTest Acc@1 (92.880)\tTest Acc@5 (99.530)\n",
            "\n",
            "Epoch: [135][0/313]\tTime 0.253 (0.253)\tData 0.187 (0.187)\tLoss 0.0332 (0.0332)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [135][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.007)\tLoss 0.0109 (0.0232)\tAcc@1 100.000 (99.312)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [135][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.006)\tLoss 0.0234 (0.0241)\tAcc@1 99.219 (99.238)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [135][300/313]\tTime 0.129 (0.122)\tData 0.009 (0.005)\tLoss 0.0222 (0.0252)\tAcc@1 99.219 (99.201)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2663)\tTest Acc@1 (92.900)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [136][0/313]\tTime 0.235 (0.235)\tData 0.169 (0.169)\tLoss 0.0036 (0.0036)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [136][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.007)\tLoss 0.0043 (0.0234)\tAcc@1 100.000 (99.172)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [136][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0059 (0.0262)\tAcc@1 100.000 (99.164)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [136][300/313]\tTime 0.129 (0.122)\tData 0.004 (0.005)\tLoss 0.0134 (0.0266)\tAcc@1 100.000 (99.162)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2838)\tTest Acc@1 (92.590)\tTest Acc@5 (99.680)\n",
            "\n",
            "Epoch: [137][0/313]\tTime 0.242 (0.242)\tData 0.177 (0.177)\tLoss 0.0591 (0.0591)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [137][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0060 (0.0254)\tAcc@1 100.000 (99.288)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [137][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0084 (0.0233)\tAcc@1 100.000 (99.363)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [137][300/313]\tTime 0.140 (0.122)\tData 0.011 (0.005)\tLoss 0.0262 (0.0252)\tAcc@1 99.219 (99.252)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2795)\tTest Acc@1 (92.440)\tTest Acc@5 (99.650)\n",
            "\n",
            "Epoch: [138][0/313]\tTime 0.246 (0.246)\tData 0.178 (0.178)\tLoss 0.0236 (0.0236)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [138][100/313]\tTime 0.122 (0.123)\tData 0.003 (0.006)\tLoss 0.0502 (0.0238)\tAcc@1 98.438 (99.234)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [138][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.0849 (0.0245)\tAcc@1 96.875 (99.223)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [138][300/313]\tTime 0.128 (0.122)\tData 0.012 (0.005)\tLoss 0.0509 (0.0260)\tAcc@1 98.438 (99.172)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2846)\tTest Acc@1 (92.610)\tTest Acc@5 (99.760)\n",
            "\n",
            "Epoch: [139][0/313]\tTime 0.243 (0.243)\tData 0.177 (0.177)\tLoss 0.0480 (0.0480)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [139][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.0259 (0.0243)\tAcc@1 99.219 (99.288)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [139][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.0593 (0.0257)\tAcc@1 97.656 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [139][300/313]\tTime 0.117 (0.122)\tData 0.002 (0.004)\tLoss 0.0447 (0.0279)\tAcc@1 97.656 (99.149)\tAcc@5 100.000 (99.995)\n",
            "Test Loss  (0.2763)\tTest Acc@1 (92.570)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [140][0/313]\tTime 0.239 (0.239)\tData 0.172 (0.172)\tLoss 0.0062 (0.0062)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [140][100/313]\tTime 0.117 (0.124)\tData 0.002 (0.008)\tLoss 0.0351 (0.0284)\tAcc@1 99.219 (99.080)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [140][200/313]\tTime 0.139 (0.123)\tData 0.005 (0.006)\tLoss 0.0724 (0.0314)\tAcc@1 98.438 (98.986)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [140][300/313]\tTime 0.127 (0.123)\tData 0.013 (0.005)\tLoss 0.0463 (0.0327)\tAcc@1 97.656 (98.936)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2979)\tTest Acc@1 (92.290)\tTest Acc@5 (99.660)\n",
            "\n",
            "Epoch: [141][0/313]\tTime 0.330 (0.330)\tData 0.275 (0.275)\tLoss 0.0084 (0.0084)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [141][100/313]\tTime 0.122 (0.124)\tData 0.002 (0.007)\tLoss 0.0201 (0.0318)\tAcc@1 99.219 (99.025)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [141][200/313]\tTime 0.113 (0.123)\tData 0.018 (0.005)\tLoss 0.0467 (0.0330)\tAcc@1 98.438 (98.935)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [141][300/313]\tTime 0.098 (0.122)\tData 0.003 (0.005)\tLoss 0.0881 (0.0318)\tAcc@1 96.875 (98.980)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2893)\tTest Acc@1 (91.980)\tTest Acc@5 (99.640)\n",
            "\n",
            "Epoch: [142][0/313]\tTime 0.240 (0.240)\tData 0.174 (0.174)\tLoss 0.0214 (0.0214)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [142][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.0172 (0.0275)\tAcc@1 100.000 (99.188)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [142][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.004)\tLoss 0.0320 (0.0315)\tAcc@1 99.219 (98.997)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [142][300/313]\tTime 0.111 (0.122)\tData 0.007 (0.004)\tLoss 0.0174 (0.0328)\tAcc@1 99.219 (98.928)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2892)\tTest Acc@1 (92.480)\tTest Acc@5 (99.710)\n",
            "\n",
            "Epoch: [143][0/313]\tTime 0.241 (0.241)\tData 0.178 (0.178)\tLoss 0.0319 (0.0319)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [143][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.007)\tLoss 0.0145 (0.0373)\tAcc@1 99.219 (98.693)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [143][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0071 (0.0346)\tAcc@1 100.000 (98.850)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [143][300/313]\tTime 0.115 (0.122)\tData 0.012 (0.005)\tLoss 0.0196 (0.0341)\tAcc@1 99.219 (98.861)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2675)\tTest Acc@1 (93.030)\tTest Acc@5 (99.670)\n",
            "\n",
            "Epoch: [144][0/313]\tTime 0.244 (0.244)\tData 0.188 (0.188)\tLoss 0.0209 (0.0209)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [144][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0103 (0.0299)\tAcc@1 100.000 (99.056)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [144][200/313]\tTime 0.111 (0.122)\tData 0.002 (0.005)\tLoss 0.0064 (0.0298)\tAcc@1 100.000 (99.118)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [144][300/313]\tTime 0.119 (0.122)\tData 0.009 (0.004)\tLoss 0.0341 (0.0295)\tAcc@1 99.219 (99.120)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2727)\tTest Acc@1 (92.440)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [145][0/313]\tTime 0.242 (0.242)\tData 0.169 (0.169)\tLoss 0.0266 (0.0266)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [145][100/313]\tTime 0.115 (0.123)\tData 0.009 (0.005)\tLoss 0.0173 (0.0348)\tAcc@1 99.219 (98.871)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [145][200/313]\tTime 0.122 (0.122)\tData 0.004 (0.005)\tLoss 0.0273 (0.0343)\tAcc@1 99.219 (98.908)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [145][300/313]\tTime 0.122 (0.122)\tData 0.007 (0.005)\tLoss 0.0165 (0.0355)\tAcc@1 100.000 (98.863)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2909)\tTest Acc@1 (92.180)\tTest Acc@5 (99.730)\n",
            "\n",
            "Epoch: [146][0/313]\tTime 0.236 (0.236)\tData 0.169 (0.169)\tLoss 0.0150 (0.0150)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [146][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0067 (0.0303)\tAcc@1 100.000 (99.072)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [146][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0097 (0.0327)\tAcc@1 100.000 (98.974)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [146][300/313]\tTime 0.134 (0.122)\tData 0.016 (0.005)\tLoss 0.0431 (0.0344)\tAcc@1 98.438 (98.920)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2878)\tTest Acc@1 (92.050)\tTest Acc@5 (99.690)\n",
            "\n",
            "Epoch: [147][0/313]\tTime 0.236 (0.236)\tData 0.169 (0.169)\tLoss 0.0094 (0.0094)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [147][100/313]\tTime 0.120 (0.123)\tData 0.003 (0.006)\tLoss 0.0303 (0.0351)\tAcc@1 99.219 (98.956)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [147][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0686 (0.0347)\tAcc@1 98.438 (98.877)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [147][300/313]\tTime 0.126 (0.122)\tData 0.014 (0.004)\tLoss 0.0292 (0.0356)\tAcc@1 99.219 (98.835)\tAcc@5 100.000 (99.995)\n",
            "Test Loss  (0.2904)\tTest Acc@1 (92.120)\tTest Acc@5 (99.590)\n",
            "\n",
            "Epoch: [148][0/313]\tTime 0.228 (0.228)\tData 0.162 (0.162)\tLoss 0.0349 (0.0349)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [148][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0448 (0.0344)\tAcc@1 98.438 (98.933)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [148][200/313]\tTime 0.121 (0.122)\tData 0.004 (0.005)\tLoss 0.0221 (0.0330)\tAcc@1 99.219 (98.943)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [148][300/313]\tTime 0.097 (0.122)\tData 0.002 (0.005)\tLoss 0.0399 (0.0332)\tAcc@1 98.438 (98.941)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.3074)\tTest Acc@1 (92.180)\tTest Acc@5 (99.640)\n",
            "\n",
            "Epoch: [149][0/313]\tTime 0.223 (0.223)\tData 0.168 (0.168)\tLoss 0.0365 (0.0365)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [149][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0378 (0.0272)\tAcc@1 98.438 (99.180)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [149][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.0381 (0.0323)\tAcc@1 98.438 (98.993)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [149][300/313]\tTime 0.123 (0.122)\tData 0.006 (0.005)\tLoss 0.0629 (0.0344)\tAcc@1 98.438 (98.902)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2721)\tTest Acc@1 (92.450)\tTest Acc@5 (99.750)\n",
            "\n",
            "Epoch: [150][0/313]\tTime 0.241 (0.241)\tData 0.179 (0.179)\tLoss 0.0296 (0.0296)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [150][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0283 (0.0265)\tAcc@1 99.219 (99.134)\tAcc@5 100.000 (99.992)\n",
            "Epoch: [150][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.0430 (0.0219)\tAcc@1 98.438 (99.296)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [150][300/313]\tTime 0.112 (0.122)\tData 0.002 (0.004)\tLoss 0.0038 (0.0190)\tAcc@1 100.000 (99.424)\tAcc@5 100.000 (99.995)\n",
            "Test Loss  (0.2345)\tTest Acc@1 (93.540)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [151][0/313]\tTime 0.240 (0.240)\tData 0.177 (0.177)\tLoss 0.0027 (0.0027)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [151][100/313]\tTime 0.110 (0.124)\tData 0.014 (0.008)\tLoss 0.0099 (0.0113)\tAcc@1 100.000 (99.760)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [151][200/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.0068 (0.0115)\tAcc@1 100.000 (99.740)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [151][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0079 (0.0108)\tAcc@1 100.000 (99.764)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2201)\tTest Acc@1 (93.970)\tTest Acc@5 (99.790)\n",
            "\n",
            "Epoch: [152][0/313]\tTime 0.269 (0.269)\tData 0.205 (0.205)\tLoss 0.0104 (0.0104)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [152][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.007)\tLoss 0.0067 (0.0109)\tAcc@1 100.000 (99.768)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [152][200/313]\tTime 0.123 (0.123)\tData 0.002 (0.005)\tLoss 0.0174 (0.0103)\tAcc@1 100.000 (99.771)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [152][300/313]\tTime 0.163 (0.123)\tData 0.056 (0.005)\tLoss 0.0061 (0.0094)\tAcc@1 100.000 (99.782)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2242)\tTest Acc@1 (94.240)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [153][0/313]\tTime 0.259 (0.259)\tData 0.194 (0.194)\tLoss 0.0051 (0.0051)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [153][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0028 (0.0071)\tAcc@1 100.000 (99.876)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [153][200/313]\tTime 0.125 (0.123)\tData 0.003 (0.004)\tLoss 0.0111 (0.0071)\tAcc@1 100.000 (99.891)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [153][300/313]\tTime 0.115 (0.122)\tData 0.005 (0.004)\tLoss 0.0085 (0.0069)\tAcc@1 99.219 (99.883)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2253)\tTest Acc@1 (94.110)\tTest Acc@5 (99.810)\n",
            "\n",
            "Epoch: [154][0/313]\tTime 0.244 (0.244)\tData 0.177 (0.177)\tLoss 0.0026 (0.0026)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [154][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0248 (0.0061)\tAcc@1 99.219 (99.884)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [154][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.0063 (0.0064)\tAcc@1 100.000 (99.899)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [154][300/313]\tTime 0.128 (0.122)\tData 0.002 (0.005)\tLoss 0.0020 (0.0064)\tAcc@1 100.000 (99.886)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2265)\tTest Acc@1 (94.230)\tTest Acc@5 (99.860)\n",
            "\n",
            "Epoch: [155][0/313]\tTime 0.241 (0.241)\tData 0.180 (0.180)\tLoss 0.0065 (0.0065)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [155][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.0024 (0.0056)\tAcc@1 100.000 (99.923)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [155][200/313]\tTime 0.122 (0.122)\tData 0.003 (0.005)\tLoss 0.0058 (0.0054)\tAcc@1 100.000 (99.922)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [155][300/313]\tTime 0.114 (0.122)\tData 0.002 (0.005)\tLoss 0.0054 (0.0052)\tAcc@1 100.000 (99.927)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2222)\tTest Acc@1 (94.200)\tTest Acc@5 (99.760)\n",
            "\n",
            "Epoch: [156][0/313]\tTime 0.252 (0.252)\tData 0.185 (0.185)\tLoss 0.0094 (0.0094)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [156][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0096 (0.0060)\tAcc@1 100.000 (99.892)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [156][200/313]\tTime 0.120 (0.122)\tData 0.003 (0.005)\tLoss 0.0078 (0.0056)\tAcc@1 100.000 (99.918)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [156][300/313]\tTime 0.122 (0.123)\tData 0.005 (0.006)\tLoss 0.0042 (0.0052)\tAcc@1 100.000 (99.925)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2293)\tTest Acc@1 (94.400)\tTest Acc@5 (99.720)\n",
            "\n",
            "Epoch: [157][0/313]\tTime 0.263 (0.263)\tData 0.192 (0.192)\tLoss 0.0037 (0.0037)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [157][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0022 (0.0050)\tAcc@1 100.000 (99.930)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [157][200/313]\tTime 0.118 (0.122)\tData 0.002 (0.005)\tLoss 0.0016 (0.0054)\tAcc@1 100.000 (99.911)\tAcc@5 100.000 (99.996)\n",
            "Epoch: [157][300/313]\tTime 0.119 (0.122)\tData 0.005 (0.005)\tLoss 0.0055 (0.0052)\tAcc@1 100.000 (99.922)\tAcc@5 100.000 (99.997)\n",
            "Test Loss  (0.2304)\tTest Acc@1 (93.850)\tTest Acc@5 (99.810)\n",
            "\n",
            "Epoch: [158][0/313]\tTime 0.242 (0.242)\tData 0.175 (0.175)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [158][100/313]\tTime 0.121 (0.123)\tData 0.003 (0.006)\tLoss 0.0043 (0.0052)\tAcc@1 100.000 (99.938)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [158][200/313]\tTime 0.125 (0.122)\tData 0.002 (0.005)\tLoss 0.0022 (0.0048)\tAcc@1 100.000 (99.949)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [158][300/313]\tTime 0.131 (0.122)\tData 0.010 (0.004)\tLoss 0.0032 (0.0049)\tAcc@1 100.000 (99.933)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2287)\tTest Acc@1 (94.360)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [159][0/313]\tTime 0.225 (0.225)\tData 0.169 (0.169)\tLoss 0.0053 (0.0053)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [159][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.0021 (0.0042)\tAcc@1 100.000 (99.954)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [159][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0070 (0.0042)\tAcc@1 100.000 (99.946)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [159][300/313]\tTime 0.126 (0.122)\tData 0.013 (0.005)\tLoss 0.0022 (0.0042)\tAcc@1 100.000 (99.945)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2312)\tTest Acc@1 (94.150)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [160][0/313]\tTime 0.249 (0.249)\tData 0.174 (0.174)\tLoss 0.0022 (0.0022)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [160][100/313]\tTime 0.125 (0.123)\tData 0.002 (0.006)\tLoss 0.0020 (0.0046)\tAcc@1 100.000 (99.946)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [160][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0046 (0.0045)\tAcc@1 100.000 (99.938)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [160][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.0025 (0.0044)\tAcc@1 100.000 (99.940)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2336)\tTest Acc@1 (94.200)\tTest Acc@5 (99.810)\n",
            "\n",
            "Epoch: [161][0/313]\tTime 0.236 (0.236)\tData 0.174 (0.174)\tLoss 0.0024 (0.0024)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [161][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0034 (0.0042)\tAcc@1 100.000 (99.946)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [161][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.0018 (0.0041)\tAcc@1 100.000 (99.946)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [161][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0088 (0.0041)\tAcc@1 100.000 (99.940)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2261)\tTest Acc@1 (94.350)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [162][0/313]\tTime 0.220 (0.220)\tData 0.159 (0.159)\tLoss 0.0014 (0.0014)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [162][100/313]\tTime 0.121 (0.124)\tData 0.002 (0.008)\tLoss 0.0043 (0.0039)\tAcc@1 100.000 (99.938)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [162][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0020 (0.0044)\tAcc@1 100.000 (99.942)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [162][300/313]\tTime 0.103 (0.122)\tData 0.002 (0.005)\tLoss 0.0014 (0.0043)\tAcc@1 100.000 (99.951)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2304)\tTest Acc@1 (94.320)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [163][0/313]\tTime 0.231 (0.231)\tData 0.169 (0.169)\tLoss 0.0042 (0.0042)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [163][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0012 (0.0042)\tAcc@1 100.000 (99.938)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [163][200/313]\tTime 0.124 (0.122)\tData 0.002 (0.005)\tLoss 0.0046 (0.0042)\tAcc@1 100.000 (99.938)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [163][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0036 (0.0040)\tAcc@1 100.000 (99.945)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2294)\tTest Acc@1 (94.320)\tTest Acc@5 (99.740)\n",
            "\n",
            "Epoch: [164][0/313]\tTime 0.229 (0.229)\tData 0.167 (0.167)\tLoss 0.0018 (0.0018)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [164][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0029 (0.0034)\tAcc@1 100.000 (99.969)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [164][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0059 (0.0035)\tAcc@1 100.000 (99.965)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [164][300/313]\tTime 0.108 (0.122)\tData 0.003 (0.005)\tLoss 0.0018 (0.0036)\tAcc@1 100.000 (99.958)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2339)\tTest Acc@1 (94.250)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [165][0/313]\tTime 0.263 (0.263)\tData 0.205 (0.205)\tLoss 0.0019 (0.0019)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [165][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0069 (0.0040)\tAcc@1 99.219 (99.930)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [165][200/313]\tTime 0.124 (0.122)\tData 0.004 (0.005)\tLoss 0.0022 (0.0038)\tAcc@1 100.000 (99.949)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [165][300/313]\tTime 0.132 (0.122)\tData 0.008 (0.005)\tLoss 0.0015 (0.0037)\tAcc@1 100.000 (99.945)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2197)\tTest Acc@1 (94.410)\tTest Acc@5 (99.830)\n",
            "\n",
            "Epoch: [166][0/313]\tTime 0.265 (0.265)\tData 0.197 (0.197)\tLoss 0.0028 (0.0028)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [166][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0034 (0.0040)\tAcc@1 100.000 (99.961)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [166][200/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0022 (0.0040)\tAcc@1 100.000 (99.949)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [166][300/313]\tTime 0.131 (0.122)\tData 0.003 (0.004)\tLoss 0.0033 (0.0037)\tAcc@1 100.000 (99.958)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2253)\tTest Acc@1 (94.330)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [167][0/313]\tTime 0.243 (0.243)\tData 0.180 (0.180)\tLoss 0.0018 (0.0018)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [167][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0019 (0.0032)\tAcc@1 100.000 (99.954)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [167][200/313]\tTime 0.125 (0.123)\tData 0.006 (0.005)\tLoss 0.0031 (0.0032)\tAcc@1 100.000 (99.961)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [167][300/313]\tTime 0.107 (0.123)\tData 0.005 (0.006)\tLoss 0.0017 (0.0032)\tAcc@1 100.000 (99.961)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2286)\tTest Acc@1 (94.250)\tTest Acc@5 (99.740)\n",
            "\n",
            "Epoch: [168][0/313]\tTime 0.238 (0.238)\tData 0.174 (0.174)\tLoss 0.0027 (0.0027)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [168][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.0052 (0.0030)\tAcc@1 100.000 (99.969)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [168][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0017 (0.0030)\tAcc@1 100.000 (99.973)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [168][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0013 (0.0033)\tAcc@1 100.000 (99.969)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2226)\tTest Acc@1 (94.460)\tTest Acc@5 (99.730)\n",
            "\n",
            "Epoch: [169][0/313]\tTime 0.260 (0.260)\tData 0.195 (0.195)\tLoss 0.0035 (0.0035)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [169][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0018 (0.0035)\tAcc@1 100.000 (99.954)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [169][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0034 (0.0036)\tAcc@1 100.000 (99.957)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [169][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0095 (0.0034)\tAcc@1 100.000 (99.958)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2288)\tTest Acc@1 (94.180)\tTest Acc@5 (99.750)\n",
            "\n",
            "Epoch: [170][0/313]\tTime 0.242 (0.242)\tData 0.176 (0.176)\tLoss 0.0030 (0.0030)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [170][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.0089 (0.0034)\tAcc@1 99.219 (99.946)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [170][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0050 (0.0032)\tAcc@1 100.000 (99.961)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [170][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0017 (0.0032)\tAcc@1 100.000 (99.958)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2252)\tTest Acc@1 (94.160)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [171][0/313]\tTime 0.240 (0.240)\tData 0.179 (0.179)\tLoss 0.0051 (0.0051)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [171][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0011 (0.0035)\tAcc@1 100.000 (99.938)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [171][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0022 (0.0034)\tAcc@1 100.000 (99.938)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [171][300/313]\tTime 0.123 (0.122)\tData 0.002 (0.004)\tLoss 0.0011 (0.0032)\tAcc@1 100.000 (99.951)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2281)\tTest Acc@1 (94.260)\tTest Acc@5 (99.820)\n",
            "\n",
            "Epoch: [172][0/313]\tTime 0.239 (0.239)\tData 0.178 (0.178)\tLoss 0.0044 (0.0044)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [172][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0017 (0.0030)\tAcc@1 100.000 (99.954)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [172][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.0016 (0.0031)\tAcc@1 100.000 (99.961)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [172][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0015 (0.0030)\tAcc@1 100.000 (99.969)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2249)\tTest Acc@1 (94.290)\tTest Acc@5 (99.760)\n",
            "\n",
            "Epoch: [173][0/313]\tTime 0.270 (0.270)\tData 0.202 (0.202)\tLoss 0.0018 (0.0018)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [173][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.007)\tLoss 0.0027 (0.0029)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [173][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0050 (0.0029)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [173][300/313]\tTime 0.125 (0.122)\tData 0.002 (0.005)\tLoss 0.0023 (0.0029)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2256)\tTest Acc@1 (94.220)\tTest Acc@5 (99.730)\n",
            "\n",
            "Epoch: [174][0/313]\tTime 0.218 (0.218)\tData 0.163 (0.163)\tLoss 0.0014 (0.0014)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [174][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0026 (0.0026)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [174][200/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.0017 (0.0028)\tAcc@1 100.000 (99.981)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [174][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0012 (0.0027)\tAcc@1 100.000 (99.987)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2274)\tTest Acc@1 (94.470)\tTest Acc@5 (99.820)\n",
            "\n",
            "Epoch: [175][0/313]\tTime 0.254 (0.254)\tData 0.184 (0.184)\tLoss 0.0045 (0.0045)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [175][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0011 (0.0030)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [175][200/313]\tTime 0.119 (0.122)\tData 0.004 (0.005)\tLoss 0.0043 (0.0029)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [175][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0031 (0.0030)\tAcc@1 100.000 (99.974)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2271)\tTest Acc@1 (94.310)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [176][0/313]\tTime 0.244 (0.244)\tData 0.177 (0.177)\tLoss 0.0018 (0.0018)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [176][100/313]\tTime 0.127 (0.123)\tData 0.002 (0.006)\tLoss 0.0021 (0.0026)\tAcc@1 100.000 (99.985)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [176][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0037 (0.0028)\tAcc@1 100.000 (99.969)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [176][300/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0342 (0.0029)\tAcc@1 99.219 (99.971)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2318)\tTest Acc@1 (94.330)\tTest Acc@5 (99.760)\n",
            "\n",
            "Epoch: [177][0/313]\tTime 0.396 (0.396)\tData 0.322 (0.322)\tLoss 0.0023 (0.0023)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [177][100/313]\tTime 0.121 (0.124)\tData 0.007 (0.007)\tLoss 0.0016 (0.0025)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [177][200/313]\tTime 0.119 (0.123)\tData 0.002 (0.006)\tLoss 0.0015 (0.0026)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [177][300/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.0017 (0.0027)\tAcc@1 100.000 (99.982)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2315)\tTest Acc@1 (94.190)\tTest Acc@5 (99.850)\n",
            "\n",
            "Epoch: [178][0/313]\tTime 0.319 (0.319)\tData 0.259 (0.259)\tLoss 0.0025 (0.0025)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [178][100/313]\tTime 0.108 (0.124)\tData 0.007 (0.007)\tLoss 0.0016 (0.0028)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [178][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.0018 (0.0028)\tAcc@1 100.000 (99.988)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [178][300/313]\tTime 0.122 (0.123)\tData 0.009 (0.005)\tLoss 0.0016 (0.0028)\tAcc@1 100.000 (99.984)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2258)\tTest Acc@1 (94.280)\tTest Acc@5 (99.750)\n",
            "\n",
            "Epoch: [179][0/313]\tTime 0.267 (0.267)\tData 0.206 (0.206)\tLoss 0.0186 (0.0186)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [179][100/313]\tTime 0.119 (0.123)\tData 0.002 (0.007)\tLoss 0.0013 (0.0027)\tAcc@1 100.000 (99.961)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [179][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0014 (0.0026)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [179][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0057 (0.0026)\tAcc@1 100.000 (99.979)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2284)\tTest Acc@1 (94.170)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [180][0/313]\tTime 0.316 (0.316)\tData 0.248 (0.248)\tLoss 0.0013 (0.0013)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [180][100/313]\tTime 0.121 (0.124)\tData 0.002 (0.006)\tLoss 0.0024 (0.0027)\tAcc@1 100.000 (99.969)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [180][200/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.0022 (0.0025)\tAcc@1 100.000 (99.981)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [180][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0021 (0.0025)\tAcc@1 100.000 (99.984)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2277)\tTest Acc@1 (94.260)\tTest Acc@5 (99.740)\n",
            "\n",
            "Epoch: [181][0/313]\tTime 0.340 (0.340)\tData 0.271 (0.271)\tLoss 0.0029 (0.0029)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [181][100/313]\tTime 0.122 (0.124)\tData 0.002 (0.007)\tLoss 0.0014 (0.0028)\tAcc@1 100.000 (99.985)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [181][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0030 (0.0027)\tAcc@1 100.000 (99.988)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [181][300/313]\tTime 0.120 (0.122)\tData 0.002 (0.005)\tLoss 0.0031 (0.0026)\tAcc@1 100.000 (99.987)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2257)\tTest Acc@1 (94.580)\tTest Acc@5 (99.790)\n",
            "\n",
            "Epoch: [182][0/313]\tTime 0.410 (0.410)\tData 0.323 (0.323)\tLoss 0.0021 (0.0021)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [182][100/313]\tTime 0.120 (0.124)\tData 0.002 (0.007)\tLoss 0.0014 (0.0027)\tAcc@1 100.000 (99.985)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [182][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0017 (0.0026)\tAcc@1 100.000 (99.984)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [182][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0087 (0.0026)\tAcc@1 100.000 (99.987)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2290)\tTest Acc@1 (94.310)\tTest Acc@5 (99.720)\n",
            "\n",
            "Epoch: [183][0/313]\tTime 0.365 (0.365)\tData 0.281 (0.281)\tLoss 0.0020 (0.0020)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [183][100/313]\tTime 0.121 (0.124)\tData 0.002 (0.006)\tLoss 0.0014 (0.0023)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [183][200/313]\tTime 0.130 (0.123)\tData 0.002 (0.005)\tLoss 0.0016 (0.0024)\tAcc@1 100.000 (99.988)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [183][300/313]\tTime 0.120 (0.123)\tData 0.002 (0.005)\tLoss 0.0018 (0.0024)\tAcc@1 100.000 (99.990)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2351)\tTest Acc@1 (94.350)\tTest Acc@5 (99.730)\n",
            "\n",
            "Epoch: [184][0/313]\tTime 0.277 (0.277)\tData 0.197 (0.197)\tLoss 0.0074 (0.0074)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [184][100/313]\tTime 0.119 (0.124)\tData 0.013 (0.007)\tLoss 0.0018 (0.0028)\tAcc@1 100.000 (99.985)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [184][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0021 (0.0025)\tAcc@1 100.000 (99.988)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [184][300/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0013 (0.0025)\tAcc@1 100.000 (99.990)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2235)\tTest Acc@1 (94.440)\tTest Acc@5 (99.810)\n",
            "\n",
            "Epoch: [185][0/313]\tTime 0.414 (0.414)\tData 0.330 (0.330)\tLoss 0.0037 (0.0037)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [185][100/313]\tTime 0.109 (0.124)\tData 0.003 (0.006)\tLoss 0.0041 (0.0022)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [185][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0051 (0.0022)\tAcc@1 100.000 (99.996)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [185][300/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0036 (0.0024)\tAcc@1 100.000 (99.987)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2256)\tTest Acc@1 (94.490)\tTest Acc@5 (99.820)\n",
            "\n",
            "Epoch: [186][0/313]\tTime 0.256 (0.256)\tData 0.194 (0.194)\tLoss 0.0021 (0.0021)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [186][100/313]\tTime 0.115 (0.123)\tData 0.002 (0.006)\tLoss 0.0034 (0.0023)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [186][200/313]\tTime 0.122 (0.122)\tData 0.004 (0.004)\tLoss 0.0033 (0.0024)\tAcc@1 100.000 (99.988)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [186][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.004)\tLoss 0.0011 (0.0025)\tAcc@1 100.000 (99.984)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2342)\tTest Acc@1 (94.190)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [187][0/313]\tTime 0.231 (0.231)\tData 0.170 (0.170)\tLoss 0.0027 (0.0027)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [187][100/313]\tTime 0.117 (0.123)\tData 0.009 (0.005)\tLoss 0.0014 (0.0021)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [187][200/313]\tTime 0.127 (0.122)\tData 0.009 (0.004)\tLoss 0.0010 (0.0023)\tAcc@1 100.000 (99.988)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [187][300/313]\tTime 0.117 (0.122)\tData 0.014 (0.004)\tLoss 0.0019 (0.0023)\tAcc@1 100.000 (99.987)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2397)\tTest Acc@1 (94.200)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [188][0/313]\tTime 0.225 (0.225)\tData 0.167 (0.167)\tLoss 0.0020 (0.0020)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [188][100/313]\tTime 0.122 (0.122)\tData 0.002 (0.005)\tLoss 0.0048 (0.0023)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [188][200/313]\tTime 0.113 (0.122)\tData 0.002 (0.004)\tLoss 0.0031 (0.0022)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [188][300/313]\tTime 0.135 (0.122)\tData 0.002 (0.004)\tLoss 0.0014 (0.0023)\tAcc@1 100.000 (99.984)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2269)\tTest Acc@1 (94.290)\tTest Acc@5 (99.740)\n",
            "\n",
            "Epoch: [189][0/313]\tTime 0.254 (0.254)\tData 0.180 (0.180)\tLoss 0.0015 (0.0015)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [189][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0020 (0.0023)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [189][200/313]\tTime 0.130 (0.122)\tData 0.009 (0.005)\tLoss 0.0013 (0.0024)\tAcc@1 100.000 (99.984)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [189][300/313]\tTime 0.119 (0.122)\tData 0.010 (0.004)\tLoss 0.0025 (0.0025)\tAcc@1 100.000 (99.979)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2231)\tTest Acc@1 (94.550)\tTest Acc@5 (99.750)\n",
            "\n",
            "Epoch: [190][0/313]\tTime 0.229 (0.229)\tData 0.165 (0.165)\tLoss 0.0052 (0.0052)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [190][100/313]\tTime 0.116 (0.123)\tData 0.004 (0.006)\tLoss 0.0013 (0.0025)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [190][200/313]\tTime 0.111 (0.122)\tData 0.007 (0.005)\tLoss 0.0023 (0.0024)\tAcc@1 100.000 (99.984)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [190][300/313]\tTime 0.120 (0.122)\tData 0.009 (0.004)\tLoss 0.0017 (0.0025)\tAcc@1 100.000 (99.982)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2203)\tTest Acc@1 (94.500)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [191][0/313]\tTime 0.233 (0.233)\tData 0.167 (0.167)\tLoss 0.0012 (0.0012)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [191][100/313]\tTime 0.122 (0.123)\tData 0.003 (0.006)\tLoss 0.0021 (0.0024)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [191][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0011 (0.0022)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [191][300/313]\tTime 0.125 (0.122)\tData 0.015 (0.004)\tLoss 0.0021 (0.0022)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2289)\tTest Acc@1 (94.330)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [192][0/313]\tTime 0.244 (0.244)\tData 0.181 (0.181)\tLoss 0.0012 (0.0012)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [192][100/313]\tTime 0.122 (0.123)\tData 0.002 (0.006)\tLoss 0.0009 (0.0026)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [192][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0018 (0.0023)\tAcc@1 100.000 (99.988)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [192][300/313]\tTime 0.117 (0.122)\tData 0.012 (0.004)\tLoss 0.0020 (0.0023)\tAcc@1 100.000 (99.987)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2230)\tTest Acc@1 (94.340)\tTest Acc@5 (99.810)\n",
            "\n",
            "Epoch: [193][0/313]\tTime 0.241 (0.241)\tData 0.183 (0.183)\tLoss 0.0014 (0.0014)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [193][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0015 (0.0024)\tAcc@1 100.000 (99.985)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [193][200/313]\tTime 0.121 (0.123)\tData 0.002 (0.005)\tLoss 0.0022 (0.0025)\tAcc@1 100.000 (99.988)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [193][300/313]\tTime 0.106 (0.122)\tData 0.007 (0.004)\tLoss 0.0050 (0.0024)\tAcc@1 100.000 (99.987)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2201)\tTest Acc@1 (94.470)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [194][0/313]\tTime 0.349 (0.349)\tData 0.257 (0.257)\tLoss 0.0015 (0.0015)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [194][100/313]\tTime 0.120 (0.124)\tData 0.002 (0.007)\tLoss 0.0017 (0.0023)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [194][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0022 (0.0023)\tAcc@1 100.000 (99.981)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [194][300/313]\tTime 0.101 (0.122)\tData 0.008 (0.005)\tLoss 0.0022 (0.0023)\tAcc@1 100.000 (99.979)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2294)\tTest Acc@1 (94.360)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [195][0/313]\tTime 0.248 (0.248)\tData 0.178 (0.178)\tLoss 0.0049 (0.0049)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [195][100/313]\tTime 0.123 (0.123)\tData 0.002 (0.006)\tLoss 0.0015 (0.0021)\tAcc@1 100.000 (99.992)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [195][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0011 (0.0021)\tAcc@1 100.000 (99.996)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [195][300/313]\tTime 0.130 (0.123)\tData 0.002 (0.005)\tLoss 0.0012 (0.0021)\tAcc@1 100.000 (99.997)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2215)\tTest Acc@1 (94.500)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [196][0/313]\tTime 0.223 (0.223)\tData 0.160 (0.160)\tLoss 0.0011 (0.0011)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [196][100/313]\tTime 0.121 (0.123)\tData 0.002 (0.006)\tLoss 0.0010 (0.0024)\tAcc@1 100.000 (99.985)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [196][200/313]\tTime 0.121 (0.122)\tData 0.003 (0.005)\tLoss 0.0040 (0.0025)\tAcc@1 100.000 (99.984)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [196][300/313]\tTime 0.117 (0.122)\tData 0.002 (0.004)\tLoss 0.0022 (0.0023)\tAcc@1 100.000 (99.990)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2206)\tTest Acc@1 (94.560)\tTest Acc@5 (99.810)\n",
            "\n",
            "Epoch: [197][0/313]\tTime 0.245 (0.245)\tData 0.179 (0.179)\tLoss 0.0013 (0.0013)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [197][100/313]\tTime 0.120 (0.123)\tData 0.002 (0.006)\tLoss 0.0009 (0.0026)\tAcc@1 100.000 (99.961)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [197][200/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0017 (0.0024)\tAcc@1 100.000 (99.973)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [197][300/313]\tTime 0.121 (0.122)\tData 0.002 (0.005)\tLoss 0.0013 (0.0023)\tAcc@1 100.000 (99.982)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2184)\tTest Acc@1 (94.710)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [198][0/313]\tTime 0.269 (0.269)\tData 0.207 (0.207)\tLoss 0.0012 (0.0012)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [198][100/313]\tTime 0.119 (0.123)\tData 0.009 (0.006)\tLoss 0.0013 (0.0020)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [198][200/313]\tTime 0.122 (0.123)\tData 0.002 (0.005)\tLoss 0.0066 (0.0021)\tAcc@1 100.000 (99.996)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [198][300/313]\tTime 0.126 (0.122)\tData 0.003 (0.004)\tLoss 0.0018 (0.0021)\tAcc@1 100.000 (99.997)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2218)\tTest Acc@1 (94.550)\tTest Acc@5 (99.850)\n",
            "\n",
            "Epoch: [199][0/313]\tTime 0.235 (0.235)\tData 0.175 (0.175)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [199][100/313]\tTime 0.121 (0.124)\tData 0.002 (0.008)\tLoss 0.0015 (0.0022)\tAcc@1 100.000 (99.977)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [199][200/313]\tTime 0.125 (0.123)\tData 0.008 (0.006)\tLoss 0.0014 (0.0023)\tAcc@1 100.000 (99.969)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [199][300/313]\tTime 0.132 (0.123)\tData 0.004 (0.005)\tLoss 0.0019 (0.0025)\tAcc@1 100.000 (99.966)\tAcc@5 100.000 (100.000)\n",
            "Test Loss  (0.2257)\tTest Acc@1 (94.520)\tTest Acc@5 (99.720)\n",
            "\n",
            "Best model's validation acc: 94.7100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "model_path = '/content/gdrive/My Drive/ResNet_18_full.pth'\n",
        "torch.save(model.state_dict(), model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Oaq3J0Ga_M",
        "outputId": "52629caa-9d31-4ced-ac8c-1cd96da9c931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model_path = '/content/gdrive/My Drive/SNL_results/ResNet_18_SNL_pretrained_70epochs.pth'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "baseline_model = ResNet18_SNL()\n",
        "baseline_model.load_state_dict(torch.load(baseline_model_path, weights_only=True, map_location=device))\n",
        "baseline_model.to(device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "99dgADYSAaxm",
        "outputId": "c6e6a1d0-f672-41fa-8825-ba55537be266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18_SNL(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simultaneus training and linearization\n"
      ],
      "metadata": {
        "id": "aRh4zk9bAXn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_counting(net, threshold):\n",
        "    relu_count = 0\n",
        "    for name, param in net.named_parameters():\n",
        "        if 'alpha' in name:\n",
        "            boolean_list = param.data > threshold\n",
        "            relu_count += (boolean_list == 1).sum()\n",
        "    return relu_count"
      ],
      "metadata": {
        "id": "LOrErGAiHP45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simultaneus_training_lasso_loss(loader, model, criterion, optimizer, lasso_coef):\n",
        "  losses = AverageMeter()\n",
        "\n",
        "  # switch to train mode\n",
        "  model.train()\n",
        "\n",
        "  for i, (inputs, targets) in enumerate(loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      reg_loss = 0\n",
        "      for name, param in model.named_parameters():\n",
        "          if 'alpha' in name:\n",
        "              reg_loss += torch.norm(param, p=1)\n",
        "\n",
        "      # compute output\n",
        "      outputs = model(inputs)\n",
        "      cur_relu_count = relu_counting(model, SNL_params['relu_lin_threshold'])\n",
        "\n",
        "      # print(f\"net loss: {criterion(outputs, targets)}\")\n",
        "      # print(f\"reg loss: {reg_loss}\")\n",
        "      # print(f\"cur relu count: {cur_relu_count}\")\n",
        "      # print(f\"total loss: {criterion(outputs, targets) + lasso_coef * reg_loss}\")\n",
        "\n",
        "      loss = criterion(outputs, targets) + lasso_coef * reg_loss\n",
        "\n",
        "      losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "      # compute gradient and do SGD step\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  return losses.avg"
      ],
      "metadata": {
        "id": "Ai_O9TcQKERA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "model = copy.deepcopy(baseline_model)\n",
        "\n",
        "# Enable alpha training\n",
        "for name, param in model.named_parameters():\n",
        "    if 'alpha' in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6DHxV_nbEemi",
        "outputId": "6f0c5089-02ef-4c47-e0e7-60ca8fececba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18_SNL(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Initial Number of ReLUs: {relu_counting(model, SNL_params['relu_lin_threshold'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERoH7laYIBbB",
        "outputId": "d09f693f-00ec-4818-ee6c-40d4fc2206b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Number of ReLUs: 491520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(\n",
        "    train_dataset,  # Training dataset\n",
        "    batch_size=SNL_training_params['batch_size'],  # Number of samples per batch\n",
        "    shuffle=True,  # Shuffle the training data\n",
        "    num_workers=2  # Number of subprocesses for data loading\n",
        ")\n",
        "\n",
        "valloader = DataLoader(\n",
        "    val_dataset,  # Validation dataset\n",
        "    batch_size=SNL_training_params['batch_size'],  # Number of samples per batch\n",
        "    shuffle=False,  # No need to shuffle validation data\n",
        "    num_workers=2  # Number of subprocesses for data loading\n",
        ")\n"
      ],
      "metadata": {
        "id": "-ZJwuai7GCT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = Adam(model.parameters(), lr=SNL_training_params['lr'])\n",
        "\n",
        "init_relu_count = relu_counting(model, SNL_params['relu_lin_threshold'])\n",
        "lowest_relu_count = init_relu_count\n",
        "\n",
        "lasso_weight = SNL_params['initial_lasso_weight']\n",
        "\n",
        "for epoch in range(SNL_training_params['epochs']):\n",
        "    # Simultaneous tarining of w and alpha with KD loss.\n",
        "    train_loss = simultaneus_training_lasso_loss(trainloader, model, criterion,\n",
        "                                                 optimizer, lasso_weight)\n",
        "\n",
        "    # validation\n",
        "    cur_step = (epoch+1) * len(trainloader)\n",
        "    _, top1, _ = test(valloader, model, criterion, device, cur_step)\n",
        "\n",
        "    # counting ReLU in the neural network by using threshold.\n",
        "    cur_relu_count = relu_counting(model, SNL_params['relu_lin_threshold'])\n",
        "    print(f\"Epoch: {epoch}, Test Accuracy: {top1:.4f}, ReLU Count: {cur_relu_count}, Lasso weight: {lasso_weight:.6f}\")\n",
        "\n",
        "\n",
        "    # Lasso weight increment\n",
        "    if cur_relu_count < lowest_relu_count:\n",
        "        lowest_relu_count = cur_relu_count\n",
        "\n",
        "    elif cur_relu_count >= lowest_relu_count and epoch >= 5:\n",
        "        lasso_weight *= SNL_params['lasso_weight_factor']\n",
        "\n",
        "    if cur_relu_count <= SNL_params['relu_budget']:\n",
        "        print(f\"Achieved relu budget after epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "print(f\"After SNL Algorithm, the current ReLU Count: {cur_relu_count}, relative count: {cur_relu_count / init_relu_count:.6f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umo_YwckIjyJ",
        "outputId": "53d60b1d-4b54-4402-894c-d76758a5bf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss  (0.5574)\tTest Acc@1 (83.160)\tTest Acc@5 (99.080)\n",
            "Epoch: 0, Test Accuracy: 83.1600, ReLU Count: 491520, Lasso weight: 0.000010\n",
            "Test Loss  (0.5403)\tTest Acc@1 (84.040)\tTest Acc@5 (99.270)\n",
            "Epoch: 1, Test Accuracy: 84.0400, ReLU Count: 394543, Lasso weight: 0.000010\n",
            "Test Loss  (0.5922)\tTest Acc@1 (83.940)\tTest Acc@5 (98.910)\n",
            "Epoch: 2, Test Accuracy: 83.9400, ReLU Count: 356833, Lasso weight: 0.000010\n",
            "Test Loss  (0.6059)\tTest Acc@1 (83.980)\tTest Acc@5 (98.840)\n",
            "Epoch: 3, Test Accuracy: 83.9800, ReLU Count: 328283, Lasso weight: 0.000010\n",
            "Test Loss  (0.6766)\tTest Acc@1 (83.370)\tTest Acc@5 (98.800)\n",
            "Epoch: 4, Test Accuracy: 83.3700, ReLU Count: 302649, Lasso weight: 0.000010\n",
            "Test Loss  (0.6646)\tTest Acc@1 (83.460)\tTest Acc@5 (98.930)\n",
            "Epoch: 5, Test Accuracy: 83.4600, ReLU Count: 279142, Lasso weight: 0.000010\n",
            "Test Loss  (0.6854)\tTest Acc@1 (83.570)\tTest Acc@5 (98.990)\n",
            "Epoch: 6, Test Accuracy: 83.5700, ReLU Count: 258417, Lasso weight: 0.000010\n",
            "Test Loss  (0.6855)\tTest Acc@1 (83.880)\tTest Acc@5 (98.940)\n",
            "Epoch: 7, Test Accuracy: 83.8800, ReLU Count: 240228, Lasso weight: 0.000010\n",
            "Test Loss  (0.7184)\tTest Acc@1 (83.370)\tTest Acc@5 (98.960)\n",
            "Epoch: 8, Test Accuracy: 83.3700, ReLU Count: 223781, Lasso weight: 0.000010\n",
            "Test Loss  (0.7787)\tTest Acc@1 (83.350)\tTest Acc@5 (98.810)\n",
            "Epoch: 9, Test Accuracy: 83.3500, ReLU Count: 208900, Lasso weight: 0.000010\n",
            "Test Loss  (0.7409)\tTest Acc@1 (82.730)\tTest Acc@5 (98.810)\n",
            "Epoch: 10, Test Accuracy: 82.7300, ReLU Count: 196506, Lasso weight: 0.000010\n",
            "Test Loss  (0.7951)\tTest Acc@1 (82.300)\tTest Acc@5 (98.840)\n",
            "Epoch: 11, Test Accuracy: 82.3000, ReLU Count: 183446, Lasso weight: 0.000010\n",
            "Test Loss  (0.7534)\tTest Acc@1 (83.280)\tTest Acc@5 (98.870)\n",
            "Epoch: 12, Test Accuracy: 83.2800, ReLU Count: 172344, Lasso weight: 0.000010\n",
            "Test Loss  (0.8177)\tTest Acc@1 (83.200)\tTest Acc@5 (98.870)\n",
            "Epoch: 13, Test Accuracy: 83.2000, ReLU Count: 162317, Lasso weight: 0.000010\n",
            "Test Loss  (0.7923)\tTest Acc@1 (83.450)\tTest Acc@5 (98.830)\n",
            "Epoch: 14, Test Accuracy: 83.4500, ReLU Count: 153159, Lasso weight: 0.000010\n",
            "Test Loss  (0.8655)\tTest Acc@1 (82.600)\tTest Acc@5 (98.580)\n",
            "Epoch: 15, Test Accuracy: 82.6000, ReLU Count: 146166, Lasso weight: 0.000010\n",
            "Test Loss  (0.8390)\tTest Acc@1 (82.560)\tTest Acc@5 (98.640)\n",
            "Epoch: 16, Test Accuracy: 82.5600, ReLU Count: 138405, Lasso weight: 0.000010\n",
            "Test Loss  (0.8558)\tTest Acc@1 (81.840)\tTest Acc@5 (98.520)\n",
            "Epoch: 17, Test Accuracy: 81.8400, ReLU Count: 133654, Lasso weight: 0.000010\n",
            "Test Loss  (0.8504)\tTest Acc@1 (82.390)\tTest Acc@5 (98.650)\n",
            "Epoch: 18, Test Accuracy: 82.3900, ReLU Count: 125768, Lasso weight: 0.000010\n",
            "Test Loss  (0.8760)\tTest Acc@1 (81.860)\tTest Acc@5 (98.610)\n",
            "Epoch: 19, Test Accuracy: 81.8600, ReLU Count: 123080, Lasso weight: 0.000010\n",
            "Test Loss  (0.8441)\tTest Acc@1 (82.690)\tTest Acc@5 (98.760)\n",
            "Epoch: 20, Test Accuracy: 82.6900, ReLU Count: 115640, Lasso weight: 0.000010\n",
            "Test Loss  (0.8554)\tTest Acc@1 (83.090)\tTest Acc@5 (98.880)\n",
            "Epoch: 21, Test Accuracy: 83.0900, ReLU Count: 111709, Lasso weight: 0.000010\n",
            "Test Loss  (1.0155)\tTest Acc@1 (80.750)\tTest Acc@5 (98.740)\n",
            "Epoch: 22, Test Accuracy: 80.7500, ReLU Count: 107826, Lasso weight: 0.000010\n",
            "Test Loss  (0.8730)\tTest Acc@1 (82.620)\tTest Acc@5 (98.790)\n",
            "Epoch: 23, Test Accuracy: 82.6200, ReLU Count: 105838, Lasso weight: 0.000010\n",
            "Test Loss  (0.8808)\tTest Acc@1 (82.230)\tTest Acc@5 (98.710)\n",
            "Epoch: 24, Test Accuracy: 82.2300, ReLU Count: 103896, Lasso weight: 0.000010\n",
            "Test Loss  (0.8495)\tTest Acc@1 (82.590)\tTest Acc@5 (98.760)\n",
            "Epoch: 25, Test Accuracy: 82.5900, ReLU Count: 99242, Lasso weight: 0.000010\n",
            "Test Loss  (0.8872)\tTest Acc@1 (82.870)\tTest Acc@5 (98.780)\n",
            "Epoch: 26, Test Accuracy: 82.8700, ReLU Count: 94402, Lasso weight: 0.000010\n",
            "Test Loss  (0.9354)\tTest Acc@1 (81.540)\tTest Acc@5 (98.710)\n",
            "Epoch: 27, Test Accuracy: 81.5400, ReLU Count: 94681, Lasso weight: 0.000010\n",
            "Test Loss  (0.9817)\tTest Acc@1 (81.540)\tTest Acc@5 (98.730)\n",
            "Epoch: 28, Test Accuracy: 81.5400, ReLU Count: 89889, Lasso weight: 0.000011\n",
            "Test Loss  (0.8899)\tTest Acc@1 (82.960)\tTest Acc@5 (98.640)\n",
            "Epoch: 29, Test Accuracy: 82.9600, ReLU Count: 85850, Lasso weight: 0.000011\n",
            "Test Loss  (0.9025)\tTest Acc@1 (82.340)\tTest Acc@5 (98.660)\n",
            "Epoch: 30, Test Accuracy: 82.3400, ReLU Count: 84836, Lasso weight: 0.000011\n",
            "Test Loss  (1.0208)\tTest Acc@1 (80.870)\tTest Acc@5 (98.730)\n",
            "Epoch: 31, Test Accuracy: 80.8700, ReLU Count: 84808, Lasso weight: 0.000011\n",
            "Test Loss  (0.9841)\tTest Acc@1 (82.160)\tTest Acc@5 (98.520)\n",
            "Epoch: 32, Test Accuracy: 82.1600, ReLU Count: 79698, Lasso weight: 0.000011\n",
            "Test Loss  (0.8912)\tTest Acc@1 (82.140)\tTest Acc@5 (98.600)\n",
            "Epoch: 33, Test Accuracy: 82.1400, ReLU Count: 80230, Lasso weight: 0.000011\n",
            "Test Loss  (0.9488)\tTest Acc@1 (82.250)\tTest Acc@5 (98.570)\n",
            "Epoch: 34, Test Accuracy: 82.2500, ReLU Count: 74882, Lasso weight: 0.000012\n",
            "Test Loss  (1.0029)\tTest Acc@1 (81.770)\tTest Acc@5 (98.560)\n",
            "Epoch: 35, Test Accuracy: 81.7700, ReLU Count: 75244, Lasso weight: 0.000012\n",
            "Test Loss  (1.0334)\tTest Acc@1 (81.210)\tTest Acc@5 (98.540)\n",
            "Epoch: 36, Test Accuracy: 81.2100, ReLU Count: 73727, Lasso weight: 0.000013\n",
            "Test Loss  (1.0955)\tTest Acc@1 (80.840)\tTest Acc@5 (98.490)\n",
            "Epoch: 37, Test Accuracy: 80.8400, ReLU Count: 71561, Lasso weight: 0.000013\n",
            "Test Loss  (1.0340)\tTest Acc@1 (80.670)\tTest Acc@5 (98.610)\n",
            "Epoch: 38, Test Accuracy: 80.6700, ReLU Count: 70109, Lasso weight: 0.000013\n",
            "Test Loss  (1.1246)\tTest Acc@1 (80.690)\tTest Acc@5 (98.030)\n",
            "Epoch: 39, Test Accuracy: 80.6900, ReLU Count: 68918, Lasso weight: 0.000013\n",
            "Test Loss  (1.0817)\tTest Acc@1 (81.020)\tTest Acc@5 (98.290)\n",
            "Epoch: 40, Test Accuracy: 81.0200, ReLU Count: 64224, Lasso weight: 0.000013\n",
            "Test Loss  (1.0187)\tTest Acc@1 (81.470)\tTest Acc@5 (98.500)\n",
            "Epoch: 41, Test Accuracy: 81.4700, ReLU Count: 64104, Lasso weight: 0.000013\n",
            "Test Loss  (1.0392)\tTest Acc@1 (80.900)\tTest Acc@5 (98.500)\n",
            "Epoch: 42, Test Accuracy: 80.9000, ReLU Count: 67072, Lasso weight: 0.000013\n",
            "Test Loss  (1.0258)\tTest Acc@1 (81.380)\tTest Acc@5 (98.500)\n",
            "Epoch: 43, Test Accuracy: 81.3800, ReLU Count: 59358, Lasso weight: 0.000015\n",
            "Test Loss  (1.0609)\tTest Acc@1 (81.390)\tTest Acc@5 (98.230)\n",
            "Epoch: 44, Test Accuracy: 81.3900, ReLU Count: 58939, Lasso weight: 0.000015\n",
            "Test Loss  (1.1106)\tTest Acc@1 (80.680)\tTest Acc@5 (98.630)\n",
            "Epoch: 45, Test Accuracy: 80.6800, ReLU Count: 57394, Lasso weight: 0.000015\n",
            "Test Loss  (1.0906)\tTest Acc@1 (80.770)\tTest Acc@5 (98.540)\n",
            "Epoch: 46, Test Accuracy: 80.7700, ReLU Count: 57208, Lasso weight: 0.000015\n",
            "Test Loss  (1.1140)\tTest Acc@1 (80.680)\tTest Acc@5 (98.420)\n",
            "Epoch: 47, Test Accuracy: 80.6800, ReLU Count: 57687, Lasso weight: 0.000015\n",
            "Test Loss  (1.1422)\tTest Acc@1 (80.720)\tTest Acc@5 (98.480)\n",
            "Epoch: 48, Test Accuracy: 80.7200, ReLU Count: 54762, Lasso weight: 0.000016\n",
            "Test Loss  (1.1213)\tTest Acc@1 (80.450)\tTest Acc@5 (98.140)\n",
            "Epoch: 49, Test Accuracy: 80.4500, ReLU Count: 52958, Lasso weight: 0.000016\n",
            "Test Loss  (1.1938)\tTest Acc@1 (79.750)\tTest Acc@5 (98.590)\n",
            "Epoch: 50, Test Accuracy: 79.7500, ReLU Count: 54885, Lasso weight: 0.000016\n",
            "Test Loss  (1.0285)\tTest Acc@1 (81.100)\tTest Acc@5 (98.470)\n",
            "Epoch: 51, Test Accuracy: 81.1000, ReLU Count: 49343, Lasso weight: 0.000018\n",
            "Test Loss  (1.0870)\tTest Acc@1 (80.480)\tTest Acc@5 (98.560)\n",
            "Epoch: 52, Test Accuracy: 80.4800, ReLU Count: 49188, Lasso weight: 0.000018\n",
            "Test Loss  (1.1149)\tTest Acc@1 (80.020)\tTest Acc@5 (98.480)\n",
            "Epoch: 53, Test Accuracy: 80.0200, ReLU Count: 51052, Lasso weight: 0.000018\n",
            "Test Loss  (1.1696)\tTest Acc@1 (80.330)\tTest Acc@5 (98.400)\n",
            "Epoch: 54, Test Accuracy: 80.3300, ReLU Count: 46676, Lasso weight: 0.000019\n",
            "Test Loss  (1.1255)\tTest Acc@1 (79.580)\tTest Acc@5 (97.930)\n",
            "Epoch: 55, Test Accuracy: 79.5800, ReLU Count: 48674, Lasso weight: 0.000019\n",
            "Test Loss  (1.1173)\tTest Acc@1 (79.870)\tTest Acc@5 (98.480)\n",
            "Epoch: 56, Test Accuracy: 79.8700, ReLU Count: 42292, Lasso weight: 0.000021\n",
            "Test Loss  (1.1319)\tTest Acc@1 (80.130)\tTest Acc@5 (98.430)\n",
            "Epoch: 57, Test Accuracy: 80.1300, ReLU Count: 45008, Lasso weight: 0.000021\n",
            "Test Loss  (1.1235)\tTest Acc@1 (79.700)\tTest Acc@5 (98.170)\n",
            "Epoch: 58, Test Accuracy: 79.7000, ReLU Count: 40538, Lasso weight: 0.000024\n",
            "Test Loss  (1.1165)\tTest Acc@1 (80.190)\tTest Acc@5 (98.390)\n",
            "Epoch: 59, Test Accuracy: 80.1900, ReLU Count: 39266, Lasso weight: 0.000024\n",
            "Test Loss  (1.1359)\tTest Acc@1 (79.230)\tTest Acc@5 (98.430)\n",
            "Epoch: 60, Test Accuracy: 79.2300, ReLU Count: 40434, Lasso weight: 0.000024\n",
            "Test Loss  (1.1560)\tTest Acc@1 (79.850)\tTest Acc@5 (98.340)\n",
            "Epoch: 61, Test Accuracy: 79.8500, ReLU Count: 39269, Lasso weight: 0.000026\n",
            "Test Loss  (1.2214)\tTest Acc@1 (79.650)\tTest Acc@5 (98.010)\n",
            "Epoch: 62, Test Accuracy: 79.6500, ReLU Count: 36006, Lasso weight: 0.000029\n",
            "Test Loss  (1.1957)\tTest Acc@1 (79.100)\tTest Acc@5 (97.990)\n",
            "Epoch: 63, Test Accuracy: 79.1000, ReLU Count: 36999, Lasso weight: 0.000029\n",
            "Test Loss  (1.1967)\tTest Acc@1 (79.050)\tTest Acc@5 (98.330)\n",
            "Epoch: 64, Test Accuracy: 79.0500, ReLU Count: 34607, Lasso weight: 0.000031\n",
            "Test Loss  (1.2558)\tTest Acc@1 (78.900)\tTest Acc@5 (98.260)\n",
            "Epoch: 65, Test Accuracy: 78.9000, ReLU Count: 33406, Lasso weight: 0.000031\n",
            "Test Loss  (1.1503)\tTest Acc@1 (79.840)\tTest Acc@5 (98.240)\n",
            "Epoch: 66, Test Accuracy: 79.8400, ReLU Count: 32401, Lasso weight: 0.000031\n",
            "Test Loss  (1.3619)\tTest Acc@1 (78.050)\tTest Acc@5 (98.200)\n",
            "Epoch: 67, Test Accuracy: 78.0500, ReLU Count: 31781, Lasso weight: 0.000031\n",
            "Test Loss  (1.2301)\tTest Acc@1 (79.520)\tTest Acc@5 (98.320)\n",
            "Epoch: 68, Test Accuracy: 79.5200, ReLU Count: 32544, Lasso weight: 0.000031\n",
            "Test Loss  (1.2596)\tTest Acc@1 (78.100)\tTest Acc@5 (98.120)\n",
            "Epoch: 69, Test Accuracy: 78.1000, ReLU Count: 31394, Lasso weight: 0.000035\n",
            "Test Loss  (1.1809)\tTest Acc@1 (78.600)\tTest Acc@5 (98.080)\n",
            "Epoch: 70, Test Accuracy: 78.6000, ReLU Count: 29733, Lasso weight: 0.000035\n",
            "Test Loss  (1.1862)\tTest Acc@1 (78.900)\tTest Acc@5 (98.290)\n",
            "Epoch: 71, Test Accuracy: 78.9000, ReLU Count: 29273, Lasso weight: 0.000035\n",
            "Test Loss  (1.2429)\tTest Acc@1 (78.440)\tTest Acc@5 (98.050)\n",
            "Epoch: 72, Test Accuracy: 78.4400, ReLU Count: 28313, Lasso weight: 0.000035\n",
            "Test Loss  (1.2466)\tTest Acc@1 (77.440)\tTest Acc@5 (98.160)\n",
            "Epoch: 73, Test Accuracy: 77.4400, ReLU Count: 29235, Lasso weight: 0.000035\n",
            "Test Loss  (1.3648)\tTest Acc@1 (77.510)\tTest Acc@5 (98.120)\n",
            "Epoch: 74, Test Accuracy: 77.5100, ReLU Count: 27550, Lasso weight: 0.000038\n",
            "Test Loss  (1.2425)\tTest Acc@1 (77.950)\tTest Acc@5 (98.030)\n",
            "Epoch: 75, Test Accuracy: 77.9500, ReLU Count: 26945, Lasso weight: 0.000038\n",
            "Test Loss  (1.2545)\tTest Acc@1 (77.440)\tTest Acc@5 (98.220)\n",
            "Epoch: 76, Test Accuracy: 77.4400, ReLU Count: 25730, Lasso weight: 0.000038\n",
            "Test Loss  (1.3183)\tTest Acc@1 (77.540)\tTest Acc@5 (98.110)\n",
            "Epoch: 77, Test Accuracy: 77.5400, ReLU Count: 26128, Lasso weight: 0.000038\n",
            "Test Loss  (1.2579)\tTest Acc@1 (78.280)\tTest Acc@5 (98.010)\n",
            "Epoch: 78, Test Accuracy: 78.2800, ReLU Count: 24327, Lasso weight: 0.000042\n",
            "Test Loss  (1.3550)\tTest Acc@1 (76.860)\tTest Acc@5 (98.200)\n",
            "Epoch: 79, Test Accuracy: 76.8600, ReLU Count: 25714, Lasso weight: 0.000042\n",
            "Test Loss  (1.2859)\tTest Acc@1 (78.030)\tTest Acc@5 (98.180)\n",
            "Epoch: 80, Test Accuracy: 78.0300, ReLU Count: 22077, Lasso weight: 0.000046\n",
            "Test Loss  (1.3743)\tTest Acc@1 (77.700)\tTest Acc@5 (97.900)\n",
            "Epoch: 81, Test Accuracy: 77.7000, ReLU Count: 22212, Lasso weight: 0.000046\n",
            "Test Loss  (1.3188)\tTest Acc@1 (77.180)\tTest Acc@5 (97.620)\n",
            "Epoch: 82, Test Accuracy: 77.1800, ReLU Count: 23031, Lasso weight: 0.000051\n",
            "Test Loss  (1.2836)\tTest Acc@1 (77.390)\tTest Acc@5 (98.220)\n",
            "Epoch: 83, Test Accuracy: 77.3900, ReLU Count: 20801, Lasso weight: 0.000056\n",
            "Test Loss  (1.3653)\tTest Acc@1 (77.060)\tTest Acc@5 (98.040)\n",
            "Epoch: 84, Test Accuracy: 77.0600, ReLU Count: 19818, Lasso weight: 0.000056\n",
            "Test Loss  (1.3341)\tTest Acc@1 (77.100)\tTest Acc@5 (97.970)\n",
            "Epoch: 85, Test Accuracy: 77.1000, ReLU Count: 20671, Lasso weight: 0.000056\n",
            "Test Loss  (1.4299)\tTest Acc@1 (76.970)\tTest Acc@5 (97.860)\n",
            "Epoch: 86, Test Accuracy: 76.9700, ReLU Count: 19789, Lasso weight: 0.000061\n",
            "Test Loss  (1.4478)\tTest Acc@1 (75.670)\tTest Acc@5 (97.610)\n",
            "Epoch: 87, Test Accuracy: 75.6700, ReLU Count: 19527, Lasso weight: 0.000061\n",
            "Test Loss  (1.3577)\tTest Acc@1 (76.310)\tTest Acc@5 (97.800)\n",
            "Epoch: 88, Test Accuracy: 76.3100, ReLU Count: 19842, Lasso weight: 0.000061\n",
            "Test Loss  (1.4217)\tTest Acc@1 (76.450)\tTest Acc@5 (97.690)\n",
            "Epoch: 89, Test Accuracy: 76.4500, ReLU Count: 17873, Lasso weight: 0.000067\n",
            "Test Loss  (1.4646)\tTest Acc@1 (75.030)\tTest Acc@5 (97.890)\n",
            "Epoch: 90, Test Accuracy: 75.0300, ReLU Count: 18205, Lasso weight: 0.000067\n",
            "Test Loss  (1.4538)\tTest Acc@1 (76.050)\tTest Acc@5 (97.480)\n",
            "Epoch: 91, Test Accuracy: 76.0500, ReLU Count: 17131, Lasso weight: 0.000074\n",
            "Test Loss  (1.5110)\tTest Acc@1 (76.180)\tTest Acc@5 (97.830)\n",
            "Epoch: 92, Test Accuracy: 76.1800, ReLU Count: 15965, Lasso weight: 0.000074\n",
            "Test Loss  (1.4084)\tTest Acc@1 (76.580)\tTest Acc@5 (97.800)\n",
            "Epoch: 93, Test Accuracy: 76.5800, ReLU Count: 16215, Lasso weight: 0.000074\n",
            "Test Loss  (1.4165)\tTest Acc@1 (76.780)\tTest Acc@5 (97.720)\n",
            "Epoch: 94, Test Accuracy: 76.7800, ReLU Count: 15490, Lasso weight: 0.000081\n",
            "Test Loss  (1.5151)\tTest Acc@1 (75.800)\tTest Acc@5 (97.700)\n",
            "Epoch: 95, Test Accuracy: 75.8000, ReLU Count: 15518, Lasso weight: 0.000081\n",
            "Test Loss  (1.5280)\tTest Acc@1 (74.540)\tTest Acc@5 (97.620)\n",
            "Epoch: 96, Test Accuracy: 74.5400, ReLU Count: 15204, Lasso weight: 0.000090\n",
            "Test Loss  (1.4582)\tTest Acc@1 (75.730)\tTest Acc@5 (97.850)\n",
            "Epoch: 97, Test Accuracy: 75.7300, ReLU Count: 14173, Lasso weight: 0.000090\n",
            "Test Loss  (1.4523)\tTest Acc@1 (75.590)\tTest Acc@5 (97.750)\n",
            "Epoch: 98, Test Accuracy: 75.5900, ReLU Count: 13702, Lasso weight: 0.000090\n",
            "Test Loss  (1.5045)\tTest Acc@1 (74.640)\tTest Acc@5 (97.640)\n",
            "Epoch: 99, Test Accuracy: 74.6400, ReLU Count: 14789, Lasso weight: 0.000090\n",
            "Test Loss  (1.5001)\tTest Acc@1 (74.990)\tTest Acc@5 (97.960)\n",
            "Epoch: 100, Test Accuracy: 74.9900, ReLU Count: 13490, Lasso weight: 0.000098\n",
            "Test Loss  (1.5567)\tTest Acc@1 (74.920)\tTest Acc@5 (97.770)\n",
            "Epoch: 101, Test Accuracy: 74.9200, ReLU Count: 13129, Lasso weight: 0.000098\n",
            "Test Loss  (1.5660)\tTest Acc@1 (74.010)\tTest Acc@5 (97.490)\n",
            "Epoch: 102, Test Accuracy: 74.0100, ReLU Count: 12763, Lasso weight: 0.000098\n",
            "Test Loss  (1.5924)\tTest Acc@1 (73.970)\tTest Acc@5 (97.570)\n",
            "Epoch: 103, Test Accuracy: 73.9700, ReLU Count: 12993, Lasso weight: 0.000098\n",
            "Test Loss  (1.5562)\tTest Acc@1 (74.320)\tTest Acc@5 (97.540)\n",
            "Epoch: 104, Test Accuracy: 74.3200, ReLU Count: 12954, Lasso weight: 0.000108\n",
            "Test Loss  (1.6357)\tTest Acc@1 (73.860)\tTest Acc@5 (97.120)\n",
            "Epoch: 105, Test Accuracy: 73.8600, ReLU Count: 12642, Lasso weight: 0.000119\n",
            "Test Loss  (1.5564)\tTest Acc@1 (73.730)\tTest Acc@5 (97.430)\n",
            "Epoch: 106, Test Accuracy: 73.7300, ReLU Count: 11943, Lasso weight: 0.000119\n",
            "Test Loss  (1.5779)\tTest Acc@1 (74.110)\tTest Acc@5 (97.730)\n",
            "Epoch: 107, Test Accuracy: 74.1100, ReLU Count: 11230, Lasso weight: 0.000119\n",
            "Test Loss  (1.6009)\tTest Acc@1 (74.360)\tTest Acc@5 (97.510)\n",
            "Epoch: 108, Test Accuracy: 74.3600, ReLU Count: 11253, Lasso weight: 0.000119\n",
            "Test Loss  (1.5757)\tTest Acc@1 (74.580)\tTest Acc@5 (97.550)\n",
            "Epoch: 109, Test Accuracy: 74.5800, ReLU Count: 10669, Lasso weight: 0.000131\n",
            "Test Loss  (1.6372)\tTest Acc@1 (73.260)\tTest Acc@5 (97.750)\n",
            "Epoch: 110, Test Accuracy: 73.2600, ReLU Count: 10981, Lasso weight: 0.000131\n",
            "Test Loss  (1.7182)\tTest Acc@1 (73.080)\tTest Acc@5 (97.460)\n",
            "Epoch: 111, Test Accuracy: 73.0800, ReLU Count: 10076, Lasso weight: 0.000144\n",
            "Test Loss  (1.5550)\tTest Acc@1 (73.890)\tTest Acc@5 (97.570)\n",
            "Epoch: 112, Test Accuracy: 73.8900, ReLU Count: 9692, Lasso weight: 0.000144\n",
            "Achieved relu budget after epoch 112\n",
            "After SNL Algorithm, the current ReLU Count: 9692, relative count: 0.019718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/gdrive/My Drive/SNL_results/ResNet_18_linearized_10K.pth'\n",
        "torch.save(model.state_dict(), model_path)\n"
      ],
      "metadata": {
        "id": "1tac5CYOJNz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6sSdgDXDlTaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning training phase\n"
      ],
      "metadata": {
        "id": "uNs33xHtAd3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linearized_model_path = '/content/gdrive/My Drive/SNL_results/ResNet_18_linearized_10K.pth'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "linearized_model = ResNet18_SNL()\n",
        "linearized_model.load_state_dict(torch.load(linearized_model_path, weights_only=True, map_location=device))\n",
        "linearized_model.to(device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16pzXfKxWJdT",
        "outputId": "c0ce274e-cc41-45b9-b661-270ab60e0969",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18_SNL(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "rounded_model = copy.deepcopy(linearized_model)\n",
        "rounded_model.to(device)\n",
        "\n",
        "# Round and Freeze alpha params\n",
        "for name, param in rounded_model.named_parameters():\n",
        "    if 'alpha' in name:\n",
        "        param.data = (param.data > SNL_params['relu_lin_threshold']).float()\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gYInTLIKpcSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(\n",
        "    train_dataset,  # Training dataset\n",
        "    batch_size=fine_tuning_params['batch_size'],  # Number of samples per batch\n",
        "    shuffle=True,  # Shuffle the training data\n",
        "    num_workers=2  # Number of subprocesses for data loading\n",
        ")\n",
        "\n",
        "valloader = DataLoader(\n",
        "    val_dataset,  # Validation dataset\n",
        "    batch_size=fine_tuning_params['batch_size'],  # Number of samples per batch\n",
        "    shuffle=False,  # No need to shuffle validation data\n",
        "    num_workers=2  # Number of subprocesses for data loading\n",
        ")\n"
      ],
      "metadata": {
        "id": "3K-AOoiNGTe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = copy.deepcopy(rounded_model)\n",
        "finetuned_model.to(device)\n",
        "\n",
        "frozen, unfrozen = [], []\n",
        "for name, param in finetuned_model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        unfrozen.append(name)\n",
        "    else:\n",
        "        frozen.append(name)\n",
        "\n",
        "frozen_str = '\\n'.join(frozen)\n",
        "print(f\"---Frozen Layers---\\n{frozen_str}\")\n",
        "unfrozen_str = '\\n'.join(unfrozen)\n",
        "print(f\"---Unfrozen Layers---\\n{unfrozen_str}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE0-ZeExKMsi",
        "outputId": "35bcea4f-00eb-458f-995c-a045b7586671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Frozen Layers---\n",
            "layer1.0.prelu1.alphas\n",
            "layer1.0.prelu2.alphas\n",
            "layer1.1.prelu1.alphas\n",
            "layer1.1.prelu2.alphas\n",
            "layer2.0.prelu1.alphas\n",
            "layer2.0.prelu2.alphas\n",
            "layer2.1.prelu1.alphas\n",
            "layer2.1.prelu2.alphas\n",
            "layer3.0.prelu1.alphas\n",
            "layer3.0.prelu2.alphas\n",
            "layer3.1.prelu1.alphas\n",
            "layer3.1.prelu2.alphas\n",
            "layer4.0.prelu1.alphas\n",
            "layer4.0.prelu2.alphas\n",
            "layer4.1.prelu1.alphas\n",
            "layer4.1.prelu2.alphas\n",
            "---Unfrozen Layers---\n",
            "conv1.weight\n",
            "bn1.weight\n",
            "bn1.bias\n",
            "layer1.0.conv1.weight\n",
            "layer1.0.bn1.weight\n",
            "layer1.0.bn1.bias\n",
            "layer1.0.conv2.weight\n",
            "layer1.0.bn2.weight\n",
            "layer1.0.bn2.bias\n",
            "layer1.1.conv1.weight\n",
            "layer1.1.bn1.weight\n",
            "layer1.1.bn1.bias\n",
            "layer1.1.conv2.weight\n",
            "layer1.1.bn2.weight\n",
            "layer1.1.bn2.bias\n",
            "layer2.0.conv1.weight\n",
            "layer2.0.bn1.weight\n",
            "layer2.0.bn1.bias\n",
            "layer2.0.conv2.weight\n",
            "layer2.0.bn2.weight\n",
            "layer2.0.bn2.bias\n",
            "layer2.0.shortcut.0.weight\n",
            "layer2.0.shortcut.1.weight\n",
            "layer2.0.shortcut.1.bias\n",
            "layer2.1.conv1.weight\n",
            "layer2.1.bn1.weight\n",
            "layer2.1.bn1.bias\n",
            "layer2.1.conv2.weight\n",
            "layer2.1.bn2.weight\n",
            "layer2.1.bn2.bias\n",
            "layer3.0.conv1.weight\n",
            "layer3.0.bn1.weight\n",
            "layer3.0.bn1.bias\n",
            "layer3.0.conv2.weight\n",
            "layer3.0.bn2.weight\n",
            "layer3.0.bn2.bias\n",
            "layer3.0.shortcut.0.weight\n",
            "layer3.0.shortcut.1.weight\n",
            "layer3.0.shortcut.1.bias\n",
            "layer3.1.conv1.weight\n",
            "layer3.1.bn1.weight\n",
            "layer3.1.bn1.bias\n",
            "layer3.1.conv2.weight\n",
            "layer3.1.bn2.weight\n",
            "layer3.1.bn2.bias\n",
            "layer4.0.conv1.weight\n",
            "layer4.0.bn1.weight\n",
            "layer4.0.bn1.bias\n",
            "layer4.0.conv2.weight\n",
            "layer4.0.bn2.weight\n",
            "layer4.0.bn2.bias\n",
            "layer4.0.shortcut.0.weight\n",
            "layer4.0.shortcut.1.weight\n",
            "layer4.0.shortcut.1.bias\n",
            "layer4.1.conv1.weight\n",
            "layer4.1.bn1.weight\n",
            "layer4.1.bn1.bias\n",
            "layer4.1.conv2.weight\n",
            "layer4.1.bn2.weight\n",
            "layer4.1.bn2.bias\n",
            "fc.weight\n",
            "fc.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = SGD(finetuned_model.parameters(), lr=fine_tuning_params['lr'], momentum=fine_tuning_params['momentum'], weight_decay=fine_tuning_params['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, fine_tuning_params['epochs'])\n",
        "\n",
        "best_top1 = 0\n",
        "\n",
        "for epoch in range(fine_tuning_params['epochs']):\n",
        "  # training\n",
        "  train(trainloader, finetuned_model, criterion, optimizer, epoch, device)\n",
        "\n",
        "  # validation\n",
        "  cur_step = (epoch+1) * len(trainloader)\n",
        "  _, top1, _ = test(valloader, finetuned_model, criterion, device, cur_step)\n",
        "  scheduler.step()\n",
        "\n",
        "  # save\n",
        "  if best_top1 < top1:\n",
        "      best_top1 = top1\n",
        "      is_best = True\n",
        "  else:\n",
        "      is_best = False\n",
        "\n",
        "  if is_best:\n",
        "      model_path = '/content/gdrive/My Drive/SNL_results/ResNet_18_SNL_linearized_10K_finetuned.pth'\n",
        "      torch.save(finetuned_model.state_dict(), model_path)\n",
        "\n",
        "  print(\"\")\n",
        "\n",
        "print(\"Best model's validation acc: {:.4%}\".format(best_top1 / 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZW1qMOeUreSh",
        "outputId": "376b5fa5-46d5-4483-994a-8f5f866c9bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/313]\tTime 0.272 (0.272)\tData 0.156 (0.156)\tLoss 0.0891 (0.0891)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [0][100/313]\tTime 0.124 (0.127)\tData 0.002 (0.005)\tLoss 0.1417 (0.1902)\tAcc@1 97.656 (94.462)\tAcc@5 99.219 (99.714)\n",
            "Epoch: [0][200/313]\tTime 0.127 (0.127)\tData 0.002 (0.004)\tLoss 0.2055 (0.1870)\tAcc@1 94.531 (94.652)\tAcc@5 100.000 (99.724)\n",
            "Epoch: [0][300/313]\tTime 0.128 (0.127)\tData 0.002 (0.004)\tLoss 0.0765 (0.1922)\tAcc@1 97.656 (94.544)\tAcc@5 100.000 (99.702)\n",
            "Test Loss  (0.2008)\tTest Acc@1 (94.400)\tTest Acc@5 (99.770)\n",
            "\n",
            "Epoch: [1][0/313]\tTime 0.221 (0.221)\tData 0.162 (0.162)\tLoss 0.1651 (0.1651)\tAcc@1 92.969 (92.969)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [1][100/313]\tTime 0.128 (0.129)\tData 0.002 (0.005)\tLoss 0.2929 (0.1871)\tAcc@1 90.625 (94.740)\tAcc@5 99.219 (99.691)\n",
            "Epoch: [1][200/313]\tTime 0.130 (0.129)\tData 0.002 (0.004)\tLoss 0.1322 (0.1829)\tAcc@1 95.312 (94.819)\tAcc@5 100.000 (99.708)\n",
            "Epoch: [1][300/313]\tTime 0.129 (0.129)\tData 0.002 (0.004)\tLoss 0.1508 (0.1865)\tAcc@1 96.094 (94.762)\tAcc@5 99.219 (99.694)\n",
            "Test Loss  (0.1986)\tTest Acc@1 (94.390)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [2][0/313]\tTime 0.282 (0.282)\tData 0.177 (0.177)\tLoss 0.1647 (0.1647)\tAcc@1 95.312 (95.312)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [2][100/313]\tTime 0.131 (0.132)\tData 0.002 (0.005)\tLoss 0.1684 (0.1882)\tAcc@1 96.094 (94.771)\tAcc@5 99.219 (99.706)\n",
            "Epoch: [2][200/313]\tTime 0.132 (0.131)\tData 0.002 (0.004)\tLoss 0.0964 (0.1856)\tAcc@1 97.656 (94.815)\tAcc@5 99.219 (99.685)\n",
            "Epoch: [2][300/313]\tTime 0.131 (0.131)\tData 0.002 (0.004)\tLoss 0.1458 (0.1851)\tAcc@1 95.312 (94.840)\tAcc@5 100.000 (99.699)\n",
            "Test Loss  (0.1961)\tTest Acc@1 (94.420)\tTest Acc@5 (99.750)\n",
            "\n",
            "Epoch: [3][0/313]\tTime 0.223 (0.223)\tData 0.166 (0.166)\tLoss 0.2343 (0.2343)\tAcc@1 96.094 (96.094)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [3][100/313]\tTime 0.131 (0.134)\tData 0.009 (0.005)\tLoss 0.2419 (0.1804)\tAcc@1 93.750 (94.903)\tAcc@5 99.219 (99.675)\n",
            "Epoch: [3][200/313]\tTime 0.127 (0.133)\tData 0.002 (0.004)\tLoss 0.0555 (0.1819)\tAcc@1 98.438 (94.955)\tAcc@5 100.000 (99.732)\n",
            "Epoch: [3][300/313]\tTime 0.132 (0.133)\tData 0.014 (0.004)\tLoss 0.1786 (0.1790)\tAcc@1 94.531 (94.991)\tAcc@5 100.000 (99.733)\n",
            "Test Loss  (0.1950)\tTest Acc@1 (94.550)\tTest Acc@5 (99.760)\n",
            "\n",
            "Epoch: [4][0/313]\tTime 0.236 (0.236)\tData 0.160 (0.160)\tLoss 0.0823 (0.0823)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [4][100/313]\tTime 0.133 (0.134)\tData 0.002 (0.005)\tLoss 0.1591 (0.1833)\tAcc@1 94.531 (95.111)\tAcc@5 100.000 (99.660)\n",
            "Epoch: [4][200/313]\tTime 0.133 (0.134)\tData 0.002 (0.004)\tLoss 0.1906 (0.1773)\tAcc@1 92.188 (95.075)\tAcc@5 100.000 (99.697)\n",
            "Epoch: [4][300/313]\tTime 0.134 (0.134)\tData 0.002 (0.004)\tLoss 0.1213 (0.1774)\tAcc@1 96.094 (95.120)\tAcc@5 99.219 (99.707)\n",
            "Test Loss  (0.1931)\tTest Acc@1 (94.580)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [5][0/313]\tTime 0.229 (0.229)\tData 0.162 (0.162)\tLoss 0.3157 (0.3157)\tAcc@1 90.625 (90.625)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [5][100/313]\tTime 0.132 (0.134)\tData 0.002 (0.005)\tLoss 0.2224 (0.1788)\tAcc@1 95.312 (94.918)\tAcc@5 99.219 (99.745)\n",
            "Epoch: [5][200/313]\tTime 0.134 (0.134)\tData 0.002 (0.004)\tLoss 0.1049 (0.1741)\tAcc@1 97.656 (95.134)\tAcc@5 99.219 (99.720)\n",
            "Epoch: [5][300/313]\tTime 0.133 (0.134)\tData 0.002 (0.004)\tLoss 0.1648 (0.1751)\tAcc@1 91.406 (95.087)\tAcc@5 100.000 (99.725)\n",
            "Test Loss  (0.1905)\tTest Acc@1 (94.610)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [6][0/313]\tTime 0.391 (0.391)\tData 0.282 (0.282)\tLoss 0.1690 (0.1690)\tAcc@1 94.531 (94.531)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [6][100/313]\tTime 0.132 (0.137)\tData 0.002 (0.006)\tLoss 0.1366 (0.1635)\tAcc@1 95.312 (95.382)\tAcc@5 100.000 (99.783)\n",
            "Epoch: [6][200/313]\tTime 0.133 (0.135)\tData 0.007 (0.005)\tLoss 0.1637 (0.1620)\tAcc@1 94.531 (95.464)\tAcc@5 100.000 (99.740)\n",
            "Epoch: [6][300/313]\tTime 0.135 (0.135)\tData 0.002 (0.004)\tLoss 0.1452 (0.1714)\tAcc@1 95.312 (95.279)\tAcc@5 100.000 (99.725)\n",
            "Test Loss  (0.1903)\tTest Acc@1 (94.590)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [7][0/313]\tTime 0.205 (0.205)\tData 0.146 (0.146)\tLoss 0.1198 (0.1198)\tAcc@1 96.094 (96.094)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [7][100/313]\tTime 0.134 (0.135)\tData 0.002 (0.005)\tLoss 0.2158 (0.1728)\tAcc@1 92.969 (95.243)\tAcc@5 100.000 (99.714)\n",
            "Epoch: [7][200/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1926 (0.1673)\tAcc@1 93.750 (95.285)\tAcc@5 100.000 (99.724)\n",
            "Epoch: [7][300/313]\tTime 0.135 (0.135)\tData 0.002 (0.004)\tLoss 0.1862 (0.1694)\tAcc@1 96.875 (95.245)\tAcc@5 99.219 (99.735)\n",
            "Test Loss  (0.1884)\tTest Acc@1 (94.650)\tTest Acc@5 (99.810)\n",
            "\n",
            "Epoch: [8][0/313]\tTime 0.242 (0.242)\tData 0.175 (0.175)\tLoss 0.0799 (0.0799)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [8][100/313]\tTime 0.135 (0.136)\tData 0.002 (0.006)\tLoss 0.0757 (0.1679)\tAcc@1 96.875 (95.282)\tAcc@5 100.000 (99.745)\n",
            "Epoch: [8][200/313]\tTime 0.133 (0.135)\tData 0.002 (0.004)\tLoss 0.0964 (0.1670)\tAcc@1 97.656 (95.320)\tAcc@5 100.000 (99.767)\n",
            "Epoch: [8][300/313]\tTime 0.135 (0.135)\tData 0.002 (0.004)\tLoss 0.0669 (0.1678)\tAcc@1 97.656 (95.294)\tAcc@5 100.000 (99.751)\n",
            "Test Loss  (0.1872)\tTest Acc@1 (94.650)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [9][0/313]\tTime 0.316 (0.316)\tData 0.230 (0.230)\tLoss 0.1862 (0.1862)\tAcc@1 93.750 (93.750)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [9][100/313]\tTime 0.133 (0.136)\tData 0.012 (0.006)\tLoss 0.1550 (0.1615)\tAcc@1 95.312 (95.343)\tAcc@5 100.000 (99.745)\n",
            "Epoch: [9][200/313]\tTime 0.128 (0.135)\tData 0.004 (0.005)\tLoss 0.0672 (0.1678)\tAcc@1 97.656 (95.347)\tAcc@5 100.000 (99.728)\n",
            "Epoch: [9][300/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.0686 (0.1657)\tAcc@1 98.438 (95.398)\tAcc@5 100.000 (99.735)\n",
            "Test Loss  (0.1863)\tTest Acc@1 (94.750)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [10][0/313]\tTime 0.223 (0.223)\tData 0.163 (0.163)\tLoss 0.2371 (0.2371)\tAcc@1 92.969 (92.969)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [10][100/313]\tTime 0.137 (0.136)\tData 0.002 (0.005)\tLoss 0.1560 (0.1690)\tAcc@1 95.312 (95.312)\tAcc@5 100.000 (99.752)\n",
            "Epoch: [10][200/313]\tTime 0.135 (0.135)\tData 0.002 (0.004)\tLoss 0.1107 (0.1625)\tAcc@1 96.875 (95.437)\tAcc@5 100.000 (99.736)\n",
            "Epoch: [10][300/313]\tTime 0.138 (0.135)\tData 0.002 (0.004)\tLoss 0.0842 (0.1627)\tAcc@1 98.438 (95.442)\tAcc@5 100.000 (99.743)\n",
            "Test Loss  (0.1855)\tTest Acc@1 (94.680)\tTest Acc@5 (99.830)\n",
            "\n",
            "Epoch: [11][0/313]\tTime 0.197 (0.197)\tData 0.141 (0.141)\tLoss 0.1658 (0.1658)\tAcc@1 93.750 (93.750)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [11][100/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1701 (0.1642)\tAcc@1 96.875 (95.444)\tAcc@5 100.000 (99.729)\n",
            "Epoch: [11][200/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1819 (0.1603)\tAcc@1 94.531 (95.503)\tAcc@5 100.000 (99.751)\n",
            "Epoch: [11][300/313]\tTime 0.133 (0.135)\tData 0.002 (0.004)\tLoss 0.1031 (0.1618)\tAcc@1 97.656 (95.445)\tAcc@5 100.000 (99.740)\n",
            "Test Loss  (0.1851)\tTest Acc@1 (94.690)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [12][0/313]\tTime 0.263 (0.263)\tData 0.204 (0.204)\tLoss 0.1392 (0.1392)\tAcc@1 94.531 (94.531)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [12][100/313]\tTime 0.132 (0.136)\tData 0.006 (0.005)\tLoss 0.1829 (0.1539)\tAcc@1 94.531 (95.854)\tAcc@5 99.219 (99.737)\n",
            "Epoch: [12][200/313]\tTime 0.135 (0.135)\tData 0.002 (0.004)\tLoss 0.1341 (0.1623)\tAcc@1 97.656 (95.643)\tAcc@5 100.000 (99.728)\n",
            "Epoch: [12][300/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.2433 (0.1592)\tAcc@1 92.969 (95.611)\tAcc@5 100.000 (99.756)\n",
            "Test Loss  (0.1840)\tTest Acc@1 (94.760)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [13][0/313]\tTime 0.227 (0.227)\tData 0.162 (0.162)\tLoss 0.1475 (0.1475)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [13][100/313]\tTime 0.136 (0.136)\tData 0.002 (0.005)\tLoss 0.1432 (0.1482)\tAcc@1 93.750 (95.854)\tAcc@5 100.000 (99.752)\n",
            "Epoch: [13][200/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1074 (0.1563)\tAcc@1 96.875 (95.748)\tAcc@5 100.000 (99.724)\n",
            "Epoch: [13][300/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1812 (0.1581)\tAcc@1 92.969 (95.665)\tAcc@5 100.000 (99.735)\n",
            "Test Loss  (0.1827)\tTest Acc@1 (94.800)\tTest Acc@5 (99.790)\n",
            "\n",
            "Epoch: [14][0/313]\tTime 0.226 (0.226)\tData 0.157 (0.157)\tLoss 0.1563 (0.1563)\tAcc@1 93.750 (93.750)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [14][100/313]\tTime 0.134 (0.135)\tData 0.002 (0.005)\tLoss 0.4080 (0.1538)\tAcc@1 88.281 (95.715)\tAcc@5 99.219 (99.752)\n",
            "Epoch: [14][200/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1455 (0.1520)\tAcc@1 95.312 (95.713)\tAcc@5 100.000 (99.767)\n",
            "Epoch: [14][300/313]\tTime 0.134 (0.134)\tData 0.002 (0.004)\tLoss 0.0835 (0.1546)\tAcc@1 96.875 (95.632)\tAcc@5 100.000 (99.753)\n",
            "Test Loss  (0.1825)\tTest Acc@1 (94.730)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [15][0/313]\tTime 0.286 (0.286)\tData 0.216 (0.216)\tLoss 0.1499 (0.1499)\tAcc@1 96.875 (96.875)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [15][100/313]\tTime 0.138 (0.136)\tData 0.007 (0.006)\tLoss 0.2119 (0.1585)\tAcc@1 94.531 (95.537)\tAcc@5 100.000 (99.752)\n",
            "Epoch: [15][200/313]\tTime 0.135 (0.135)\tData 0.002 (0.005)\tLoss 0.1880 (0.1593)\tAcc@1 95.312 (95.573)\tAcc@5 99.219 (99.763)\n",
            "Epoch: [15][300/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.2118 (0.1555)\tAcc@1 95.312 (95.699)\tAcc@5 99.219 (99.759)\n",
            "Test Loss  (0.1809)\tTest Acc@1 (94.790)\tTest Acc@5 (99.790)\n",
            "\n",
            "Epoch: [16][0/313]\tTime 0.206 (0.206)\tData 0.140 (0.140)\tLoss 0.1666 (0.1666)\tAcc@1 96.094 (96.094)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [16][100/313]\tTime 0.135 (0.135)\tData 0.002 (0.005)\tLoss 0.1516 (0.1549)\tAcc@1 96.875 (95.676)\tAcc@5 100.000 (99.760)\n",
            "Epoch: [16][200/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1629 (0.1535)\tAcc@1 93.750 (95.748)\tAcc@5 100.000 (99.743)\n",
            "Epoch: [16][300/313]\tTime 0.135 (0.135)\tData 0.002 (0.004)\tLoss 0.1765 (0.1552)\tAcc@1 95.312 (95.743)\tAcc@5 100.000 (99.756)\n",
            "Test Loss  (0.1806)\tTest Acc@1 (94.810)\tTest Acc@5 (99.820)\n",
            "\n",
            "Epoch: [17][0/313]\tTime 0.222 (0.222)\tData 0.165 (0.165)\tLoss 0.1809 (0.1809)\tAcc@1 95.312 (95.312)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [17][100/313]\tTime 0.135 (0.135)\tData 0.002 (0.004)\tLoss 0.0842 (0.1519)\tAcc@1 97.656 (95.730)\tAcc@5 100.000 (99.714)\n",
            "Epoch: [17][200/313]\tTime 0.132 (0.135)\tData 0.002 (0.004)\tLoss 0.1770 (0.1505)\tAcc@1 94.531 (95.721)\tAcc@5 99.219 (99.732)\n",
            "Epoch: [17][300/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1578 (0.1516)\tAcc@1 94.531 (95.787)\tAcc@5 100.000 (99.743)\n",
            "Test Loss  (0.1806)\tTest Acc@1 (94.750)\tTest Acc@5 (99.820)\n",
            "\n",
            "Epoch: [18][0/313]\tTime 0.352 (0.352)\tData 0.248 (0.248)\tLoss 0.1128 (0.1128)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [18][100/313]\tTime 0.135 (0.136)\tData 0.015 (0.006)\tLoss 0.2452 (0.1504)\tAcc@1 93.750 (96.047)\tAcc@5 100.000 (99.752)\n",
            "Epoch: [18][200/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.0735 (0.1536)\tAcc@1 98.438 (95.833)\tAcc@5 100.000 (99.740)\n",
            "Epoch: [18][300/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.0947 (0.1522)\tAcc@1 96.875 (95.798)\tAcc@5 100.000 (99.753)\n",
            "Test Loss  (0.1795)\tTest Acc@1 (94.810)\tTest Acc@5 (99.800)\n",
            "\n",
            "Epoch: [19][0/313]\tTime 0.206 (0.206)\tData 0.136 (0.136)\tLoss 0.1812 (0.1812)\tAcc@1 92.969 (92.969)\tAcc@5 99.219 (99.219)\n",
            "Epoch: [19][100/313]\tTime 0.134 (0.135)\tData 0.002 (0.005)\tLoss 0.0660 (0.1487)\tAcc@1 99.219 (96.024)\tAcc@5 100.000 (99.752)\n",
            "Epoch: [19][200/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1310 (0.1510)\tAcc@1 94.531 (95.872)\tAcc@5 100.000 (99.763)\n",
            "Epoch: [19][300/313]\tTime 0.134 (0.135)\tData 0.002 (0.003)\tLoss 0.1189 (0.1486)\tAcc@1 96.875 (95.915)\tAcc@5 99.219 (99.782)\n",
            "Test Loss  (0.1798)\tTest Acc@1 (94.870)\tTest Acc@5 (99.780)\n",
            "\n",
            "Epoch: [20][0/313]\tTime 0.231 (0.231)\tData 0.165 (0.165)\tLoss 0.0689 (0.0689)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [20][100/313]\tTime 0.134 (0.135)\tData 0.002 (0.005)\tLoss 0.1357 (0.1514)\tAcc@1 95.312 (95.746)\tAcc@5 100.000 (99.791)\n",
            "Epoch: [20][200/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.0743 (0.1436)\tAcc@1 98.438 (95.962)\tAcc@5 100.000 (99.806)\n",
            "Epoch: [20][300/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.1563 (0.1491)\tAcc@1 96.875 (95.884)\tAcc@5 100.000 (99.779)\n",
            "Test Loss  (0.1787)\tTest Acc@1 (94.890)\tTest Acc@5 (99.790)\n",
            "\n",
            "Epoch: [21][0/313]\tTime 0.275 (0.275)\tData 0.204 (0.204)\tLoss 0.1120 (0.1120)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
            "Epoch: [21][100/313]\tTime 0.134 (0.136)\tData 0.002 (0.006)\tLoss 0.0816 (0.1399)\tAcc@1 96.875 (95.931)\tAcc@5 100.000 (99.776)\n",
            "Epoch: [21][200/313]\tTime 0.134 (0.135)\tData 0.002 (0.004)\tLoss 0.0837 (0.1435)\tAcc@1 97.656 (95.938)\tAcc@5 100.000 (99.775)\n",
            "Epoch: [21][300/313]\tTime 0.135 (0.135)\tData 0.002 (0.004)\tLoss 0.1278 (0.1467)\tAcc@1 96.094 (95.876)\tAcc@5 100.000 (99.772)\n",
            "Test Loss  (0.1783)\tTest Acc@1 (94.890)\tTest Acc@5 (99.790)\n",
            "\n",
            "Epoch: [22][0/313]\tTime 0.205 (0.205)\tData 0.149 (0.149)\tLoss 0.0880 (0.0880)\tAcc@1 96.875 (96.875)\tAcc@5 100.000 (100.000)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-41543739f166>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_tuning_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinetuned_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e57f324004ad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, model, criterion, optimizer, epoch, device, print_freq, display)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modified Lasso Training"
      ],
      "metadata": {
        "id": "xobyAF3ulYpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simultaneus_training_lasso_loss_modified(loader, model, criterion, optimizer, lasso_coef):\n",
        "  losses = AverageMeter()\n",
        "\n",
        "  # switch to train mode\n",
        "  model.train()\n",
        "\n",
        "  for i, (inputs, targets) in enumerate(loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      reg_loss = 0\n",
        "      for name, param in model.named_parameters():\n",
        "          if 'alpha' in name:\n",
        "              reg_loss += torch.norm(param, p=1)\n",
        "\n",
        "      # compute output\n",
        "      outputs = model(inputs)\n",
        "      cur_relu_count = relu_counting(model, SNL_params['relu_lin_threshold'])\n",
        "\n",
        "      # print(f\"net loss: {criterion(outputs, targets)}\")\n",
        "      # print(f\"reg loss: {reg_loss}\")\n",
        "      # print(f\"cur relu count: {cur_relu_count}\")\n",
        "      # print(f\"total loss: {criterion(outputs, targets) + lasso_coef * reg_loss}\")\n",
        "\n",
        "      loss = criterion(outputs, targets) + lasso_coef * reg_loss\n",
        "\n",
        "      losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "      # compute gradient and do SGD step\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  return losses.avg"
      ],
      "metadata": {
        "id": "BZr12CQ8ld2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "PAU6OZ_UFqI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_path = '/content/gdrive/My Drive/SNL_results/ResNet_18_SNL_linearized_10K_finetuned.pth'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "finetuned_model = ResNet18_SNL()\n",
        "finetuned_model.load_state_dict(torch.load(finetuned_model_path, weights_only=True, map_location=device))\n",
        "finetuned_model.to(device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS5paztiNCnK",
        "outputId": "b7988118-e3a2-4f33-bc48-a4d72b168205",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18_SNL(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu1): LearnableAlpha()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu2): LearnableAlpha()\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',  # Directory to save/load the dataset\n",
        "    train=False,  # Load training set\n",
        "    download=True,  # Download dataset if not already present\n",
        "    transform=transform  # Apply the defined transforms\n",
        ")\n",
        "\n",
        "# Step 3: Create a DataLoader\n",
        "testloader = DataLoader(\n",
        "    test_dataset,  # Dataset to load\n",
        "    batch_size=32,  # Number of samples per batch\n",
        "    shuffle=True,  # Shuffle the data\n",
        "    num_workers=2  # Number of subprocesses for data loading\n",
        ")\n",
        "\n",
        "for images, labels in testloader:\n",
        "    print(f\"Batch size: {images.size(0)}, Image shape: {images.size()[1:]}, Labels: {labels}\")\n",
        "    break  # Just to check one batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJZTSU_3osDp",
        "outputId": "e35c4b6b-ee48-4c77-ace6-7850db3adbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Batch size: 32, Image shape: torch.Size([3, 32, 32]), Labels: tensor([5, 9, 4, 7, 3, 1, 5, 5, 6, 9, 4, 5, 4, 6, 6, 8, 4, 0, 0, 0, 5, 3, 4, 5,\n",
            "        0, 8, 5, 7, 9, 3, 6, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(loader, model, criterion, run_name):\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      total = 0\n",
        "      correct = 0\n",
        "      running_loss = 0.0\n",
        "\n",
        "      for data in loader:  # Iterate over the test data loader\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          outputs = model(inputs)  # Forward pass\n",
        "          loss = criterion(outputs, labels)  # Compute loss\n",
        "\n",
        "          running_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "          # Get predictions (class with the highest probability)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "          # Update the total number of samples and correct predictions\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "      # Calculate average loss and accuracy\n",
        "      avg_loss = running_loss / len(loader)\n",
        "      acc = 100 * correct / total\n",
        "\n",
        "  print(f'{run_name} Loss: {avg_loss:.4f}, {run_name} Accuracy: {acc:.2f}%')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# print(\"pretrained model\")\n",
        "# eval_model(trainloader, baseline_model, criterion, 'train')\n",
        "# eval_model(valloader, baseline_model, criterion, 'val')\n",
        "# eval_model(testloader, baseline_model, criterion, 'test')\n",
        "\n",
        "# print(\"linearized model\")\n",
        "# eval_model(trainloader, linearized_model, criterion, 'train')\n",
        "# eval_model(valloader, linearized_model, criterion, 'val')\n",
        "# eval_model(testloader, linearized_model, criterion, 'test')\n",
        "\n",
        "# print(\"rounded model\")\n",
        "# eval_model(trainloader, rounded_model, criterion, 'train')\n",
        "# eval_model(valloader, rounded_model, criterion, 'val')\n",
        "# eval_model(testloader, rounded_model, criterion, 'test')\n",
        "\n",
        "print(\"finetuned model\")\n",
        "eval_model(trainloader, finetuned_model, criterion, 'train')\n",
        "eval_model(valloader, finetuned_model, criterion, 'val')\n",
        "eval_model(testloader, finetuned_model, criterion, 'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTxuj1fpNNrN",
        "outputId": "006b888b-9466-4a6c-8dba-ecc2b77d7b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finetuned model\n",
            "train Loss: 0.1391, train Accuracy: 96.25%\n",
            "val Loss: 0.1768, val Accuracy: 94.89%\n",
            "test Loss: 0.7478, test Accuracy: 79.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice:\n",
        "\n",
        "\n",
        "*   Performance improvement method - knowledge distillation, was used in the paper and not here. Hence I accept a difference in performance of several percents accuracy to their favor.\n",
        "*   In addition, in most steps I cut the training (much) earlier then required in the paper.\n",
        "*   Some hyperparameters are also inaccurate\n",
        "\n"
      ],
      "metadata": {
        "id": "83wcTIcqUnqN"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOLc8WL9flKTJZrpqXASXvP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}